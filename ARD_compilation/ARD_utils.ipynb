{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98dc3fac",
   "metadata": {},
   "source": [
    "### Author: Md Fahim Hasan\n",
    "### Work Email: mdfahim.hasan@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2558bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as ddf\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.enums import MergeAlg, Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c13ce3",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fa4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_arr_object(raster_file, rasterio_obj=False, band=1, get_file=True, change_dtype=True):\n",
    "    \"\"\"\n",
    "    Get raster array and raster file.\n",
    "\n",
    "    :param raster_file: Input raster filepath.\n",
    "    :param rasterio_obj: Set True if raster_file is a rasterio object.\n",
    "    :param band: Selected band to read. Default set to 1.\n",
    "    :param get_file: Set to False if raster file is not required.\n",
    "    :param change_dtype: Set to True if want to change raster data type to float. Default set to True.\n",
    "\n",
    "    :return: Raster numpy array and rasterio object file (get_file=True, rasterio_obj=False).\n",
    "    \"\"\"\n",
    "    if not rasterio_obj:\n",
    "        raster_file = rio.open(raster_file)\n",
    "    else:\n",
    "        get_file = False\n",
    "    raster_arr = raster_file.read(band)\n",
    "    if change_dtype:\n",
    "        raster_arr = raster_arr.astype(np.float32)\n",
    "        if raster_file.nodata:\n",
    "            raster_arr[np.isclose(raster_arr, raster_file.nodata)] = np.nan\n",
    "    if get_file:\n",
    "        return raster_arr, raster_file\n",
    "    else:\n",
    "        return raster_arr\n",
    "    \n",
    "\n",
    "def write_array_to_raster(raster_arr, raster_file, transform, output_path, ref_file=None, nodata=-9999):\n",
    "    \"\"\"\n",
    "    Write raster array to Geotiff format.\n",
    "\n",
    "    :param raster_arr: Raster array data to be written.\n",
    "    :param raster_file: Original rasterio raster file containing geo-coordinates.\n",
    "    :param transform: Affine transformation matrix.\n",
    "    :param output_path: Output filepath.\n",
    "    :param ref_file: Write output raster considering parameters from reference raster file.\n",
    "    :param nodata: no_data_value set as -9999.\n",
    "\n",
    "    :return: Output filepath.\n",
    "    \"\"\"\n",
    "    if ref_file:\n",
    "        raster_file = rio.open(ref_file)\n",
    "        transform = raster_file.transform\n",
    "\n",
    "    with rio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=raster_arr.shape[0],\n",
    "            width=raster_arr.shape[1],\n",
    "            dtype=raster_arr.dtype,\n",
    "            count=1,  # raster_file.count\n",
    "            crs=raster_file.crs,\n",
    "            transform=transform,\n",
    "            nodata=nodata\n",
    "    ) as dst:\n",
    "        dst.write(raster_arr, 1) #raster_file.count\n",
    "\n",
    "    return output_path\n",
    "   \n",
    "     \n",
    "def make_lat_lon_array_from_raster(input_raster, nodata=-9999):\n",
    "    \"\"\"\n",
    "    Make lat, lon array for each pixel using the input raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Input raster filepath that will be used as reference raster.\n",
    "    nodata : No data value. Default set to -9999.\n",
    "    \n",
    "    returns: Lat, lon array with nan value (-9999) applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    raster_file = rio.open(input_raster)\n",
    "    raster_arr = raster_file.read(1)\n",
    "\n",
    "    # calculating lat, lon of each cells centroid\n",
    "    height, width = raster_arr.shape\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rio.transform.xy(rows=rows, cols=cols, transform=raster_file.transform)\n",
    "    \n",
    "    # flattening and reshaping to the input_raster's array size\n",
    "    xs = np.array(xs).flatten()\n",
    "    ys = np.array(ys).flatten()\n",
    "    \n",
    "    lon_arr = xs.reshape(raster_arr.shape)\n",
    "    lat_arr = ys.reshape(raster_arr.shape)\n",
    "    \n",
    "    # assigning no_data_value\n",
    "    lon_arr[raster_arr==nodata] = nodata\n",
    "    lat_arr[raster_arr==nodata] = nodata\n",
    "    \n",
    "    return lon_arr, lat_arr\n",
    "\n",
    "\n",
    "def mask_raster_array_by_shapefile(input_raster, mask_shape, output_dir=None, raster_name=None, invert=False,\n",
    "                                   crop=True, save_masked_arr=False):\n",
    "    \"\"\"\n",
    "    Mask a raster using a input shapefile.\n",
    "\n",
    "    Parameters:\n",
    "    input_raster: Input raster filepath.\n",
    "    mask_shape : Reference shape file to crop input_raster.\n",
    "    output_dir : Defaults to None. Set a output raster directory path if save_masked_arr is True.\n",
    "    raster_name : Defaults to None. Set a output raster name if save_masked_arr is True.\n",
    "    invert : If False (default) pixels outside shapes will be masked.\n",
    "             If True, pixels inside shape will be masked.\n",
    "    crop : Whether to crop the raster to the extent of the shapes. Set to False if invert=True is used.\n",
    "    save_masked_arr : Set to true if want to save cropped/masked raster array. If True, must provide output_raster_name and\n",
    "                       output_dir.\n",
    "\n",
    "    returns : Masked raster array and masked raster filepath.\n",
    "    \"\"\"\n",
    "    input_arr, input_file = read_raster_arr_object(input_raster)\n",
    "    \n",
    "    shapefile = gpd.read_file(mask_shape)\n",
    "    geoms = shapefile['geometry'].values  # list of shapely geometries\n",
    "    geoms = [mapping(geoms[0])]\n",
    "    \n",
    "    # masking\n",
    "    masked_arr, masked_transform = mask(dataset=input_file, shapes=geoms, filled=True, crop=crop, invert=invert, \n",
    "                                        all_touched=False)\n",
    "    masked_arr = masked_arr.squeeze()  # Remove axes of length 1 from the array\n",
    "    \n",
    "\n",
    "    if save_masked_arr:\n",
    "        # naming output file\n",
    "        makedirs([output_dir])\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "\n",
    "        # saving output raster\n",
    "        masked_raster = write_array_to_raster(raster_arr=masked_arr, raster_file=input_file, transform=masked_transform,\n",
    "                              output_path=output_raster)\n",
    "        return masked_arr, masked_raster\n",
    "   \n",
    "    else: # in case raster is not saved return only masked raster array\n",
    "        return masked_arr\n",
    "\n",
    "    \n",
    "def resample_raster_based_on_ref_raster(input_raster, ref_raster, output_dir, raster_name, resampling_alg=Resampling.bilinear,\n",
    "                                        paste_value_on_ref_raster=False):\n",
    "    \"\"\"\n",
    "    Resample raster based on a refernce raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Filepath of input raster to resample.\n",
    "    ref_raster : Filepath of input raster to be used in determining resample height/width/affine transformation/crs/dtype/nodata.\n",
    "    output_raster : Filepath of resampled output raster.\n",
    "    resampling_alg : resampling algorithm. Can be Resampling.nearest/ Resampling.bilinear/Resampling.cubic or \n",
    "                     any resampling algorith rasterio supports Default set to Resampling.bilinear.\n",
    "    paste_value_on_ref_raster : Set to True if want to have nodata pixels on the resampled raster similar to reference raster. \n",
    "    \n",
    "    returns: The resampled output raster filepath.\n",
    "    \"\"\"\n",
    "    makedirs([output_dir])\n",
    "    \n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    \n",
    "    # target shape. use a reference raster (created using GIS for a specific region) to decide.\n",
    "    resampled_height, resampled_width = ref_arr.shape\n",
    "\n",
    "    with rio.open(input_raster) as dataset:\n",
    "        # resample data to target shape\n",
    "        resampled_arr = dataset.read(1,\n",
    "                            out_shape=(1,\n",
    "                                       resampled_height,\n",
    "                                       resampled_width),\n",
    "                            resampling=resampling_alg)\n",
    "\n",
    "        resampled_arr = resampled_arr.squeeze() # removing the 1 (for count) from the dimension\n",
    "        \n",
    "        if paste_value_on_ref_raster:\n",
    "            resampled_arr = np.where(np.isnan(ref_arr), -9999, resampled_arr)\n",
    "        \n",
    "        # Saving the resampled data\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "        write_array_to_raster(raster_arr=resampled_arr, raster_file=ref_file, \n",
    "                              transform=ref_file.transform, output_path=output_raster, \n",
    "                              ref_file=None, nodata=-9999)\n",
    "        \n",
    "        return output_raster  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ff022",
   "metadata": {},
   "source": [
    "## Soil-Elevation 4km ARD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00328e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_soil_elevation_data_4km(dataset_list, output_folder, reference_raster, paste_value_on_ref_raster=True):\n",
    "    \"\"\"\n",
    "    Resample soil and elavation datasets to 4km.\n",
    "    \n",
    "    :param dataset_list : A list of dataset file paths.\n",
    "    :param output_folder : Filepath of output folder to save data.\n",
    "    :param reference_raster : A reference raster filepath. This raster will be used as resampling reference. \n",
    "                              Can use one of the finalized weather datasets.   \n",
    "    :param paste_value_on_ref_raster : Set to True if want to have nodata pixels on the resampled raster similar to reference raster.\n",
    "   \n",
    "     returns: Resampled 4km spatial resolution soil and elevation datasets.\n",
    "    \"\"\"\n",
    "    for data in dataset_list:\n",
    "        raster_name = os.path.basename(data)\n",
    "        if 'elevation' in raster_name:\n",
    "            raster_name = 'elevation.tif'\n",
    "        elif 'slope' in raster_name:\n",
    "            raster_name = 'slope.tif'\n",
    "        \n",
    "        # resampling\n",
    "        resample_raster_based_on_ref_raster(input_raster=data, ref_raster=reference_raster, \n",
    "                                            output_dir=output_folder, raster_name=raster_name, \n",
    "                                            resampling_alg=Resampling.bilinear, \n",
    "                                            paste_value_on_ref_raster=paste_value_on_ref_raster)\n",
    "        \n",
    "                \n",
    "def generate_soil_elevation_ARD(input_data_directory, output_folder, savename):\n",
    "    \"\"\"\n",
    "    Compile soil-elevation datasets to a dataframe. The dataframe will be regarded as a Analytical Ready Dataset (ARD).\n",
    "    \n",
    "    params:\n",
    "    input_data_directory : (str) Filepath of input datasets' directory.\n",
    "    output_folder : (str) Filepath of oputput directory/folder. \n",
    "    savename : (str) Name of the output parquet file.\n",
    "    \n",
    "    returns: compiled soil and elevation ARD dataframe.\n",
    "    \"\"\"\n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    datasets = glob(os.path.join(input_data_directory, '*.tif'))\n",
    "    \n",
    "    # a dictionary where daily dataset values will be stored under variable_name \n",
    "    variable_dict = {}  \n",
    "\n",
    "    for count, data in enumerate(datasets):\n",
    "             \n",
    "        variable_name = os.path.basename(data).split('.')[0]  # extracted variable name \n",
    "        \n",
    "        print(f'compiling data for {variable_name}...')\n",
    "\n",
    "        data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "\n",
    "        # Assigning all values to the variable_dict\n",
    "        variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "\n",
    "        # adding lat/lon info\n",
    "        if data == datasets[-1]:\n",
    "            lon_arr, lat_arr = make_lat_lon_array_from_raster(data, nodata=-9999)\n",
    "            lon_arr = lon_arr.flatten()\n",
    "            lat_arr = lat_arr.flatten()\n",
    "                \n",
    "            variable_dict['lat'] = list(lat_arr)\n",
    "            variable_dict['lon'] = list(lon_arr)\n",
    "            \n",
    "    variable_df = pd.DataFrame(variable_dict)\n",
    "    variable_df = variable_df.dropna()\n",
    "    \n",
    "    # saving data\n",
    "    if '.parquet'in savename:\n",
    "        output_parquet_file = os.path.join(output_folder, savename)\n",
    "    else:\n",
    "        output_parquet_file = os.path.join(output_folder, f'{savename}.parquet')\n",
    "    \n",
    "    variable_df.to_parquet(output_parquet_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf707c",
   "metadata": {},
   "source": [
    "## Soil-Elevation 100m ARD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005c4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_resample_soil_elevation_datasets_to_shapefile(input_datasets_fp, output_dir, masking_shapefile, target_raster,\n",
    "                                                       paste_value_on_target_raster=True):\n",
    "    \"\"\"\n",
    "    Mask soil and elevation datasets with a shapefile and resample to resolution of target satellite data (100m).\n",
    "      \n",
    "    params:\n",
    "    input_datasets_fp : A list of input dataset filepaths. \n",
    "    output_dir : Output data directory where masked and resampled data will be saved. \n",
    "    masking_shapefile : Filepath of shapefile to mask the data with.\n",
    "    target_raster : 100m ref raster. Should be a satellite raster of 100m resolution that is inside the shapefile. \n",
    "    paste_value_on_target_raster : Set to True if want to have nodata pixels on the resampled raster similar to target raster. \n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "\n",
    "    # resampling data only the required variables\n",
    "    for data in input_datasets_fp:\n",
    "        variable_name = os.path.basename(data).split('.')[0]\n",
    "        if 'elevation' in variable_name:\n",
    "            variable_name = 'elevation'\n",
    "        elif 'slope' in variable_name:\n",
    "            variable_name = 'slope'\n",
    "        \n",
    "        print(f'Masking > Resampling data for {variable_name}...')\n",
    "        \n",
    "        masked_output_dir =  os.path.join(output_dir, 'masked') #saving initial masked rasters in an 'interim folder' inside each variable's folder\n",
    "        resampled_output_dir =  output_dir\n",
    "        makedirs([resampled_output_dir, masked_output_dir])\n",
    "        \n",
    "        raster_name = variable_name + '.tif'\n",
    "        masked_raster_fp = os.path.join(masked_output_dir, raster_name)\n",
    "        \n",
    "        # masking with shapefile\n",
    "        mask_raster_array_by_shapefile(input_raster=data, mask_shape=masking_shapefile, \n",
    "                                       output_dir=masked_output_dir, raster_name=raster_name, \n",
    "                                       invert=False, crop=True, save_masked_arr=True)\n",
    "        \n",
    "        # resampling to make sure the processed raster's pixels allign with the target_raster's pixels\n",
    "        resample_raster_based_on_ref_raster(input_raster=masked_raster_fp, ref_raster=target_raster, \n",
    "                                            output_dir=resampled_output_dir, raster_name=raster_name,\n",
    "                                            paste_value_on_ref_raster=paste_value_on_target_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8c8f7",
   "metadata": {},
   "source": [
    "## Weather 4km ARD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3f58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_4km_preARD_dataframes(data_directories_list, output_folder, savename, \n",
    "                                         dataset_in_each_chunk=3):\n",
    "    \"\"\"\n",
    "    Compile 4km weather datasets to multiple preARD dataframes. \n",
    "    Weather datasets have extended daily records and will take high memory to compile into single dataframe at once; therefore, \n",
    "    compiling weather datasets into multiple dataframes (referring to as preARD). These preARD dataframes will be combined into \n",
    "    a single dataframe and averaged to form the 4km weather ARD in a later step.\n",
    "    \n",
    "    params:\n",
    "    data_directories_list : (list) List of str of all weather data (model-interpolated/resampled) directories.The code will automatically get data in the sub-directories.\n",
    "    output_folder : (str) Main output folder. The code will automatically save data in the individual sub-directories.\n",
    "    savename : (str) Name of the output parquet file.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    # All datasets except avg_temp, min_Rhumid, max_Rhumid, TotNetSR have modeled and TWC data mixed (TWC data mixed from 2015)\n",
    "    # The number of days in avg_temp, min_Rhumid, max_Rhumid, TotNetSR are 7809, while in other dataasets it's 7821\n",
    "    # creating a separate data_chunk for avg_temp, min_Rhumid, max_Rhumid, TotNetSR so that they can be saved asw separate preARD\n",
    "    separated_datasets = []\n",
    "    for i in data_directories_list:\n",
    "        dataname =  os.path.basename(i)\n",
    "        \n",
    "        if dataname in ['avg_temp', 'min_Rhumid', 'max_Rhumid', 'TotNet_SR']:\n",
    "            separated_datasets.append(i)\n",
    "    \n",
    "    # Removing 'avg_temp', 'min_Rhumid', 'max_Rhumid', 'TotNetSR' from data directories list\n",
    "    data_directories_list = [i for i in data_directories_list if i not in separated_datasets]\n",
    "    \n",
    "    # creating a list of list of directories. Each individual list will be processed as a chunk and saved as individual parquet\n",
    "    data_chunks = [data_directories_list[x:x+dataset_in_each_chunk] for x in range(0, len(data_directories_list), dataset_in_each_chunk)]\n",
    "        \n",
    "    # creating a separate chunk for avg_temp, min_Rhumid, max_Rhumid, TotNetSR\n",
    "    data_chunks.append(separated_datasets)\n",
    "    \n",
    "    \n",
    "    for num_chunk, datasets in enumerate(data_chunks):\n",
    "        # will be used to multiply lat/lon data \n",
    "        num_days = len(glob(os.path.join(datasets[0], '*.tif')))\n",
    "    \n",
    "        # a dictionary where daily dataset values will be stored under variable_name \n",
    "        variable_dict = {}  \n",
    "        \n",
    "        for data_dir in datasets:\n",
    "            all_data = glob(os.path.join(data_dir, '*.tif')) # making list of all dataset in a particular data folder\n",
    "            all_data = sorted(all_data)  # to sort data by date so that all variables are compiled in same serial\n",
    "        \n",
    "            variable_name = os.path.basename(data_dir).split('.')[0]  # extracted variable name \n",
    "            \n",
    "            print(f'compiling data for {variable_name}...')\n",
    "\n",
    "            # loop for reading datasets and storing pixel info in a dictionary\n",
    "            for count, data in enumerate(all_data):\n",
    "                # retrieving and storing data\n",
    "                data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "\n",
    "                # extarcting, formatting, and storing date info\n",
    "                date = os.path.basename(data).split('.')[0].split('_')[-1]\n",
    "\n",
    "                len_data = len(data_arr)  # number of pixels in each daily dataset (array)\n",
    "                date_list = [int(date)] * len_data\n",
    "\n",
    "                # Assigning all values to the variable_dict\n",
    "                if count == 0:\n",
    "                    variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "                    variable_dict['date'] = date_list\n",
    "\n",
    "                else:\n",
    "                    variable_dict[variable_name].extend(list(data_arr))  # storing flattened data in a dictionary under the variable name)\n",
    "                    variable_dict['date'].extend(date_list)\n",
    "\n",
    "                # adding lat/lon info\n",
    "                if data_dir == datasets[-1]:\n",
    "                    lon_arr, lat_arr = make_lat_lon_array_from_raster(data, nodata=-9999)\n",
    "                    lon_arr = lon_arr.flatten()\n",
    "                    lat_arr = lat_arr.flatten()\n",
    "                \n",
    "                    if count == 0:\n",
    "                        variable_dict['lat'] = list(lat_arr)\n",
    "                        variable_dict['lon'] = list(lon_arr)\n",
    "                    else:\n",
    "                        variable_dict['lat'].extend(list(lat_arr))\n",
    "                        variable_dict['lon'].extend(list(lon_arr))\n",
    "\n",
    "        variable_df = pd.DataFrame(variable_dict)\n",
    "        variable_ddf = ddf.from_pandas(variable_df, npartitions=10)\n",
    "        variable_ddf = variable_ddf.dropna()\n",
    "        variable_ddf = variable_ddf.reset_index()\n",
    "    \n",
    "        # saving data\n",
    "        output_parquet_file = os.path.join(output_folder, savename, f'{savename}_{num_chunk}.parquet')\n",
    "        variable_ddf.to_parquet(output_parquet_file, write_index=False)\n",
    "\n",
    "        \n",
    "def compile_preARD_multiDF_to_ARD(parquet_folder, output_folder, savename):\n",
    "    \"\"\"\n",
    "    Compile multiple dataframe of era5 data (generated by compile_era5_daily_data_to_multiple_dataframe()) \n",
    "    into a single dataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_folder : Filepath of folder where multiple parquet files (dataframes) are saved.\n",
    "    output_folder : Filepath of output folder where single parquet file (dataframe) with all era5 variales will be saved.\n",
    "    save_keyword : A keyword (str) to distinguish between 4km/8km parquet files. Can set to '4km'/'8km'.\n",
    "    \n",
    "    returns: Compiled single dataframe.\n",
    "    \"\"\"\n",
    "    parquet_files = glob(os.path.join(parquet_folder, '*.parquet'))\n",
    "    for parq in parquet_files:\n",
    "        df = pd.read_parquet(parq)\n",
    "        df = df.drop(columns=['index'])\n",
    "        if parq == parquet_files[0]:\n",
    "            compiled_df = df\n",
    "        else:\n",
    "            compiled_df = compiled_df.merge(df, on=['date', 'lat', 'lon'])\n",
    "    \n",
    "    compiled_df['month_day'] = compiled_df['date'].apply(lambda x: f'{str(x)[4:6]}_{str(x)[6:]}')\n",
    "\n",
    "    # Averaging dataframe for month_day to generate the final ARD\n",
    "    compiled_df = compiled_df.drop(columns=['date'])\n",
    "    columns_to_compile = [col for col in compiled_df.columns if col not in ['month_day', 'lat', 'lon']]  # removing the columns to use in group_by from grouping\n",
    "    \n",
    "    compiled_df = compiled_df.groupby(['month_day', 'lat', 'lon'])[columns_to_compile].mean().reset_index()\n",
    "    \n",
    "    # saving the ARD\n",
    "    if '.parquet' not in savename:\n",
    "        savename = savename + '.parquet'\n",
    "        \n",
    "    output_parquet = os.path.join(output_folder, savename)\n",
    "    compiled_df.to_parquet(output_parquet, index=False)\n",
    "       \n",
    "    return compiled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b56f9",
   "metadata": {},
   "source": [
    "## Weather 100m ARD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3faa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_mask_resample_weather_datasets_to_shapefile(input_data_dir_list, output_main_dir, masking_shapefile, \n",
    "                                                         target_raster_whole_aoi, target_raster_smaller_aoi):\n",
    "    \"\"\"\n",
    "    Performs resample > mask > ressample operations to a weather data to take it from low resolution (4km) to \n",
    "    high resolution (100m) for specified smaller AOI.\n",
    "    \n",
    "    params:\n",
    "    input_data_dir_list : List of input dataset directory dilepaths.\n",
    "    output_main_dir : Output data main directory where masked and resampled data will be saved. \n",
    "                      Subdirectories for each variable will be selected by the code.\n",
    "    masking_shapefile : Filepath of shapefile to mask the data with.\n",
    "    target_raster_whole_aoi : 100m ref raster for a larger AOI (similar to input data extent). \n",
    "    target_raster_smaller_aoi : 100m ref raster for the smaller AOI for which data will be masked and resampled.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "\n",
    "    # resampling data only the required variables\n",
    "    for data_dir in input_data_dir_list:\n",
    "        variable_name = os.path.basename(data_dir)\n",
    "        print(f'Resampling > Masking > Resampling data for {variable_name}...')\n",
    "        \n",
    "        all_rasters = glob(os.path.join(data_dir, '*.tif'))\n",
    "        \n",
    "        # to save 100m raster for the first resample\n",
    "        interim_output_dir = os.path.join(output_main_dir, variable_name, 'resampled')\n",
    "        # to save masked rasters in an interim folder inside each variable's folder\n",
    "        masked_output_dir =  os.path.join(output_main_dir, variable_name, 'masked') \n",
    "        # to save final resampled raster\n",
    "        resampled_output_dir =  os.path.join(output_main_dir, variable_name)\n",
    "        \n",
    "        makedirs([resampled_output_dir, interim_output_dir, masked_output_dir])\n",
    "        \n",
    "        for raster in all_rasters:\n",
    "            # Step 1: Resampling to 100m with target-raster_whole_aoi\n",
    "            raster_name = os.path.basename(raster).split('.')[0] + '.tif'\n",
    "            first_resampled_raster = resample_raster_based_on_ref_raster(input_raster=raster, \n",
    "                                                                         ref_raster=target_raster_whole_aoi, \n",
    "                                                                         output_dir=interim_output_dir, \n",
    "                                                                         raster_name=raster_name)\n",
    "            \n",
    "            # Step 2: Mask raster with the masking_shapefile (smaller AOI) \n",
    "            masked_arr, masked_raster_fp = mask_raster_array_by_shapefile(input_raster=first_resampled_raster, \n",
    "                                                                          mask_shape=masking_shapefile, \n",
    "                                                                          output_dir=masked_output_dir, raster_name=raster_name, \n",
    "                                                                          invert=False, crop=True, save_masked_arr=True)\n",
    "            \n",
    "            # Step 3: Resampling the masked raster to make sure the processed raster's pixels allign with target_raster_smaller_aoi's pixels\n",
    "            resample_raster_based_on_ref_raster(input_raster=masked_raster_fp, ref_raster=target_raster_smaller_aoi, \n",
    "                                                output_dir=resampled_output_dir, raster_name=raster_name)\n",
    "            \n",
    "            # Deleting the resampled raster for larger aoi\n",
    "            os.remove(first_resampled_raster)\n",
    "            \n",
    "            \n",
    "def create_weather_satellite_100m_preARD_dataframes(weather_data_directories_list, satellite_data_directories_list,\n",
    "                                                    output_folder, savename, dataset_in_each_chunk=3):\n",
    "    \"\"\"\n",
    "    Compile 100m weather datasets to multiple preARD dataframes. \n",
    "    Weather datasets have extended daily records and will take high memory to compile into single dataframe at once; therefore, \n",
    "    compiling weather datasets into multiple dataframes (referring to as preARD). These preARD dataframes will be combined into \n",
    "    a single dataframe and averaged to form the 4km weather ARD in a later step.\n",
    "    \n",
    "    *** satellite datasets will be given input as a separate list because it doesn't have the same daily coverage as the weather \n",
    "    data and can cause unequal array issue while being compiled with the weather datasets.\n",
    "    \n",
    "    params:\n",
    "    weather_data_directories_list : List of filepaths of weather data directories.\n",
    "    satellite_data_directories_list : List of filepaths of satellite data directories.\n",
    "    output_folder : Filepath (str) of output folder. \n",
    "    savename : (str) Name of the output parquet file.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    # All datasets except avg_temp, min_Rhumid, max_Rhumid, TotNetSR have modeled and TWC data mixed (TWC data mixed from 2015)\n",
    "    # The number of days in avg_temp, min_Rhumid, max_Rhumid, TotNetSR are 7809, while in other dataasets it's 7821\n",
    "    # creating a separate data_chunk for avg_temp, min_Rhumid, max_Rhumid, TotNetSR so that they can be saved asw separate preARD\n",
    "    separated_datasets = []\n",
    "    for i in weather_data_directories_list:\n",
    "        dataname =  os.path.basename(i)\n",
    "        \n",
    "        if dataname in ['avg_temp', 'min_Rhumid', 'max_Rhumid', 'TotNet_SR']:\n",
    "            separated_datasets.append(i)\n",
    "    \n",
    "    # Removing 'avg_temp', 'min_Rhumid', 'max_Rhumid', 'TotNetSR' from data directories list\n",
    "    weather_data_directories_list = [i for i in weather_data_directories_list if i not in separated_datasets]\n",
    "    \n",
    "    # creating a list of list of directories. Each individual list will be processed as a chunk and saved as individual parquet\n",
    "    data_chunks = [weather_data_directories_list[x:x+dataset_in_each_chunk] for x in range(0, len(weather_data_directories_list), \n",
    "                                                                                   dataset_in_each_chunk)]\n",
    "        \n",
    "    # creating a separate chunk for avg_temp, min_Rhumid, max_Rhumid, TotNetSR\n",
    "    data_chunks.append(separated_datasets)\n",
    "        \n",
    "    # Adding satellite_data_directoroes_list as a separate data_chunk\n",
    "    data_chunks.append(satellite_data_directories_list)\n",
    "    \n",
    "    for num_chunk, datasets in enumerate(data_chunks):\n",
    "        # will be used to multiply lat/lon data \n",
    "        num_days = len(glob(os.path.join(datasets[0], '*.tif')))\n",
    "    \n",
    "        # a dictionary where daily dataset values will be stored under variable_name \n",
    "        variable_dict = {}  \n",
    "        \n",
    "        for data_dir in datasets:\n",
    "            all_data = glob(os.path.join(data_dir, '*.tif')) # making list of all dataset in a particular data folder\n",
    "            all_data = sorted(all_data)  # to sort data by date so that all variables are compiled in same serial\n",
    "        \n",
    "            variable_name = os.path.basename(data_dir).split('.')[0]  # extracted variable name \n",
    "            \n",
    "            print(f'compiling data for {variable_name}...')\n",
    "\n",
    "            # loop for reading datasets and storing pixel info in a dictionary\n",
    "            for count, data in enumerate(all_data):\n",
    "                # retrieving and storing data\n",
    "                data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "\n",
    "                # extarcting, formatting, and storing date info\n",
    "                date = os.path.basename(data).split('.')[0].split('_')[-1]\n",
    "\n",
    "                len_data = len(data_arr)  # number of pixels in each daily dataset (array)\n",
    "                date_list = [int(date)] * len_data\n",
    "\n",
    "                # Assigning all values to the variable_dict\n",
    "                if count == 0:\n",
    "                    variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "                    variable_dict['date'] = date_list\n",
    "\n",
    "                else:\n",
    "                    variable_dict[variable_name].extend(list(data_arr))  # storing flattened data in a dictionary under the variable name)\n",
    "                    variable_dict['date'].extend(date_list)\n",
    "\n",
    "                # adding lat/lon info\n",
    "                if data_dir == datasets[-1]:\n",
    "                    lon_arr, lat_arr = make_lat_lon_array_from_raster(data, nodata=-9999)\n",
    "                    lon_arr = lon_arr.flatten()\n",
    "                    lat_arr = lat_arr.flatten()\n",
    "                \n",
    "                    if count == 0:\n",
    "                        variable_dict['lat'] = list(lat_arr)\n",
    "                        variable_dict['lon'] = list(lon_arr)\n",
    "                    else:\n",
    "                        variable_dict['lat'].extend(list(lat_arr))\n",
    "                        variable_dict['lon'].extend(list(lon_arr))\n",
    "\n",
    "    \n",
    "        variable_df = pd.DataFrame(variable_dict)\n",
    "        variable_ddf = ddf.from_pandas(variable_df, npartitions=10)\n",
    "        variable_ddf = variable_ddf.dropna()\n",
    "        variable_ddf = variable_ddf.reset_index()\n",
    "    \n",
    "        # saving data\n",
    "        output_parquet_file = os.path.join(output_folder, savename, f'{savename}_{num_chunk}.parquet')\n",
    "        variable_ddf.to_parquet(output_parquet_file, write_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba31417",
   "metadata": {},
   "source": [
    "## Miscellanious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d071ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(input_dir_file, copy_dir, search_by='*.tif', rename=None):\n",
    "    \"\"\"\n",
    "    Copy a file to the specified directory.\n",
    "\n",
    "    :param input_dir_file: File path of input directory/ Path of the file to copy.\n",
    "    :param copy_dir: File path of copy directory.\n",
    "    :param search_by: Default set to '*.tif'.\n",
    "    :param rename: New name of file if required. Default set to None. DOesn't work if a directory is being copied.\n",
    "\n",
    "    :returns: File path of copied file.\n",
    "    \"\"\"\n",
    "    makedirs([copy_dir])\n",
    "    if '.tif' not in input_dir_file:\n",
    "        input_files = glob(os.path.join(input_dir_file, search_by))\n",
    "\n",
    "        for each in input_files:\n",
    "            file_name = os.path.basename(each)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "            shutil.copyfile(each, copy_file)\n",
    "\n",
    "    else:\n",
    "        if rename is not None:\n",
    "            copy_file = os.path.join(copy_dir, f'{rename}.tif')\n",
    "        else:\n",
    "            file_name = os.path.basename(input_dir_file)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "        shutil.copyfile(input_dir_file, copy_file)\n",
    "\n",
    "    return copy_file\n",
    "\n",
    "\n",
    "def makedirs(directory_list):\n",
    "    \"\"\"\n",
    "    Make directory (if not exists) from a list of directory.\n",
    "\n",
    "    :param directory_list: A list of directories to create.\n",
    "\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    for directory in directory_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1951cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
