{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6de401a",
   "metadata": {},
   "source": [
    "### Author: Md Fahim Hasan\n",
    "### Work Email: mdfahim.hasan@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18f24d",
   "metadata": {},
   "source": [
    "__Install pytorch first if not already installed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fa2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import dump, load\n",
    "\n",
    "from ipynb.fs.full.general_utils import *\n",
    "from ipynb.fs.full.ML_utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a070dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59aa26",
   "metadata": {},
   "source": [
    "### Early Stopping class\n",
    "__This script was taken from this [GitHub Repo](https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b46d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, save_folder, savename, patience=7, verbose=False, delta=0, trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_folder = save_folder\n",
    "        self.savename = savename\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        \n",
    "        savepath = os.path.join(self.save_folder, self.savename + '.pt')\n",
    "        torch.save(model.state_dict(), savepath)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea01b5",
   "metadata": {},
   "source": [
    "### Neural network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc7989",
   "metadata": {},
   "source": [
    "#### comment on `Early Stopping`:\n",
    "\n",
    "I implemented something `unconventional` in `early stopping` in the neural network code. From `maximum temperature` deep learning modeling, I noticed that the training stops early with conventional early stopping. This happens because at a random epoch the validation loss might get very low and the model uses that validation loss to check early stopping criteria. At that point, the training loss might not be minimized/stable/generalized and there is large gap between train and validation loss. I ran models for a good number of epochs without early stopping and noticed that even after that very low (sudden) validation loss point, training loss decreases and validation loss becomes more generalized (the gap between train and valiation loss decreases/closes). In my thought, we should let the model train for some epochs and then start monitoring `early stopping`. That's why I incorporated `start_EarlyStop_count_from_epoch` argument in the `train_model()` function for the neural network. The default value of `start_EarlyStop_count_from_epoch` is set to 0 so that early stopping can be implemented in a conventional way from the very first epoch.\n",
    "\n",
    "I experimented with different model architecture, learning ratesa, weight decay, batch_size, train-validation-test ratio.May be a optimal architectue (which i couldn't get to) and hyperparameters set will provide better performance and `conventional early stopping` can be implemented instead of `unconventional early stopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cff2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Neural Network Class for nonlinear regression type model. Creates model with user defined feed forward networks.\n",
    "\n",
    "    Methods:\n",
    "        initialize_weights(): Initializes weight for the Neural Network model.\n",
    "        to_torch(): Convert numpy array to torch.Tensor.\n",
    "        standardize(): Standardizes an input torch Tensor.\n",
    "        _forward(): Calculates outputs of each layer given inputs in X.\n",
    "        train(): Trains the model with given training and observed data.\n",
    "                 ** This method() will not be used in our model training.\n",
    "        distribute_T_for_backprop(): Distributes observed data to each pixel/sample for backpropagation purpose.\n",
    "        train_with_distributed_T(): Trains the model with given training and observed data in a distributed approach\n",
    "                                    (Observed data is distributed to each pixel/sample in each epoch before\n",
    "                                    initiating backpropagation).\n",
    "        predict(): Uses trained model to predict on given data.\n",
    "        get_performance_trace(): Provides model loss for each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hiddens_list, n_outputs, dropout=0.1, activation_func='tanh', device='cuda'):\n",
    "        \"\"\"\n",
    "        Creates a neural network with the given structure.\n",
    "\n",
    "        :param n_inputs: int. Number of attributes/predictors that will be used in the model.\n",
    "        :param n_hiddens_list: list. A list of number of units in each hidden layer. Each member of the list represents one\n",
    "                               hidden layer.\n",
    "        :param n_outputs: int. Number of output/prediction. Generally 1.\n",
    "        :paran dropout : float. Dropout value. Default set to 0.1.\n",
    "        :param activation_func: str. Name of the activation function. Can take 'tanh'/'relu'/'leakyrelu'.\n",
    "        :param device: str. Name of the device to run the model. Either 'cpu'/'cuda'.\n",
    "        \"\"\"\n",
    "        # Call parent class (torch.nn.Module) constructor\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        print(f'Model running on {device}....')\n",
    "\n",
    "        # For printing\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden_layers = n_hiddens_list\n",
    "\n",
    "        # To build list of layers, must use torch.nn.ModuleList, not plain list\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "\n",
    "        if activation_func == 'tanh':\n",
    "            self.activation_func = torch.nn.Tanh()\n",
    "        elif activation_func == 'relu':\n",
    "            self.activation_func = torch.nn.ReLU()\n",
    "        elif activation_func == 'leakyrelu':\n",
    "            self.activation_func = torch.nn.LeakyReLU()\n",
    "        else:\n",
    "            raise Exception(\"Activation function should be 'tanh'/'relu'/'leakyrelu'\")\n",
    "\n",
    "        for nh in n_hiddens_list:\n",
    "            self.hidden_layers.append(torch.nn.Sequential(\n",
    "                torch.nn.Dropout(dropout),\n",
    "                torch.nn.Linear(n_inputs, nh),\n",
    "                self.activation_func))\n",
    "\n",
    "            n_inputs = nh  # output of each hidden layer will be input of next hidden layer\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(n_inputs, n_outputs)\n",
    "        self.initialize_weights()\n",
    "        self.to(self.device)  # transfers the whole thing to 'cuda' if device='cuda'\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'NeuralNetwork({}, {}, {}, activation func={})'.format(self.n_inputs, self.n_hidden_layers,\n",
    "                                                                      self.n_outputs, self.activation_func)\n",
    "    def __str__(self):\n",
    "        s = self.__repr__()\n",
    "        if self.n_epochs > 0:  # self.total_epochs\n",
    "            s += '\\n Trained for {} epochs.'.format(self.n_epochs)\n",
    "            s += '\\n Final standardized training error {:.4g}.'.format(self.performance_trace[-1])\n",
    "        return s\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes weight for the Neural Network model. For 'tanh' initializing method is 'xavier_normal'. For 'relu' and\n",
    "        'leakyrelu' initialization method is 'kaiming_normal'.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                if isinstance(self.activation_func, torch.nn.Tanh):\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                elif isinstance(self.activation_func, torch.nn.ReLU):\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                elif isinstance(self.activation_func, torch.nn.LeakyReLU):\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def to_torch(self, M, torch_type=torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        Convert numpy array to torch Tensor.\n",
    "\n",
    "        :param M: numpy array.\n",
    "        :param torch_type: torch data type. Default set to Float.\n",
    "\n",
    "        :return: A torch.Tensor.\n",
    "        \"\"\"\n",
    "        if not isinstance(M, torch.Tensor):\n",
    "            M = torch.from_numpy(M).type(torch_type).to(self.device)\n",
    "        return M\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculates outputs of each layer given inputs in X.\n",
    "\n",
    "        :param X: torch.Tensor. Standardized input array representing attributes as columns and samples as row.\n",
    "\n",
    "        :return: torch.Tensor. Standardized output array representing model prediction.\n",
    "        \"\"\"\n",
    "        Y = X\n",
    "        for hidden_layers in self.hidden_layers:\n",
    "            Y = hidden_layers(Y)  # going through hidden layers\n",
    "\n",
    "        # Final output\n",
    "        Y = self.output_layer(Y)\n",
    "\n",
    "        return Y\n",
    "    \n",
    "def standardize(M):\n",
    "    \"\"\"\n",
    "    Standardizes an input torch Tensor.\n",
    "\n",
    "    :param M: torch.tensor.\n",
    "\n",
    "    :return: Standardized torch Tensor, mean, and standard deviation values.\n",
    "    \"\"\"\n",
    "    M_means = torch.mean(M, dim=0, keepdim=False)\n",
    "    M_stds = torch.std(M, dim=0, keepdim=False)\n",
    "\n",
    "    Ms = (M - M_means) / M_stds\n",
    "\n",
    "    return Ms, M_means, M_stds\n",
    "\n",
    "def normalize(M):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    normalized_data = scaler.fit_transform(M)\n",
    "    \n",
    "    return normalized_data, scaler\n",
    "    \n",
    "def create_dataloader(x, y, batch_size=64):\n",
    "    features = torch.tensor(np.array(x)).float()\n",
    "    target = torch.tensor(np.array(y)).float()\n",
    "\n",
    "    dataset = TensorDataset(features, target)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "        \n",
    "    \n",
    "def train_model(model, train_dataloader, validation_dataloader, n_epochs, \n",
    "                save_folder, savename, \n",
    "                early_stop=False, start_EarlyStop_count_from_epoch=0, patience=3, \n",
    "                method='adam', learning_rate=0.01, weight_decay=0.01, device='cuda', \n",
    "                verbose=True, standardization=True):\n",
    "    \"\"\"\n",
    "    Trains the model with given training and observed data.\n",
    "    ** This method() will not be used in our model training.\n",
    "\n",
    "    :param model: model object coming from NeuralNetwork class.\n",
    "    :param train_dataloader : train_dataloader object.\n",
    "    :param validation dataloader : validation_dataloader object. \n",
    "    :param n_epochs: int. Number of passes to take through all samples.\n",
    "    :param save_folder : Filepath of folder to save model. \n",
    "    :param save_name : checkpoint name to save with.\n",
    "    :param erly_stop : Set to True to enable early stopping.\n",
    "    :param start_EarlyStop_count_from_epoch : Integer value of epoch from which early_stop tracking will start for validation_loss.\n",
    "                                              Default set to 0 to start tracking from the very beginning.\n",
    "    :param patience : How long to wait after last time validation loss improved. Default 7.\n",
    "    :param method: str. Optimization algorithm. Can take 'adam'/'sgd'.\n",
    "    :param learning_rate: float. Controls the step size of each update, only for sgd and adam.\n",
    "    :param weight_decay : float. Controls L2_regularization. Default set to 0.01.\n",
    "    :param device: str. Name of the device to run the model. Default set to 'cuda'.\n",
    "    :param verbose: boolean. If True, prints training progress statement.\n",
    "    :param standardization : Set to False if want to normalize input variables between 0-1 instead of standardizing with mean & std.\n",
    "    \n",
    "    :returns: If standardization = True - A trained NN model, training loss, validation loss, training mean, and training standard deviation\n",
    "              If standardization = False - A trained NN model, training loss, validation loss, and normalization scaler.\n",
    "    \"\"\"\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # Call the requested optimizer method to train the weights.\n",
    "    if method == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif method == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "    else:\n",
    "        raise Exception(\"method must be 'sgd', 'adam'\")\n",
    "\n",
    "    mse_func = torch.nn.MSELoss()  # mse function\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(save_folder, savename, patience=patience, verbose=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "\n",
    "        training_loss = 0\n",
    "        for features, target in train_dataloader:\n",
    "            # reshaping target to have them in single column \n",
    "            target = target.reshape(-1, 1)  \n",
    "\n",
    "            if standardization:\n",
    "                # Standardization\n",
    "                Xs, X_means, X_stds = standardize(features)\n",
    "\n",
    "            else:\n",
    "                # Normalization\n",
    "                Xs, scaler = normalize(features)  # this is numpy array\n",
    "                Xs = torch.tensor(Xs).float()  # converting normalized array to torch tensor again\n",
    "\n",
    "            # moving features and target to cuda\n",
    "            Xs = Xs.to(device)\n",
    "            T = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            Xs.requires_grad_(True)\n",
    "            Y = model(Xs)\n",
    "            # calculating MSE Loss\n",
    "            mse_loss = mse_func(Y, T)  \n",
    "            # Backward pass\n",
    "            mse_loss.backward()  \n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # Reset the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # summing to training loss in an epoch\n",
    "            training_loss += mse_loss.item()\n",
    "            # Storing MSE Loss\n",
    "            train_losses.append(mse_loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation  \n",
    "        validation_loss = 0\n",
    "        with torch.no_grad(): # tells PyTorch not to compute gradients\n",
    "            for features, target in validation_dataloader:\n",
    "                # reshaping target to have them in single column \n",
    "                target = target.reshape(-1, 1)  \n",
    "\n",
    "                if standardization:\n",
    "                    # Standardization and moving to cuda (if device is cuda)\n",
    "                    Xs = (features - X_means) /  X_stds\n",
    "                else:\n",
    "                    Xs = scaler.transform(features)  # this is numpy array\n",
    "                    Xs = torch.tensor(Xs).float() # converting normalized array to torch tensor again\n",
    "\n",
    "                # moving features and target to cuda\n",
    "                Xs = Xs.to(device)\n",
    "                T = target.to(device)\n",
    "\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                prediction = model(Xs)\n",
    "                #calculate MSE loss\n",
    "                mse_loss = mse_func(prediction, T) \n",
    "                # summing to validation loss in an epoch\n",
    "                validation_loss += mse_loss.item()\n",
    "                # Storing MSE Loss\n",
    "                valid_losses.append(mse_loss.item())\n",
    "    \n",
    "\n",
    "        # Calculating mean MSE after each epoch\n",
    "        mean_train_loss = training_loss / len(train_dataloader)\n",
    "        avg_train_losses.append(mean_train_loss)\n",
    "\n",
    "        mean_validation_loss = validation_loss / len(validation_dataloader)\n",
    "        avg_valid_losses.append(mean_validation_loss)\n",
    "\n",
    "        # Print performance on train and validation dataset\n",
    "        print(f'avg. MSE Loss in epoch {epoch + 1}:',\n",
    "              f'train loss: {mean_train_loss}', '|' ,f'validation loss: {mean_validation_loss}')\n",
    "\n",
    "\n",
    "        # early_stopping needs the validation loss to check if it has decreased, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        if early_stop:\n",
    "            if epoch == start_EarlyStop_count_from_epoch:\n",
    "                early_stopping(mean_validation_loss, model)\n",
    "                start_EarlyStop_count_from_epoch += 1  # increasing 'start_EarlyStop_count_from_epoch' value to keep tracking\n",
    "                 \n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        else: #save model if early_stop=False\n",
    "            savepath = os.path.join(save_folder, savename + '.pt')\n",
    "            torch.save(model.state_dict(), savepath)\n",
    "            \n",
    "\n",
    "    # saving model training time\n",
    "    end_time = timeit.default_timer()\n",
    "    runtime = (end_time-start_time)/60\n",
    "    run_str = f'model training time {runtime} mins'\n",
    "    print('model training time {:.3f} mins'.format(runtime))\n",
    "\n",
    "    runtime_save = os.path.join(save_folder, savename+'_training_runtime.txt')\n",
    "    with open(runtime_save, 'w') as file:\n",
    "        file.write(run_str)\n",
    "\n",
    "\n",
    "    if standardization:\n",
    "        # **** model checkpoint was saved during model training*****\n",
    "        # saving model results and data preparataion tools\n",
    "        dump(avg_train_losses, open(os.path.join(save_folder, savename + '_trainloss.pkl'), 'wb'))\n",
    "        dump(avg_valid_losses, open(os.path.join(save_folder, savename + '_validationloss.pkl'), 'wb'))\n",
    "        dump(X_means, open(os.path.join(save_folder, savename + '_trainmean.pkl'), 'wb'))\n",
    "        dump(X_stds, open(os.path.join(save_folder, savename + '_trainstd.pkl'), 'wb'))\n",
    "\n",
    "        return model, avg_train_losses, avg_valid_losses, X_means, X_stds\n",
    "    else:\n",
    "        # **** model checkpoint was saved during model training*****\n",
    "        # saving model results and data preparataion tools\n",
    "        dump(avg_train_losses, open(os.path.join(save_folder, savename + '_trainloss.pkl'), 'wb'))\n",
    "        dump(avg_valid_losses, open(os.path.join(save_folder, savename + '_validationloss.pkl'), 'wb'))\n",
    "        dump(scaler, open(os.path.join(save_folder, savename + '_trainscaler.pkl'), 'wb'))\n",
    "        return model, avg_train_losses, avg_valid_losses, scaler\n",
    "    \n",
    "\n",
    "def load_trained_model(model_initialized, model_path, train_loss_path, validation_loss_path, \n",
    "                       train_mean_path, train_std_path, standardization=True, train_scaler_path=None):\n",
    "    \"\"\"\n",
    "    Load trained neural network model.\n",
    "    \n",
    "    : model_initialization : Initialized model from NeuralNetwork() class. \n",
    "                             Need to have the same n_inputs, n_hiddens_list, n_outputs.\n",
    "    :param model_path : Filepath of saved trained neural network model.\n",
    "    :param train_loss_path : Filepath of saved trained neural network training loss.\n",
    "    :param validation_loss_path : Filepath of saved trained neural network validation loss.\n",
    "    :param train_mean_path : Filepath of saved trained neural network input feature mean for feature standardization.\n",
    "    :param train_std_path : Filepath of saved trained neural network input feature std. deviation for feature standardization.\n",
    "    :param standardization : Set to True if data was standardized during training. \n",
    "                             If data was normalized during training, set to False and define a \n",
    "                             train_scaler_path set (also set train_mean_path/train_std_path to None)\n",
    "    :param train_scaler_path : If standardization=False, set a train_scaler_path to load it.\n",
    "    \n",
    "    :returns: If standardization = True - A trained NN model, training loss, validation loss, training mean, and training standard deviation\n",
    "              If standardization = False - A trained NN model, training loss, validation loss, and normalization scaler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading trained model\n",
    "    loaded_model = model_initialized\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()  # taking loaded model to evaluation mode for making prediction\n",
    "    \n",
    "    print('*****Trained model loaded and evaluation mode activated*****')\n",
    "    \n",
    "    if standardization:\n",
    "        # loading model results and data preparataion tools\n",
    "        avg_train_losses = load(open(train_loss_path, 'rb'))\n",
    "        avg_valid_losses = load(open(validation_loss_path, 'rb'))\n",
    "        train_mean = load(open(train_mean_path, 'rb'))\n",
    "        train_std = load(open(train_std_path, 'rb'))\n",
    "        \n",
    "        return loaded_model, avg_train_losses, avg_valid_losses, train_mean, train_std\n",
    "        \n",
    "    else: # in case of normalization\n",
    "        if train_scaler_path is None:\n",
    "            raise Exception('##### Set a train_scaler_path. train_mean_path & std_mean_path have to be None #####')\n",
    "        # loading model results and data preparataion tools\n",
    "        avg_train_losses = load(open(train_loss_path, 'rb'))\n",
    "        avg_valid_losses = load(open(validation_loss_path, 'rb'))\n",
    "        train_scaler = load(open(train_scaler_path, 'rb'))\n",
    "\n",
    "        return loaded_model, avg_train_losses, avg_valid_losses, train_scaler\n",
    "\n",
    "    \n",
    "def predict(trained_nn_model, input_df, mean, std, standardize=True, normalize_scaler=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make prediction using the trained model.\n",
    "    \n",
    "    params:\n",
    "    trained_nn_model : Trained Neural Network Model instance.\n",
    "    input_df : Input dataframe.\n",
    "    mean : Training mean from the model training process used to scale the data.\n",
    "    std : Training std from the model training process used to scale the data.\n",
    "    standardize : Set to True (default) to standardize. Setting False will normalize the data.\n",
    "    normalize_scaler : Scaler from model training to use in data normalization. \n",
    "                       Only needed if standardize=False. Default set to None.\n",
    "    device : Default set to 'cuda'.\n",
    "    \n",
    "    returns: A tensor of model prediction.\n",
    "    \"\"\"\n",
    "    # converting dataframe to torch tensor\n",
    "    input_df = reindex_df(input_df)\n",
    "    input_tensor = torch.tensor(np.array(input_df)).float()\n",
    "    \n",
    "    if standardize: # standardization\n",
    "        input_tensor = (input_tensor - mean) /  std\n",
    "    else: # Normalization\n",
    "        input_array = normalize_scaler.transform(input_tensor)  # this is numpy array\n",
    "        input_tensor = torch.tensor(input_array).float() # converting normalized array to torch tensor again\n",
    "    \n",
    "    input_tensor = input_tensor.to(device) # moving to gpu in device='cuda'\n",
    "    \n",
    "    # prediction and moving to cpu again\n",
    "    prediction = trained_nn_model(input_tensor).cpu().detach()  \n",
    "    print('Model predeiction (type: torch tensor) generated and moved to cpu', '\\n')\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039998c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DL_model_to_generate_prediction(trained_dl_model, x_train, predictor_dataset, ref_raster,\n",
    "                                        standardization_mean, standardization_std,\n",
    "                                        output_folder, variable_name_keyword, remove_neg_values=False):\n",
    "    \"\"\"\n",
    "    Uses trained DL model to generate prediicted daily raster.\n",
    "    \n",
    "    params:\n",
    "    trained_ml_model : A trained ML model object. This will come from train_model() function.\n",
    "    x_train : x_train dataframe generated by split_train_val_test_set() function.\n",
    "    predictor_dataset : Filepath of parquet file of predictor dataset.\n",
    "    ref_raster : Filepath of a reference raster that will be used to rasterize. \n",
    "                 Use any TWC raster data as that is the target resolution.\n",
    "    standardization_mean : Training mean from the model training process used to scale the data.\n",
    "    standardization_std : Training std from the model training process used to scale the data.\n",
    "    output_folder : Output folder filepath to save predeicted daily raster.\n",
    "    variable_name_keyword : a keyword (str) that will be used to save predicted raster. For example 'total_precip'. \n",
    "                            Date will be added automatically from era5 dataset.\n",
    "                            \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    columns_trained_with = x_train.columns.tolist()\n",
    "    \n",
    "    era5_df = pd.read_parquet(predictor_dataset)\n",
    "    \n",
    "    era5_df['year'] = era5_df['date'].apply(lambda x: int(str(x)[0:4]))\n",
    "    era5_df['month'] = era5_df['date'].apply(lambda x: int(str(x)[4:6]))\n",
    "    era5_df['day'] = era5_df['date'].apply(lambda x: int(str(x)[6:8]))\n",
    "    \n",
    "    \n",
    "    # renaming total_precip column to total_precip_era5 as in  x_train its named as total_precip_era5\n",
    "    if 'total_precip' in list(era5_df.columns):\n",
    "        era5_df = era5_df.rename(columns={'total_precip': 'total_precip_era5'})\n",
    "    \n",
    "    # selecting columns for which model was trained with using xtrain columns\n",
    "    selected_era5_df = era5_df[columns_trained_with]\n",
    "    selected_era5_df = reindex_df(selected_era5_df)\n",
    "    \n",
    "    # generating model prediction for all dates in era5 dataframe\n",
    "    prediction_arr = predict(trained_nn_model=trained_dl_model, input_df=selected_era5_df, \n",
    "                             mean=standardization_mean, std=standardization_std, \n",
    "                             standardize=True, normalize_scaler=None, device='cuda')\n",
    "    prediction_arr = prediction_arr.numpy()\n",
    "    \n",
    "    # sometimes model can predict values less than zero when it can't happen (for example precipitation)\n",
    "    # removing that\n",
    "    if remove_neg_values:\n",
    "        prediction_arr = np.where(prediction_arr<0, 0, prediction_arr)\n",
    "    \n",
    "    print('Model prediction converted to numpy array')\n",
    "    \n",
    "    # Attaching date+lat+lon info with the predicted high resolution precipitation data\n",
    "    era5_dates_lat_lon = era5_df[['date', 'lat', 'lon']].reset_index()\n",
    "    prediction_df = pd.DataFrame(prediction_arr, columns=['high res. prediction'])\n",
    "    prediction_df = era5_dates_lat_lon.join(prediction_df, on='index')\n",
    "    \n",
    "    # creating prediction raster for each day\n",
    "    unique_dates = list(np.unique(era5_df['date']))\n",
    "    print('Generating model interpolated daily rasters...')\n",
    "    \n",
    "    for date in unique_dates:\n",
    "        pred_df_1day = prediction_df[prediction_df['date']==date] # prediction for single day\n",
    "        \n",
    "        # converting to geodataframe\n",
    "        pred_1day_gdf = gpd.GeoDataFrame(pred_df_1day, \n",
    "                                         geometry=gpd.points_from_xy(pred_df_1day.lon, pred_df_1day.lat))\n",
    "        \n",
    "        raster_name = f'{variable_name_keyword}_{date}.tif'\n",
    "        output_raster = os.path.join(output_folder, raster_name)\n",
    "        rasterize_shapefile(input_file=pred_1day_gdf, output_raster=output_raster, attribute='high res. prediction', \n",
    "                            ref_raster=ref_raster, date=None, grid_shapefile=None, \n",
    "                            merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999, paste_on_ref_raster=True)\n",
    "    print('All daily rasters generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
