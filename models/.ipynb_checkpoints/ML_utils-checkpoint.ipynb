{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f25c747",
   "metadata": {},
   "source": [
    "### Author: Md Fahim Hasan\n",
    "### Work Email: mdfahim.hasan@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import dask.dataframe as ddf\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold, RepeatedKFold\n",
    "\n",
    "from ipynb.fs.full.general_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdf8ab",
   "metadata": {},
   "source": [
    "# Read me\n",
    "\n",
    "__This scripts consist of functions required for building, tuning, analyzing machine learning models. Currently, two machine learning models (Random Forest and LGBM can be implemented by this scripts. The functions can be easily modified to incorporate model models into the script.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec55d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_df(df):\n",
    "    \"\"\"\n",
    "    Reindex dataframe based on column names.\n",
    "    Parameters:\n",
    "    df : Pandas dataframe.\n",
    "    Returns: Reindexed dataframe.\n",
    "    \"\"\"\n",
    "    sorted_columns = sorted(df.columns)\n",
    "    df = df.reindex(sorted_columns, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_train_val_test_set(input_dataset_fp, pred_attr, exclude_columns, test_perc=0.3, validation_perc=0.3, random_state=0,\n",
    "                             outdir=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation, and test data based on a train/test/validation ratio.\n",
    "    \n",
    "    parameters:\n",
    "    input_dataset_fp : Input csv/parquet file (with filepath) containing all the predictors.\n",
    "    pred_attr : Variable name which will be predicted. Defaults to 'Subsidence'.\n",
    "    exclude_columns : Tuple of columns that will not be included in training the fitted_model.\n",
    "    test_perc : The percentage of test dataset. Defaults to 0.3.\n",
    "    validation_perc : The percentage of validation dataset. Defaults to 0.3.\n",
    "    random_state : Seed value. Defaults to 0.\n",
    "    output_dir : Set a output directory if training and test dataset need to be saved. Defaults to None.\n",
    "    verbose : Set to True if want to print which columns are being dropped and which will be included in the model.\n",
    "    \n",
    "    returns: X_train, X_val, X_test, y_train, y_val, y_test arrays.\n",
    "    \"\"\"\n",
    "    if '.csv' in input_dataset_fp:\n",
    "        input_df = pd.read_csv(input_dataset_fp)\n",
    "    elif '.parquet' in input_dataset_fp:\n",
    "        input_df = pd.read_parquet(input_dataset_fp)\n",
    "    elif isinstance(input_dataset_fp, pd.DataFrame):\n",
    "        input_df = input_dataset_fp\n",
    "    else:\n",
    "        raise Exception('input_dataset_fp must be a .csv/.parquet file or pandas dataframe')\n",
    "\n",
    "        \n",
    "    drop_columns = exclude_columns + [pred_attr]  # dropping unwanted columns/colums that will not be used in model training\n",
    "    x = input_df.drop(columns=drop_columns)\n",
    "    y = input_df[pred_attr]\n",
    "    \n",
    "    # Reindexing for ensuring that columns go into the model in same serial every time\n",
    "    x = reindex_df(x)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Dropping Columns-', exclude_columns, '\\n')\n",
    "        print('Predictors:', x.columns)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_perc, random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=validation_perc, random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "    if outdir:\n",
    "        makedirs([outdir])\n",
    "        x_train_df = pd.DataFrame(x_train)\n",
    "        x_train_df.to_csv(os.path.join(outdir, 'x_train.csv'), index=False)\n",
    "\n",
    "        y_train_df = pd.DataFrame(y_train)\n",
    "        y_train_df.to_csv(os.path.join(outdir, 'y_train.csv'), index=False)\n",
    "\n",
    "        x_val_df = pd.DataFrame(x_val)\n",
    "        x_val_df.to_csv(os.path.join(outdir, 'x_val.csv'), index=False)\n",
    "\n",
    "        y_val_df = pd.DataFrame(y_val)\n",
    "        y_val_df.to_csv(os.path.join(outdir, 'y_val.csv'), index=False)\n",
    "        \n",
    "        x_test_df = pd.DataFrame(x_test)\n",
    "        x_test_df.to_csv(os.path.join(outdir, 'x_test.csv'), index=False)\n",
    "\n",
    "        y_test_df = pd.DataFrame(y_test)\n",
    "        y_test_df.to_csv(os.path.join(outdir, 'y_test.csv'), index=False)\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def split_train_val_test_set_by_date(input_dataset_fp, pred_attr, exclude_columns, test_perc=0.3, \n",
    "                                     validation_perc=0, random_state=0,\n",
    "                                     outdir=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation, and test data based on a ratio of dates.\n",
    "    \n",
    "    parameters:\n",
    "    input_dataset_fp : Input csv/parquet file (with filepath) containing all the predictors.\n",
    "    pred_attr : Variable name which will be predicted. Defaults to 'Subsidence'.\n",
    "    exclude_columns : Tuple of columns that will not be included in training the fitted_model.\n",
    "    test_perc : The percentage of test dataset. Defaults to 0.3.\n",
    "    validation_perc : The percentage of validation dataset. Default set to 0 (for no validation data).\n",
    "    random_state : Seed value. Defaults to 0.\n",
    "    output_dir : Set a output directory if training and test dataset need to be saved. Defaults to None.\n",
    "    verbose : Set to True if want to print which columns are being dropped and which will be included in the model.\n",
    "    \n",
    "    returns: X_train, X_val, X_test, y_train, y_val, y_test arrays and list of train_dates, validation_dates, and test_dates.\n",
    "    \"\"\"\n",
    "    if '.csv' in input_dataset_fp:\n",
    "        input_df = pd.read_csv(input_dataset_fp)\n",
    "    elif '.parquet' in input_dataset_fp:\n",
    "        input_df = pd.read_parquet(input_dataset_fp)\n",
    "    elif isinstance(input_dataset_fp, pd.DataFrame):\n",
    "        input_df = input_dataset_fp\n",
    "    else:\n",
    "        raise Exception('input_dataset_fp must be a .csv/.parquet file or pandas dataframe')\n",
    "\n",
    "    if exclude_columns is not None:\n",
    "        # dropping unwanted columns/colums that will not be used in model training\n",
    "        input_df = input_df.drop(columns=exclude_columns)  # should not drop date column\n",
    "    \n",
    "    # Permuting dates randomly and calculating train/test/validation length\n",
    "    unique_dates = input_df['date'].unique()\n",
    "    np.random.seed(0)  # setting a seed for same split every time\n",
    "    unique_dates = list(np.random.permutation(unique_dates)) \n",
    "    \n",
    "    train_length = round(len(unique_dates) * (1-test_perc-validation_perc))\n",
    "    test_length = round(len(unique_dates) * test_perc)\n",
    "    validation_length = round(len(unique_dates) * validation_perc)\n",
    "    \n",
    "    # seleting train/test/validation dataset based on randomly permuted dates \n",
    "    train_dates = unique_dates[:train_length]\n",
    "    test_dates = unique_dates[train_length: (train_length+test_length)]\n",
    "    validation_dates = unique_dates[(train_length+test_length): (train_length+test_length+validation_length)]\n",
    "    \n",
    "    train_df = input_df[input_df['date'].isin(train_dates)]\n",
    "    test_df = input_df[input_df['date'].isin(test_dates)]\n",
    "    validation_df = input_df[input_df['date'].isin(validation_dates)]\n",
    "    \n",
    "    x_train = reindex_df(train_df.drop(columns=['date', pred_attr]))\n",
    "    x_test = reindex_df(test_df.drop(columns=['date', pred_attr]))\n",
    "    x_val = reindex_df(validation_df.drop(columns=['date', pred_attr]))\n",
    "    \n",
    "    y_train = train_df[pred_attr]\n",
    "    y_test = test_df[pred_attr]\n",
    "    y_val = validation_df[pred_attr]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('Dropping Columns-', exclude_columns, '\\n')\n",
    "        print('Predictors:', x_train.columns)\n",
    "\n",
    "    if outdir:\n",
    "        makedirs([outdir])\n",
    "        x_train_df = pd.DataFrame(x_train)\n",
    "        x_train_df.to_csv(os.path.join(outdir, 'x_train.csv'), index=False)\n",
    "\n",
    "        y_train_df = pd.DataFrame(y_train)\n",
    "        y_train_df.to_csv(os.path.join(outdir, 'y_train.csv'), index=False)\n",
    "\n",
    "        x_val_df = pd.DataFrame(x_val)\n",
    "        x_val_df.to_csv(os.path.join(outdir, 'x_val.csv'), index=False)\n",
    "\n",
    "        y_val_df = pd.DataFrame(y_val)\n",
    "        y_val_df.to_csv(os.path.join(outdir, 'y_val.csv'), index=False)\n",
    "        \n",
    "        x_test_df = pd.DataFrame(x_test)\n",
    "        x_test_df.to_csv(os.path.join(outdir, 'x_test.csv'), index=False)\n",
    "\n",
    "        y_test_df = pd.DataFrame(y_test)\n",
    "        y_test_df.to_csv(os.path.join(outdir, 'y_test.csv'), index=False)\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, train_dates, validation_dates, test_dates\n",
    "\n",
    "\n",
    "def calculate_plot_mutual_information(x_train, y_train, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Calculate and plot mutual information (MI) betwwen predictor and trainig data.\n",
    "    \n",
    "    params:\n",
    "    x_train : x_train dataframe/csv/parquet.\n",
    "    y_train : y_train dataframe/csv/parquet.\n",
    "    exculde_columns : List of columns to exclude from x_train\n",
    "    \n",
    "    returns: a series holding mutual information (MI) score of the predictors. Plot of MI score of the predictors.\n",
    "    \"\"\"\n",
    "    if '.csv' in x_train:\n",
    "        x_train_df = pd.read_csv(x_train)\n",
    "        y_train_df = pd.read_csv(y_train)\n",
    "    \n",
    "    elif '.parquet' in x_train:\n",
    "        x_train_df = pd.read_parquetv(x_train)\n",
    "        y_train_df = pd.read_parquet(y_train)\n",
    "    else:\n",
    "        x_train_df = x_train\n",
    "        y_train_df = y_train    \n",
    "    \n",
    "    if exclude_columns is not None:\n",
    "        x_train_df = x_train_df.drop(columns=exclude_columns)\n",
    "        \n",
    "    mutual_info = mutual_info_regression(x_train_df, y_train_df, random_state=0)\n",
    "    mutual_info = pd.Series(mutual_info)\n",
    "    mutual_info.index = x_train_df.columns\n",
    "    \n",
    "    mutual_info = mutual_info.sort_values(ascending=False)\n",
    "    \n",
    "    mutual_info.plot.bar()\n",
    "    \n",
    "    return mutual_info\n",
    "\n",
    "\n",
    "def tune_hyperparameter(x, y, model='rf', n_folds=5, repeated_Kfold=False, n_repeats=5, \n",
    "                         random_search=True, n_iter=50, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization using RandomizedSearchCV/GridSearchCV.\n",
    "    \n",
    "    *****\n",
    "    good resources for building LGBM model\n",
    "    \n",
    "    https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "    *****\n",
    "\n",
    "    Parameters:\n",
    "    x_val, y_val : x_val (predictor) and y_val (target) arrays from split_train_test_ratio function.\n",
    "    model : Model for which hyperparameters will be tuned. Can only tune hyperparameters for RF regressor now. \n",
    "            Default set to 'rf'.\n",
    "    n_folds : Number of folds in K Fold CV. Default set to 5.\n",
    "    repeated_Kfold : Set to True if want to perform repeated Kfold. If False (default), will run for KFold.\n",
    "    n_repeats : If repeated_Kfold is True, number of repeats. Default set to 5.\n",
    "    random_search : Set to False if want to perform GridSearchCV. Default set to True to perform RandomizedSearchCV.\n",
    "    n_iter : Number of parameter combinations to be tested in RandomizedSearchCV if random_search is True.\n",
    "    n_jobs (rf/gbdt param): The number of jobs to run in parallel. Defaults to -1(using all processors).\n",
    "    \n",
    "    Returns : Optimized Hyperparameters.\n",
    "    \"\"\"\n",
    "    global regressor\n",
    "    # creating parameter dictionary\n",
    "    # hyperparameters are optimized from param_to_optimize_dict \n",
    "    # if hyperparamter optimization is off/not needed uses paramters from default_params_dict\n",
    "    # ******* after hyperparamter optimization, assign optimized values to default_params_dict***************\n",
    "    param_to_optimize_dict = {'rf': {'n_estimators': [100, 200, 300, 400, 500],\n",
    "                                     'max_depth': [7, 10, 15, 20],\n",
    "                                     'max_features': [6, 7, 10, 'log2'],\n",
    "                                     'min_samples_leaf': [5e-4, 1e-5, 1e-3, 6, 12, 20, 25],\n",
    "                                     'min_samples_split': [6, 7, 8, 10],\n",
    "                                     'max_samples': [None, 0.9, 0.8, 0.7]\n",
    "                                 },\n",
    "                          'lgbm': {'n_estimators': [100, 200, 250],\n",
    "                                   'max_depth': [7, 10, 13],\n",
    "                                   'learning_rate': [0.01, 0.05],\n",
    "                                   'subsample': [0.8, 0.7, 0.6],\n",
    "                                   'colsample_bytree': [0.8, 0.7],\n",
    "                                   'colsample_bynode': [0.8, 0.7],\n",
    "                                   'path_smooth': [0.1, 0.2, 0.3],\n",
    "                                   'num_leaves': [30, 50, 70],\n",
    "                                   'min_child_samples': [20, 25, 40],\n",
    "#                                    'data_sample_strategy' : ['goss']\n",
    "                                   }\n",
    "                          }\n",
    "    param_dict = param_to_optimize_dict\n",
    "    print('Model Name:', model)\n",
    "    pprint(param_dict[model])\n",
    "    \n",
    "    \n",
    "    # creating model structures\n",
    "    if model == 'rf':\n",
    "        regressor = RandomForestRegressor(random_state=0, n_jobs=n_jobs, bootstrap=True, oob_score=True)\n",
    "    \n",
    "    elif model == 'lgbm':\n",
    "        # the boosting_type has been set to 'goss' for faster training. Can use 'gdbt'/'dart'. Change params_dict accordingly\n",
    "        regressor = LGBMRegressor(tree_learner='serial', random_state=0, \n",
    "                                  deterministic=True, force_row_wise=True, n_jobs=n_jobs)\n",
    "    else:\n",
    "        raise Exception(\"model should be 'rf'/'lgbm'. Other types are not supported currently\")\n",
    "    \n",
    "    \n",
    "    scoring_metrics = ['r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_error']\n",
    "    \n",
    "    # Hyperparameter optimization block\n",
    "    # KFold or repeated KFold\n",
    "    if repeated_Kfold:\n",
    "        kfold = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=0)\n",
    "    else:\n",
    "        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    # Random search or grid search\n",
    "    if random_search:\n",
    "        fitted_model = RandomizedSearchCV(estimator=regressor, param_distributions=param_dict[model], n_iter=n_iter,\n",
    "                                cv=kfold, verbose=1, random_state=0, n_jobs=n_jobs,\n",
    "                                scoring=scoring_metrics, refit=scoring_metrics[1], return_train_score=True)\n",
    "    else:\n",
    "        fitted_model = GridSearchCV(estimator=regressor, param_grid=param_dict[model], cv=kfold, verbose=1, n_jobs=n_jobs,\n",
    "                          scoring=scoring_metrics, refit=scoring_metrics[1], return_train_score=True)\n",
    "\n",
    "    fitted_model.fit(x, y)  #  this will be x_val and y_val if tune_hyperparameter=True\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('best parameters for RMSE value', '\\n')\n",
    "    pprint(fitted_model.best_params_)\n",
    "    print('\\n')\n",
    "    print('Train Results....')\n",
    "    best_rmse = fitted_model.cv_results_['mean_train_neg_root_mean_squared_error'][fitted_model.best_index_]\n",
    "    best_r2 = fitted_model.cv_results_['mean_train_r2'][fitted_model.best_index_]\n",
    "    best_MAE = fitted_model.cv_results_['mean_train_neg_mean_absolute_error'][fitted_model.best_index_]\n",
    "    print('Best tuning-train RMSE: {:.3f}'.format(best_rmse))\n",
    "    print('Best tuning-train R2: {:.3f}'.format(best_r2))\n",
    "    print('Best tuning-train MAE: {:.3f}'.format(best_MAE))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Test Results....')\n",
    "    best_rmse = fitted_model.cv_results_['mean_test_neg_root_mean_squared_error'][fitted_model.best_index_]\n",
    "    best_r2 = fitted_model.cv_results_['mean_test_r2'][fitted_model.best_index_]\n",
    "    best_MAE = fitted_model.cv_results_['mean_test_neg_mean_absolute_error'][fitted_model.best_index_]\n",
    "    print('Best tuning-test RMSE: {:.3f}'.format(best_rmse))\n",
    "    print('Best tuning-test R2: {:.3f}'.format(best_r2))\n",
    "    print('Best tuning-test MAE: {:.3f}'.format(best_MAE))\n",
    "\n",
    "    if model == 'rf':\n",
    "        param_dict = {'n_estimators': fitted_model.best_params_['n_estimators'],\n",
    "                      'max_depth': fitted_model.best_params_['max_depth'],\n",
    "                      'max_features': fitted_model.best_params_['max_features'],\n",
    "                      'min_samples_leaf': fitted_model.best_params_['min_samples_leaf'],\n",
    "                      'min_samples_split': fitted_model.best_params_['min_samples_split'],\n",
    "                      'max_samples': fitted_model.best_params_['max_samples']\n",
    "                     }\n",
    "    elif model == 'lgbm':\n",
    "        param_dict = {'n_estimators': fitted_model.best_params_['n_estimators'],\n",
    "                      'max_depth': fitted_model.best_params_['max_depth'],\n",
    "                      'learning_rate': fitted_model.best_params_['learning_rate'],\n",
    "                      'subsample': fitted_model.best_params_['subsample'],\n",
    "                      'colsample_bytree': fitted_model.best_params_['colsample_bytree'],\n",
    "                      'colsample_bynode': fitted_model.best_params_['colsample_bynode'],\n",
    "                      'path_smooth': fitted_model.best_params_['path_smooth'],\n",
    "                      'num_leaves': fitted_model.best_params_['num_leaves'],\n",
    "                      'min_child_samples': fitted_model.best_params_['min_child_samples'],\n",
    "#                       'data_sample_strategy' : fitted_model.best_params_['data_sample_strategy']\n",
    "                     }\n",
    "        \n",
    "    return param_dict\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, params_dict, model='rf', n_jobs=-1,\n",
    "                load_model=False, save_model=False, save_folder=None, save_name=None,\n",
    "                tune_hyperparameters=False, repeated_Kfold=False, n_folds=5, n_iter=10, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Train a Random Forest Regressor model with given hyperparameters. \n",
    "\n",
    "    \n",
    "    *******\n",
    "    # To run the model without saving/loading the trained model, use load_model=False, save_model=False, save_folder=None, \n",
    "        save_name=None.\n",
    "    # To run the model and save it without loading any trained model, use load_model=False, save_model=True, \n",
    "        save_folder='give a folder path', save_name='give a name'.\n",
    "    # To load a pretrained model without running a new model, use load_model=True, save_model=False, \n",
    "        save_folder='give the saved folder path', save_name='give the saved name'.\n",
    "    *******\n",
    "\n",
    "    \n",
    "    params: \n",
    "    x_train, y_train : x_train (predictor) and y_train (target) arrays from split_train_test_ratio function.\n",
    "    model : str of type of model. The code can only run random forest regession model. Default set to 'rf'.\n",
    "    params_dict : ML model param dictionary. Currently supports 'random forest (RF)' and 'LGBM (lgbm)' Goss.\n",
    "                  **** when tuning hyperparameters set params_dict=None.\n",
    "                  For RF the dictionary should be like the folowing with user defined values- \n",
    "                    param_dict = {'n_estimators': 200,\n",
    "                                  'max_depth': 8,\n",
    "                                  'max_features': 'log2',\n",
    "                                  'min_samples_leaf': 6,\n",
    "                                  'min_samples_split': 4,\n",
    "                                  'max_samples': None\n",
    "                                 }\n",
    "                For LGBM the dictionary shoudl be like the folowing with user defined values- \n",
    "                    param_dict = {'n_estimators': 250,\n",
    "                                  'max_depth': 13,\n",
    "                                  'learning_rate': 0.05,\n",
    "                                  'subsample': 0.7,\n",
    "                                  'colsample_bytree': 0.8,\n",
    "                                  'colsample_bynode': 0.7 ,\n",
    "                                  'path_smooth': 0.2,\n",
    "                                  'num_leaves': 70,\n",
    "                                  'min_child_samples': 40,\n",
    "                                  'data_sample_strategy' : 'goss'\n",
    "                                  }\n",
    "                                 \n",
    "    n_jobs (rf/lgbm param): The number of jobs to run in parallel. Default set to to -1 (using all processors).\n",
    "    load_model : Set to True if want to load saved model. Default set to False.\n",
    "    save_model : Set to True if want to save model. Default set to False.\n",
    "    save_folder : Filepath of folder to save model. Default set to None for save_model=False..\n",
    "    save_name : Model's name to save with. Default set to None for save_model=False.\n",
    "    \n",
    "    returns: trained RF regression model.\n",
    "    \"\"\"\n",
    "    if not load_model:\n",
    "        start_time = timeit.default_timer()\n",
    "        if tune_hyperparameters:\n",
    "            params_dict= tune_hyperparameter(x=x_train, y=y_train, model=model, \n",
    "                                             n_folds=n_folds, repeated_Kfold=repeated_Kfold, \n",
    "                                             n_repeats=n_repeats, n_iter=n_iter,\n",
    "                                             random_search=True, n_jobs=n_jobs)\n",
    "        \n",
    "        if model == 'rf':\n",
    "            n_estimators = params_dict['n_estimators']\n",
    "            max_depth = params_dict['max_depth']\n",
    "            max_features = params_dict['max_features']\n",
    "            min_samples_leaf = params_dict['min_samples_leaf']\n",
    "            min_samples_split = params_dict['min_samples_split']\n",
    "            max_samples = params_dict['max_samples']\n",
    "            regressor_model = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, \n",
    "                                                    min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split,\n",
    "                                                    max_samples=max_samples, random_state=0, n_jobs=n_jobs, bootstrap=True, \n",
    "                                                    oob_score=True)\n",
    "        elif model == 'lgbm':\n",
    "            n_estimators = params_dict['n_estimators']\n",
    "            max_depth = params_dict['max_depth']\n",
    "            learning_rate = params_dict['learning_rate']\n",
    "            subsample = params_dict['subsample']\n",
    "            colsample_bytree = params_dict['colsample_bytree']\n",
    "            colsample_bynode = params_dict['colsample_bynode']\n",
    "            path_smooth = params_dict['path_smooth']\n",
    "            num_leaves = params_dict['num_leaves']\n",
    "            min_child_samples = params_dict['min_child_samples']\n",
    "            \n",
    "            data_sample_strategy = 'goss'  # using 'goss' by default here as we are using 'GOSS' Gradient boosting methods\n",
    "            \n",
    "            # Configuring the regressor with the parameters\n",
    "            regressor_model = LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                            subsample=subsample, colsample_bytree=colsample_bytree, \n",
    "                                            colsample_bynode=colsample_bynode, path_smooth=path_smooth, num_leaves=num_leaves,\n",
    "                                            min_child_samples=min_child_samples, data_sample_strategy=data_sample_strategy,\n",
    "                                            tree_learner='serial', random_state=0, \n",
    "                                            deterministic=True, force_row_wise=True, n_jobs=n_jobs)\n",
    "\n",
    "        trained_model = regressor_model.fit(x_train, y_train)\n",
    "        y_pred = trained_model.predict(x_train)\n",
    "        \n",
    "        print('Train RMSE = {:.3f}'.format(calculate_rmse(Y_pred=y_pred, Y_obsv=y_train)))\n",
    "        print('Train R2 = {:.3f}'.format(calculate_r2(Y_pred=y_pred, Y_obsv=y_train)))\n",
    "\n",
    "        if save_model:\n",
    "            makedirs([save_folder])\n",
    "            if '.joblib' not in save_name:\n",
    "                model_save_name =  save_name + '.joblib'\n",
    "            save_path = os.path.join(save_folder, model_save_name)\n",
    "            joblib.dump(trained_model, save_path, compress=3)\n",
    "        \n",
    "        # printing and saving runtime\n",
    "        end_time = timeit.default_timer()\n",
    "        runtime = (end_time-start_time)/60\n",
    "        run_str = f'model training time {runtime} mins'\n",
    "        print('model training time {:.3f} mins'.format(runtime))\n",
    "        \n",
    "        if tune_hyperparameters: # saving hyperparameter tuning + model training time\n",
    "            runtime_save = os.path.join(save_folder, save_name+'_tuning_training_runtime.txt')\n",
    "            with open(runtime_save, 'w') as file:\n",
    "                file.write(run_str)\n",
    "        else: # saving model training time with given parameters\n",
    "            runtime_save = os.path.join(save_folder, save_name+'_training_runtime.txt')\n",
    "            with open(runtime_save, 'w') as file:\n",
    "                file.write(run_str)\n",
    "        \n",
    "    else:\n",
    "        if '.joblib' not in save_name:\n",
    "            save_name =  save_name + '.joblib'\n",
    "        saved_model_path = os.path.join(save_folder, save_name)\n",
    "        trained_model = joblib.load(saved_model_path)\n",
    "        \n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def run_model_to_generate_prediction(trained_ml_model, x_train, predictor_era5_dataset, ref_raster, \n",
    "                                     output_folder, variable_name_keyword):\n",
    "    \"\"\"\n",
    "    Uses trained ML model to generate prediicted daily raster.\n",
    "    \n",
    "    params:\n",
    "    trained_ml_model : A trained ML model object. This will come from train_model() function.\n",
    "    x_train : x_train dataframe generated by split_train_val_test_set() function.\n",
    "    predictor_era5_dataset : Filepath of parquet file of ERA5 dataset.\n",
    "    ref_raster : Filepath of a reference raster that will be used to rasterize. \n",
    "                 Use any TWC raster data as that is the target resolution. \n",
    "    output_folder : Output folder filepath to save predeicted daily raster.\n",
    "    variable_name_keyword : a keyword (str) that will be used to save predicted raster. For example 'total_precip'. \n",
    "                            Date will be added automatically from era5 dataset.\n",
    "                            \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    columns_trained_with = x_train.columns.tolist()\n",
    "    \n",
    "    era5_df = pd.read_parquet(predictor_era5_dataset)\n",
    "    era5_df['year'] = era5_df['date'].apply(lambda x: int(str(x)[0:4]))\n",
    "    era5_df['month'] = era5_df['date'].apply(lambda x: int(str(x)[4:6]))\n",
    "    era5_df['day'] = era5_df['date'].apply(lambda x: int(str(x)[6:8]))\n",
    "\n",
    "    era5_rename_dict = {'max_temp': 'max_temp_era5',  \n",
    "                        'avg_Rhumid': 'avg_Rhumid_era5', \n",
    "                        'min_temp': 'min_temp_era5', \n",
    "                        'avg_wind_speed': 'avg_wind_speed_era5'}\n",
    "    era5_df = era5_df.rename(columns=era5_rename_dict)\n",
    "    \n",
    "    # selecting columns for which model was trained with using xtrain columns\n",
    "    selected_era5_df = era5_df[columns_trained_with]\n",
    "    selected_era5_df = reindex_df(selected_era5_df)\n",
    "    \n",
    "    prediction_arr= trained_ml_model.predict(selected_era5_df)\n",
    "    \n",
    "    # Attaching date+lat+lon info with the predicted high resolution precipitation data\n",
    "    era5_dates_lat_lon = era5_df[['date', 'lat', 'lon']].reset_index()\n",
    "    prediction_df = pd.DataFrame(prediction_arr, columns=['high res. prediction'])\n",
    "    prediction_df = era5_dates_lat_lon.join(prediction_df, on='index')\n",
    "    \n",
    "    # creating prediction raster for each day\n",
    "    unique_dates = list(np.unique(era5_df['date']))\n",
    "    print('Generating model interpolated daily rasters...')\n",
    "    \n",
    "    for date in unique_dates:\n",
    "        pred_df_1day = prediction_df[prediction_df['date']==date] # prediction for single day\n",
    "        \n",
    "        # converting to geodataframe\n",
    "        pred_1day_gdf = gpd.GeoDataFrame(pred_df_1day, \n",
    "                                         geometry=gpd.points_from_xy(pred_df_1day.lon, pred_df_1day.lat))\n",
    "        \n",
    "        raster_name = f'{variable_name_keyword}_{date}.tif'\n",
    "        output_raster = os.path.join(output_folder, raster_name)\n",
    "        rasterize_shapefile(input_file=pred_1day_gdf, output_raster=output_raster, attribute='high res. prediction', \n",
    "                            ref_raster=ref_raster, date=None, grid_shapefile=None, \n",
    "                            merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999, paste_on_ref_raster=True)\n",
    "    print('All daily rasters generated')\n",
    "        \n",
    "        \n",
    "def plot_predictor_importance(trained_model, x_train, outdir=None, predictor_imp_keyword='rf'):\n",
    "    x_train_df = pd.DataFrame(x_train)\n",
    "    col_labels = np.array(x_train_df.columns)\n",
    "    \n",
    "    importance = np.array(trained_model.feature_importances_)\n",
    "    imp_dict = {'feature_names': col_labels, 'feature_importance': importance}\n",
    "    imp_df = pd.DataFrame(imp_dict)\n",
    "    imp_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    sns.barplot(x=imp_df['feature_names'], y=imp_df['feature_importance'], palette='rocket')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Variable Importance')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.tight_layout()\n",
    "    if outdir is not None:\n",
    "        savepath = os.path.join(outdir, predictor_imp_keyword + '_pred_importance.png')\n",
    "        plt.savefig(savepath, dpi=600)\n",
    "        print('Feature importance plot saved')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669a300",
   "metadata": {},
   "source": [
    "## Error Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a9f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(Y_pred, Y_obsv):\n",
    "    \"\"\"\n",
    "    Calculates R2 value of model prediction vs observed data.\n",
    "\n",
    "    :param Y_pred: prediction array or panda series object.\n",
    "    :param Y_obsv: observed array or panda series object.\n",
    "\n",
    "    :return: R2 value.\n",
    "    \"\"\"\n",
    "    if isinstance(Y_pred, np.ndarray):\n",
    "        Y_pred = pd.Series(Y_pred)\n",
    "        r2_val = r2_score(Y_obsv, Y_pred)\n",
    "    else:  # in case of pandas series\n",
    "        r2_val = r2_score(Y_obsv, Y_pred)\n",
    "    return r2_val\n",
    "\n",
    "\n",
    "def calculate_rmse(Y_pred, Y_obsv):\n",
    "    \"\"\"\n",
    "    Calculates RMSE value of model prediction vs observed data.\n",
    "\n",
    "    :param Y_pred: prediction array or panda series object.\n",
    "    :param Y_obsv: observed array or panda series object.\n",
    "\n",
    "    :return: RMSE value.\n",
    "    \"\"\"\n",
    "    if isinstance(Y_pred, np.ndarray):\n",
    "        Y_pred = pd.Series(Y_pred)\n",
    "        rmse_val = mean_squared_error(y_true=Y_obsv, y_pred=Y_pred, squared=False)\n",
    "    else:  # in case of pandas series\n",
    "        rmse_val = mean_squared_error(y_true=Y_obsv, y_pred=Y_pred, squared=False)\n",
    "    return rmse_val\n",
    "\n",
    "\n",
    "def scatter_plot(Y_pred, Y_obsv, xlabel, ylabel, title=None, savedir=None, plot_name=None, alpha=0.5, color_format='o',\n",
    "                 axis_lim=None):\n",
    "    \"\"\"\n",
    "    Makes scatter plot of model prediction vs observed data.\n",
    "\n",
    "    :param Y_pred: flattened prediction array.\n",
    "    :param Y_obsv: flattened observed array.\n",
    "    :param savedir: filepath to save the plot.\n",
    "    :param plot_name: plot name to save with. Default set to None.\n",
    "    :param alpha: plot/scatter dots transparency level.\n",
    "    :param color_format: Color and plot type format. For example, for 'bo' 'b' means blue color and 'o' means dot plot.\n",
    "    :param axis_lim: A list of minimum and maximum values of x and y axis. \n",
    "                     Default set to None (will calculate and set xlim, ylim itself)\n",
    "    \n",
    "    :return: A scatter plot of model prediction vs observed data.\n",
    "    \"\"\"\n",
    "    if isinstance(Y_pred, np.ndarray):\n",
    "        Y_pred = pd.Series(Y_pred)\n",
    "    \n",
    "    min_value = min(Y_pred.min(), Y_obsv.min())\n",
    "    max_value = max(Y_pred.max(), Y_obsv.max())\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(Y_obsv, Y_pred, color_format, alpha=alpha)\n",
    "    ax.plot([0, 1], [0, 1], '-r', transform=ax.transAxes)\n",
    "    ax.set_xlabel(xlabel) # 'Observed'\n",
    "    ax.set_ylabel(ylabel) # 'Predicted'\n",
    "    \n",
    "    # setting x and y axis maximum and minimum value\n",
    "    if axis_lim:\n",
    "        ax.set_xlim(axis_lim)\n",
    "        ax.set_ylim(axis_lim)\n",
    "    else:    \n",
    "        ax.set_xlim([min_value, max_value])\n",
    "        ax.set_ylim([min_value, max_value])\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    r2_val = calculate_r2(Y_pred, Y_obsv)\n",
    "    r2_str = 'R2: {:.3f}'.format(r2_val)\n",
    "    ax.text(0.1, 0.9, s=r2_str, transform=ax.transAxes)\n",
    "\n",
    "    \n",
    "    if savedir is not None:\n",
    "        makedirs([savedir])\n",
    "        fig_loc = os.path.join(savedir, plot_name)\n",
    "        fig.savefig(fig_loc, dpi=300)\n",
    "        \n",
    "        \n",
    "def result_calc_test_dataset(trained_ml_model, list_test_dates, x_train, combined_dataset, era5_target_variable, \n",
    "                             twc_target_variable, output_csv):\n",
    "    \"\"\"\n",
    "    Calculate ML model performance on test dataset for individual dates.\n",
    "    \n",
    "    params:\n",
    "    trained_ml_model : Trained ml model.\n",
    "    list_test_dates : List of test dates.\n",
    "    x_train : x_train dataframe. \n",
    "    combined_dataset : Filepath of TWC and ERA5 cobined datase or the loaded dataframe.\n",
    "    era5_target_variable : target variable name from ERA5 dataset.\n",
    "    twc_target_variable : target variable name from TWC dataset.\n",
    "    output_csv : Filepath of output csv.\n",
    "    \n",
    "    \n",
    "    returns: Test results dataframe with before and after ML R2 values.\n",
    "    \"\"\"\n",
    "    if isinstance(combined_dataset, pd.DataFrame):\n",
    "        combined_df = combined_dataset\n",
    "    else:\n",
    "        combined_df = pd.read_parquet(combined_dataset)\n",
    "    \n",
    "    before_ML_R2 = []\n",
    "    after_ML_R2 = []\n",
    "    for date in list_test_dates:\n",
    "        # before ML\n",
    "        df_for_date = combined_df[combined_df.date==date]\n",
    "        era5_target_val = df_for_date[era5_target_variable]\n",
    "        twc_target_val = df_for_date[[twc_target_variable]].values\n",
    "        \n",
    "        r2_before = calculate_r2(Y_pred=era5_target_val, Y_obsv=twc_target_val)\n",
    "        before_ML_R2.append(r2_before)\n",
    "        \n",
    "        # after ML\n",
    "        ml_df = df_for_date[x_train.columns]\n",
    "        ml_df = reindex_df(ml_df)\n",
    "        \n",
    "        y_pred = trained_ml_model.predict(ml_df)\n",
    "        r2_after = calculate_r2(Y_pred=y_pred, Y_obsv=twc_target_val)\n",
    "        after_ML_R2.append(r2_after)\n",
    "\n",
    "    test_results = pd.DataFrame({'date': list_test_dates, 'before_ML_R2': before_ML_R2, 'after_ML_R2': after_ML_R2})\n",
    "    test_results.to_csv(output_csv)\n",
    "    \n",
    "    # prining results\n",
    "    print(f'{len(list_test_dates)} number of total dates in test dataset')\n",
    "    print(f'{len(test_results[test_results.after_ML_R2>0.6])} days have R2 > 0.6')\n",
    "    print(f'{len(test_results[test_results.after_ML_R2>0.5])} days have R2 > 0.5')\n",
    "    print(f'{len(test_results[test_results.after_ML_R2>0.3])} days have R2 > 0.3')\n",
    "    \n",
    "    return test_results      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15fabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
