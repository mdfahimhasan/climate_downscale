{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6760cc0",
   "metadata": {},
   "source": [
    "### Author: Md Fahim Hasan\n",
    "### Work Email: mdfahim.hasan@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3d0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from glob import glob\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as ddf\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.enums import MergeAlg, Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5b73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_data_value=-9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8c682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_arr_object(raster_file, rasterio_obj=False, band=1, get_file=True, change_dtype=True):\n",
    "    \"\"\"\n",
    "    Get raster array and raster file.\n",
    "\n",
    "    :param raster_file: Input raster filepath.\n",
    "    :param rasterio_obj: Set True if raster_file is a rasterio object.\n",
    "    :param band: Selected band to read. Default set to 1.\n",
    "    :param get_file: Set to False if raster file is not required.\n",
    "    :param change_dtype: Set to True if want to change raster data type to float. Default set to True.\n",
    "\n",
    "    :return: Raster numpy array and rasterio object file (get_file=True, rasterio_obj=False).\n",
    "    \"\"\"\n",
    "    if not rasterio_obj:\n",
    "        raster_file = rio.open(raster_file)\n",
    "    else:\n",
    "        get_file = False\n",
    "    raster_arr = raster_file.read(band)\n",
    "    if change_dtype:\n",
    "        raster_arr = raster_arr.astype(np.float32)\n",
    "        if raster_file.nodata:\n",
    "            raster_arr[np.isclose(raster_arr, raster_file.nodata)] = np.nan\n",
    "    if get_file:\n",
    "        return raster_arr, raster_file\n",
    "    else:\n",
    "        return raster_arr\n",
    "\n",
    "\n",
    "def write_array_to_raster(raster_arr, raster_file, transform, output_path, ref_file=None, nodata=no_data_value):\n",
    "    \"\"\"\n",
    "    Write raster array to Geotiff format.\n",
    "\n",
    "    :param raster_arr: Raster array data to be written.\n",
    "    :param raster_file: Original rasterio raster file containing geo-coordinates.\n",
    "    :param transform: Affine transformation matrix.\n",
    "    :param output_path: Output filepath.\n",
    "    :param ref_file: Write output raster considering parameters from reference raster file.\n",
    "    :param nodata: no_data_value set as -9999.\n",
    "\n",
    "    :return: Output filepath.\n",
    "    \"\"\"\n",
    "    if ref_file:\n",
    "        raster_file = rio.open(ref_file)\n",
    "        transform = raster_file.transform\n",
    "\n",
    "    with rio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=raster_arr.shape[0],\n",
    "            width=raster_arr.shape[1],\n",
    "            dtype=raster_arr.dtype,\n",
    "            count=1,  # raster_file.count\n",
    "            crs=raster_file.crs,\n",
    "            transform=transform,\n",
    "            nodata=nodata\n",
    "    ) as dst:\n",
    "        dst.write(raster_arr, 1) #raster_file.count\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def rasterize_shapefile(input_file, output_raster, attribute, ref_raster, date=None, grid_shapefile=None, \n",
    "                        merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999, paste_on_ref_raster=False):\n",
    "    \"\"\"\n",
    "    rasterize shapefile.\n",
    "    \n",
    "    params:\n",
    "    input_file : Filepath of parquet (or already read geodataframe) file with the attribute. If parquet file should have a \n",
    "                 grid_id column to be matched with the grid_shapefile.\n",
    "    output_raster : Filepath of output raster file.\n",
    "    grid_shapefile : If parquet file in given as input_file, filepath of grid/geometry shapefile. \n",
    "                     Should have a grid_id column to be matched with the parquet file.\n",
    "                     Default set to None so that \n",
    "    attribute : Attriute column (in str) of parquet/gdf to rasterize. \n",
    "    ref_raster : Reference raster to be used in assigning rasterization shape, transform.\n",
    "    date : Default set to None. Set to str of date if want to filter the parquet/gdf for a specific date (specially for weather data).\n",
    "    merge_alg : Rasterio merge algorithm. Can be either MergeAlg.replace or MergeAlg.add. Default set to MergeAlg.replace.\n",
    "    dtype : Data type of raster. Default set to Float32.\n",
    "    no_data_value : No data value assigned to raster. Default set to -9999.\n",
    "    paste_on_ref_raster : Set to True if want to paste rasterized values on a reference rasters. In this case, the raster will\n",
    "                          have similar no_data pixels as reference rasters.              \n",
    "    \n",
    "    returns: The output raster filepath.\n",
    "    \"\"\"\n",
    "    if 'parquet' in input_file: # if parquet file used as input_file\n",
    "        gdf = read_parquet_as_geodataframe(parquet_file=input_file, grid_geometry_file=grid_shapefile, save=False)\n",
    "    else: # if geodataframe used as input_file\n",
    "        gdf = input_file\n",
    "        \n",
    "    if date is not None:\n",
    "        gdf = gdf[gdf['date'] == date]\n",
    "\n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    input_shape = ((geom, value) for geom, value in zip(gdf.geometry, gdf[attribute]))\n",
    "\n",
    "    raster_arr = rasterize(shapes=input_shape, out_shape=ref_arr.shape, fill=no_data_value, out=None, \n",
    "                           transform=ref_file.transform, all_touched=True, \n",
    "                           default_value=no_data_value, dtype=dtype, merge_alg=merge_alg)\n",
    "    \n",
    "    if paste_on_ref_raster:\n",
    "        raster_arr[np.isnan(ref_arr)] = no_data_value\n",
    "    \n",
    "    write_array_to_raster(raster_arr, raster_file=ref_file, transform=ref_file.transform, \n",
    "                          output_path=output_raster, ref_file=None, nodata=no_data_value)\n",
    "\n",
    "    return output_raster\n",
    "\n",
    "\n",
    "def resample_raster_based_on_ref_raster(input_raster, ref_raster, output_dir, raster_name, resampling_alg=Resampling.bilinear,\n",
    "                                        paste_value_on_ref_raster=False):\n",
    "    \"\"\"\n",
    "    Resample raster based on a refernce raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Filepath of input raster to resample.\n",
    "    ref_raster : Filepath of input raster to be used in determining resample height/width/affine transformation/crs/dtype/nodata.\n",
    "    output_raster : Filepath of resampled output raster.\n",
    "    resampling_alg : resampling algorithm. Can be Resampling.nearest/ Resampling.bilinear/Resampling.cubic or \n",
    "                     any resampling algorith rasterio supports Default set to Resampling.bilinear.\n",
    "    paste_value_on_ref_raster : Set to True if want to have nodata pixels on the resampled raster similar to reference raster. \n",
    "    \n",
    "    returns: The resampled output raster filepath.\n",
    "    \"\"\"\n",
    "    makedirs([output_dir])\n",
    "    \n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    \n",
    "    # target shape. use a reference raster (created using GIS for a specific region) to decide.\n",
    "    resampled_height, resampled_width = ref_arr.shape\n",
    "\n",
    "    with rio.open(input_raster) as dataset:\n",
    "        # resample data to target shape\n",
    "        resampled_arr = dataset.read(1,\n",
    "                            out_shape=(1,\n",
    "                                       resampled_height,\n",
    "                                       resampled_width),\n",
    "                            resampling=resampling_alg)\n",
    "\n",
    "        resampled_arr = resampled_arr.squeeze() # removing the 1 (for count) from the dimension\n",
    "        \n",
    "        if paste_value_on_ref_raster:\n",
    "            resampled_arr = np.where(np.isnan(ref_arr), -9999, resampled_arr)\n",
    "        \n",
    "        # Saving the resampled data\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "        write_array_to_raster(raster_arr=resampled_arr, raster_file=ref_file, \n",
    "                              transform=ref_file.transform, output_path=output_raster, \n",
    "                              ref_file=None, nodata=-9999)\n",
    "        \n",
    "        return output_raster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb1fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(directory_list):\n",
    "    \"\"\"\n",
    "    Make directory (if not exists) from a list of directory.\n",
    "\n",
    "    :param directory_list: A list of directories to create.\n",
    "\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    for directory in directory_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "            \n",
    "def make_lat_lon_array_from_raster(input_raster, nodata=-9999):\n",
    "    \"\"\"\n",
    "    Make lat, lon array for each pixel using the input raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Input raster filepath that will be used as reference raster.\n",
    "    nodata : No data value. Default set to -9999.\n",
    "    \n",
    "    returns: Lat, lon array with nan value (-9999) applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    raster_file = rio.open(input_raster)\n",
    "    raster_arr = raster_file.read(1)\n",
    "\n",
    "    # calculating lat, lon of each cells centroid\n",
    "    height, width = raster_arr.shape\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rio.transform.xy(rows=rows, cols=cols, transform=raster_file.transform)\n",
    "    \n",
    "    # flattening and reshaping to the input_raster's array size\n",
    "    xs = np.array(xs).flatten()\n",
    "    ys = np.array(ys).flatten()\n",
    "    \n",
    "    lon_arr = xs.reshape(raster_arr.shape)\n",
    "    lat_arr = ys.reshape(raster_arr.shape)\n",
    "    \n",
    "    # assigning no_data_value\n",
    "    lon_arr[raster_arr==nodata] = nodata\n",
    "    lat_arr[raster_arr==nodata] = nodata\n",
    "    \n",
    "    return lon_arr, lat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd14a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_twc_daily_data_to_dataframe(savename, twc_data_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Compile twc daily data in a dataframe. All datasets have to be of same shape.\n",
    "    \n",
    "    params:\n",
    "    savename : (str) Name of the output parquet file.\n",
    "    twc_data_folder : TWC daily data main folder. The code will automatically get data in the sub-directories.\n",
    "    output_folder : Main output folder. The code will automatically save data in the individual sub-directories.\n",
    "    \n",
    "    returns: compiled TWC dataframe.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    # making list of variables in the twc raster data folder\n",
    "    variable_names = os.listdir(twc_data_folder)\n",
    "    variable_paths = [os.path.join(twc_data_folder, folder) for folder in variable_names]\n",
    "    \n",
    "    # will be used to multiply lat/lon data \n",
    "    # the condition is added because we have to process different TWC data and not all has max_temp or total_precip\n",
    "    num_days = len(glob(os.path.join(twc_data_folder, 'max_temp', '*.tif')))\n",
    "    if num_days > 1:\n",
    "        pass\n",
    "    else:\n",
    "        num_days = len(glob(os.path.join(twc_data_folder, 'total_precip', '*.tif')))\n",
    "    \n",
    "    \n",
    "    variable_dict = {}  # a dictionary where daily dataset values will be stored under variable_name \n",
    "    \n",
    "    for path in variable_paths:\n",
    "        all_data = glob(os.path.join(path, '*.tif')) # making list of all dataset in a particular folder\n",
    "        all_data = sorted(all_data)  # to sort data by date so that all variables are compiled in same serial\n",
    "        \n",
    "        variable_name = os.path.basename(path).split('.')[0]  # extracted variable name \n",
    "        if variable_name not in ['lat', 'lon']:\n",
    "            print(f'compiling data for {variable_name}...')\n",
    "\n",
    "            # loop for reading datasets and storing pixel info in a dictionary\n",
    "            for count, data in enumerate(all_data):\n",
    "                # retrieving and storing data\n",
    "                data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "\n",
    "                # extarcting, formatting, and storing date info\n",
    "                date = os.path.basename(data).split('.')[0].split('_')[-1]\n",
    "                year, month, day = date[:4], date[4:6], date[6:]\n",
    "\n",
    "                len_data = len(data_arr)  # number of pixels in each daily dataset (array)\n",
    "                year_list = [int(year)] * len_data\n",
    "                month_list = [int(month)] * len_data\n",
    "                day_list = [int(day)] * len_data\n",
    "                date_list = [int(date)] * len_data\n",
    "\n",
    "                # Assigning all values to the variable_dict\n",
    "                if count == 0:\n",
    "                    variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "                    variable_dict['date'] = date_list\n",
    "                    variable_dict['year'] = year_list\n",
    "                    variable_dict['month'] = month_list\n",
    "                    variable_dict['day'] = day_list\n",
    "\n",
    "                else:\n",
    "                    variable_dict[variable_name].extend(list(data_arr))  # storing flattened data in a dictionary under the variable name)\n",
    "                    variable_dict['date'].extend(date_list)\n",
    "                    variable_dict['year'].extend(year_list)\n",
    "                    variable_dict['month'].extend(month_list)\n",
    "                    variable_dict['day'].extend(day_list)\n",
    "        else: # for lat/lon\n",
    "            data = glob(os.path.join(path, '*.tif'))[0]##############\n",
    "            data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "            data_duplicated_for_days = list(data_arr) * num_days\n",
    "            variable_dict[variable_name] = data_duplicated_for_days\n",
    "\n",
    "    twc_variable_df = pd.DataFrame(variable_dict)\n",
    "    twc_variable_ddf = ddf.from_pandas(twc_variable_df, npartitions=20)\n",
    "    twc_variable_ddf = twc_variable_ddf.dropna()\n",
    "    twc_variable_ddf = twc_variable_ddf.reset_index()\n",
    "    \n",
    "    if '.parquet' in savename:\n",
    "        output_parquet_file = os.path.join(output_folder, savename)\n",
    "    else:\n",
    "        output_parquet_file = os.path.join(output_folder, savename+'.parquet')\n",
    "    \n",
    "    twc_variable_ddf.to_parquet(output_parquet_file)\n",
    "    \n",
    "    end_time = time()\n",
    "    print('time taken', round((end_time-start_time)/60, 3), 'mins')\n",
    "\n",
    "    return twc_variable_ddf\n",
    "\n",
    "\n",
    "\n",
    "def combine_twc_era5_datasets(twc_dataset, era5_dataset, output_file, merge_on=['date', 'lat', 'lon'], how='inner'):\n",
    "    \"\"\"\n",
    "    Combine twc era5 datasets.\n",
    "    \n",
    "    params:\n",
    "    twc_dataset : TWC dataframe filepath or dataframe.\n",
    "    era5_datset : ERA5 dataframe filepath or dataframe.\n",
    "    output_file : Combined TWC and ERA5 dataframe output filepath.\n",
    "    merge_on : List of columns to use in dataframe merging. Default set to ['date', 'lat', 'lon'].\n",
    "    how : Type of merging. Default set to 'inner'.\n",
    "    \n",
    "    returns: Combined TWC and ERA5 dataframe.\n",
    "    \"\"\"\n",
    "    if isinstance(twc_dataset, pd.DataFrame):\n",
    "        twc_df = twc_dataset\n",
    "        era5_df = era5_dataset\n",
    "    else:\n",
    "        twc_df = pd.read_parquet(twc_dataset)\n",
    "        era5_df = pd.read_parquet(era5_dataset)\n",
    "    \n",
    "    twc_era5_combined = twc_df.merge(era5_df, on=merge_on, suffixes=('_twc', '_era5'), how=how)\n",
    "    twc_era5_combined.to_parquet(output_file)\n",
    "\n",
    "    return twc_era5_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f0d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(input_dir_file, copy_dir, search_by='*.tif', rename=None):\n",
    "    \"\"\"\n",
    "    Copy a file to the specified directory.\n",
    "\n",
    "    :param input_dir_file: File path of input directory/ Path of the file to copy.\n",
    "    :param copy_dir: File path of copy directory.\n",
    "    :param search_by: Default set to '*.tif'.\n",
    "    :param rename: New name of file if required. Default set to None. DOesn't work if a directory is being copied.\n",
    "\n",
    "    :returns: File path of copied file.\n",
    "    \"\"\"\n",
    "    makedirs([copy_dir])\n",
    "    if '.tif' not in input_dir_file:\n",
    "        input_files = glob(os.path.join(input_dir_file, search_by))\n",
    "\n",
    "        for each in input_files:\n",
    "            file_name = os.path.basename(each)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "            shutil.copyfile(each, copy_file)\n",
    "\n",
    "    else:\n",
    "        if rename is not None:\n",
    "            copy_file = os.path.join(copy_dir, f'{rename}.tif')\n",
    "        else:\n",
    "            file_name = os.path.basename(input_dir_file)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "        shutil.copyfile(input_dir_file, copy_file)\n",
    "\n",
    "    return copy_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84716419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dated_images_rasters(list_of_images, num_cols, figsize=(8, 6), shapefile=None,\n",
    "                              title=None):\n",
    "    \"\"\"\n",
    "    Plot images with date information. Date should be in the end like this '20211231'\n",
    "    \n",
    "    params:\n",
    "    list_of_images : List of images filepath to plot.\n",
    "    num_cols : number of columns in the plot.\n",
    "    figsize : Figsise. Default set to (8, 6).\n",
    "    shapefile : Shapefile filepath if want to show in plot. Default set to None.\n",
    "    title : Title string. Default set to None.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    # setting number of row\n",
    "    if len(list_of_images)%num_cols == 0:\n",
    "        num_rows = len(list_of_images)//num_cols\n",
    "    else:\n",
    "        num_rows = len(list_of_images)//num_cols + 1\n",
    "    \n",
    "    if shapefile is not None:\n",
    "        gdf = gpd.read_file(shapefile)\n",
    "    \n",
    "    # define the figures and axes\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)\n",
    "    \n",
    "    for image in list_of_images:\n",
    "        # extarcting date\n",
    "        date = os.path.basename(image).split('.')[0].split('_')[-1]\n",
    "        year, month, date = date[:4], date[4:6], date[6:]\n",
    "        date = f'{year}_{month}_{date}'\n",
    "        \n",
    "        img = rio.open(image)\n",
    "        \n",
    "        row = list_of_images.index(image)//num_cols\n",
    "        col = list_of_images.index(image)%num_cols\n",
    "        \n",
    "        if shapefile is not None:\n",
    "            gdf.plot(facecolor='none', edgecolor='black', ax=ax[row, col])\n",
    "        \n",
    "        show(img, ax=ax[row, col], cmap='Spectral_r')\n",
    "        ax[row, col].set_title(f'Date {date}', fontsize=9)\n",
    "        \n",
    "    if title is not None:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "        \n",
    "def plot_dated_images_scatter(images_list_data1, images_list_data2, num_cols, figsize=(8, 6), \n",
    "                                      title=None, xlabel=None, ylabel=None):\n",
    "    \"\"\"\n",
    "    Plot scatter plot of two list of images.Each image should have date info the end like this '20211231'\n",
    "    \n",
    "    params:\n",
    "    images_list_data1 : 1st List of images filepath to plot.\n",
    "    images_list_data2 : 2nd List of images filepath to plot.\n",
    "    num_cols : number of columns in the plot.\n",
    "    figsize : Figsise. Default set to (8, 6).\n",
    "    title, xlabel, ylabel : Title, xlabel, ylabel string. Default set to None.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # setting number of row\n",
    "    if len(images_list_data1)%num_cols == 0:\n",
    "        num_rows = len(images_list_data1)//num_cols\n",
    "    else:\n",
    "        num_rows = len(images_list_data1)//num_cols + 1\n",
    "    \n",
    "    # define the figures and axes\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)\n",
    "    \n",
    "    for image1 in images_list_data1:\n",
    "        # extarcting date\n",
    "        date = os.path.basename(image1).split('.')[0].split('_')[-1]\n",
    "        \n",
    "        # loop to find data of matching dates in 2nd image list \n",
    "        for image2 in images_list_data2:    \n",
    "            if date in image2:\n",
    "                # formatting date\n",
    "                year, month, dateee = date[:4], date[4:6], date[6:]\n",
    "                date_fmt = f'{year}_{month}_{dateee}'\n",
    "                \n",
    "                # opening matching images\n",
    "                img1 = rio.open(image1).read(1).flatten()\n",
    "                img2 = rio.open(image2).read(1).flatten()\n",
    "                \n",
    "                # filtering out nan or -9999 values\n",
    "                img_df = pd.DataFrame({'img1': img1, 'img2': img2})\n",
    "                img_df = img_df.dropna()\n",
    "                img_df = img_df[(img_df['img1'] != -9999) & (img_df['img2'] != -9999)]\n",
    "                \n",
    "                # calculating r2 score between the variables\n",
    "                r2_val = r2_score(img_df.img1, img_df.img2)  \n",
    "                \n",
    "                # deciding [row, col] index of the plot\n",
    "                row = images_list_data1.index(image1)//num_cols\n",
    "                col = images_list_data1.index(image1)%num_cols\n",
    "        \n",
    "                # scatter plot\n",
    "                ax[row, col].plot(img_df.img1, img_df.img2, 'b.', alpha=0.05)\n",
    "                \n",
    "                # setting xlabel, ylabel min max\n",
    "                maxx = max(img_df.img1.max(), img_df.img2.max())\n",
    "                maxy = max(img_df.img2.max(), img_df.img2.max())\n",
    "                minx = min(img_df.img1.min(), img_df.img2.min())\n",
    "                miny = min(img_df.img1.min(), img_df.img2.min())\n",
    "                \n",
    "                ax[row, col].set_xlim(minx, maxx)\n",
    "                ax[row, col].set_ylim(miny, maxx)\n",
    "                \n",
    "                # including R2 value\n",
    "                if maxx>5:\n",
    "                    min_pos = minx+0.1\n",
    "                    max_pos = maxx-1\n",
    "                else:\n",
    "                    min_pos = 0.01\n",
    "                    max_pos = maxx-0.3\n",
    "                ax[row, col].annotate(text='R2 = {:.3f}'.format(r2_val), xy=(min_pos, max_pos), fontsize=8)\n",
    "                \n",
    "                # setting 1:1 line\n",
    "                pt = (0, 0)\n",
    "                ax[row, col].axline(pt, slope=1, color='red', linewidth=0.3)\n",
    "                \n",
    "                # setting individual plot title\n",
    "                ax[row, col].set_title(f'Date {date_fmt}', fontsize=9)\n",
    "                \n",
    "                # setting xlabel and ylabel\n",
    "                if (xlabel is not None) & (ylabel is not None):\n",
    "                    ax[row, col].set_xlabel(xlabel, fontsize=9)\n",
    "                    ax[row, col].set_ylabel(ylabel, fontsize=9)\n",
    "                \n",
    "        # setting overall plot title \n",
    "        if title is not None:\n",
    "            plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "\n",
    "    \n",
    "def plot_era5_twc_downscaled_rasters(era5_data, twc_data, downscaled_data,  \n",
    "                                     title=None, suptitle_pos=0.75,\n",
    "                                     xlabels=['ERA5 Data (28km res.)',\n",
    "                                              'TWC Data (4km res.)',\n",
    "                                              'Model Interpolated Data (4km res.)'],\n",
    "                                     cbar_axes_pos=[1, 0.27, 0.02, 0.42]):\n",
    "    \"\"\"\n",
    "    Plot ERA5 (original 28km), TWC, and model-downscaled datasets.\n",
    "    \n",
    "    :param era5_data : Filepath of a single day data from ERA5 datasets.\n",
    "    :param twc_data : Filepath of a single day data from TWC datasets.\n",
    "    :param downscaled data : Filepath of a single day data from downscaled datasets.\n",
    "    :param title : A title to use in the plot. Default set to None.\n",
    "    :param suptitle_pos : A float value that designates suptitle position in the y-direction. Default set to 0.75.\n",
    "    :param xlabels : A list of xlabel for the three rasters. Default set to: \n",
    "                    ['ERA5 Data (28km res.)', 'TWC Data (4km res.)', 'Model Interpolated Data (4km res.)']\n",
    "    :param cbar_axes_pos : A list of float values indicating colorbar position. Change it to place the colorbar perfectly.\n",
    "                           Default set to [1, 0.27, 0.02, 0.42].\n",
    "    \n",
    "    returns: Plot of ERA5, TWC, and downscaled raster with same colorbar.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Data read and replacing -9999 values with np.nan \n",
    "    era5_arr = rio.open(era5_data).read(1)\n",
    "    era5_arr = np.where(era5_arr < -100, np.nan, era5_arr)\n",
    "    \n",
    "    twc_arr = rio.open(twc_data).read(1)\n",
    "    twc_arr = np.where(twc_arr < -100, np.nan, twc_arr)\n",
    "\n",
    "    downscaled_arr = rio.open(downscaled_data).read(1)\n",
    "    downscaled_arr = np.where(downscaled_arr < -100, np.nan, downscaled_arr)\n",
    "\n",
    "    arr_to_plot = [era5_arr, twc_arr, downscaled_arr]\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        im = ax.imshow(arr_to_plot[i], cmap='RdYlGn_r')\n",
    "        ax.set_xlabel(xlabels[i])\n",
    "    \n",
    "    \n",
    "    # Title\n",
    "    fig.suptitle(title, y=suptitle_pos, fontsize=16)\n",
    "    \n",
    "    # Placing colorbar\n",
    "    cbar_ax = fig.add_axes(cbar_axes_pos)\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7737ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
