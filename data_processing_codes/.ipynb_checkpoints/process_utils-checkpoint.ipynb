{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a357e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h3\n",
    "import json\n",
    "import boto3\n",
    "import shutil\n",
    "import shapely\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gemgis as gg\n",
    "from time import time\n",
    "from glob import glob\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as ddf\n",
    "from rasterio.plot import show\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.fill import fillnodata\n",
    "from sklearn.metrics import r2_score\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.enums import MergeAlg, Resampling\n",
    "from shapely.geometry import box, mapping, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd01823",
   "metadata": {},
   "source": [
    "# Read me\n",
    "\n",
    "### __This scripts consist of all functions required for processing weather, soil, and elevation data that have been queried from CSW. Functions include shapefile to raster conversion, reading/writing raster data, resample and masking raster data etc..__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fd27b",
   "metadata": {},
   "source": [
    "## Refernce rasters and nodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b4b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_california_28km_refraster = '../reference_rasters/cities_California_28km_ref_raster.tif'\n",
    "cities_california_8km_refraster = '../reference_rasters/cities_California_8km_ref_raster.tif' \n",
    "cities_california_4km_refraster = '../reference_rasters/cities_California_4km_ref_raster.tif'  \n",
    "cities_California_100m_refraster = '../reference_rasters/cities_California_100m_ref_raster.tif'\n",
    "\n",
    "cities_california_buffer_28km_refraster = '../reference_rasters/cities_California_buffer_28km_ref_raster.tif' \n",
    "cities_california_buffer_8km_refraster = '../reference_rasters/cities_California_buffer_8km_ref_raster.tif'   \n",
    "cities_california_buffer_4km_refraster = '../reference_rasters/cities_California_buffer_4km_ref_raster.tif'   \n",
    "\n",
    "no_data_value = -9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca619a29",
   "metadata": {},
   "source": [
    "## Raster / Vector operations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd4c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_arr_object(raster_file, rasterio_obj=False, band=1, get_file=True, change_dtype=True):\n",
    "    \"\"\"\n",
    "    Get raster array and raster file.\n",
    "\n",
    "    :param raster_file: Input raster filepath.\n",
    "    :param rasterio_obj: Set True if raster_file is a rasterio object.\n",
    "    :param band: Selected band to read. Default set to 1.\n",
    "    :param get_file: Set to False if raster file is not required.\n",
    "    :param change_dtype: Set to True if want to change raster data type to float. Default set to True.\n",
    "\n",
    "    :return: Raster numpy array and rasterio object file (get_file=True, rasterio_obj=False).\n",
    "    \"\"\"\n",
    "    if not rasterio_obj:\n",
    "        raster_file = rio.open(raster_file)\n",
    "    else:\n",
    "        get_file = False\n",
    "    raster_arr = raster_file.read(band)\n",
    "    if change_dtype:\n",
    "        raster_arr = raster_arr.astype(np.float32)\n",
    "        if raster_file.nodata:\n",
    "            raster_arr[np.isclose(raster_arr, raster_file.nodata)] = np.nan\n",
    "    if get_file:\n",
    "        return raster_arr, raster_file\n",
    "    else:\n",
    "        return raster_arr\n",
    "\n",
    "\n",
    "def write_array_to_raster(raster_arr, raster_file, transform, output_path, ref_file=None, nodata=no_data_value):\n",
    "    \"\"\"\n",
    "    Write raster array to Geotiff format.\n",
    "\n",
    "    :param raster_arr: Raster array data to be written.\n",
    "    :param raster_file: Original rasterio raster file containing geo-coordinates.\n",
    "    :param transform: Affine transformation matrix.\n",
    "    :param output_path: Output filepath.\n",
    "    :param ref_file: Write output raster considering parameters from reference raster file.\n",
    "    :param nodata: no_data_value set as -9999.\n",
    "\n",
    "    :return: Output filepath.\n",
    "    \"\"\"\n",
    "    if ref_file:\n",
    "        raster_file = rio.open(ref_file)\n",
    "        transform = raster_file.transform\n",
    "\n",
    "    with rio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=raster_arr.shape[0],\n",
    "            width=raster_arr.shape[1],\n",
    "            dtype=raster_arr.dtype,\n",
    "            count=1,  # raster_file.count\n",
    "            crs=raster_file.crs,\n",
    "            transform=transform,\n",
    "            nodata=nodata\n",
    "    ) as dst:\n",
    "        dst.write(raster_arr, 1) #raster_file.count\n",
    "\n",
    "    return output_path\n",
    "   \n",
    "    \n",
    "def rasterize_shapefile(input_file, output_raster, attribute, ref_raster, date=None, grid_shapefile=None, \n",
    "                        merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999, paste_on_ref_raster=False):\n",
    "    \"\"\"\n",
    "    rasterize shapefile.\n",
    "    \n",
    "    params:\n",
    "    input_file : Filepath of parquet (or already read geodataframe) file with the attribute. If parquet file should have a \n",
    "                 grid_id column to be matched with the grid_shapefile.\n",
    "    output_raster : Filepath of output raster file.\n",
    "    grid_shapefile : If parquet file in given as input_file, filepath of grid/geometry shapefile. \n",
    "                     Should have a grid_id column to be matched with the parquet file.\n",
    "                     Default set to None so that \n",
    "    attribute : Attriute column (in str) of parquet/gdf to rasterize. \n",
    "    ref_raster : Reference raster to be used in assigning rasterization shape, transform.\n",
    "    date : Default set to None. Set to str of date if want to filter the parquet/gdf for a specific date (specially for weather data).\n",
    "    merge_alg : Rasterio merge algorithm. Can be either MergeAlg.replace (to replace value) or \n",
    "                MergeAlg.add (to add value to existing value). Default set to MergeAlg.replace.\n",
    "    dtype : Data type of raster. Default set to Float32.\n",
    "    no_data_value : No data value assigned to raster. Default set to -9999.\n",
    "    paste_on_ref_raster : Set to True if want to paste rasterized values on a reference rasters. In this case, the raster will\n",
    "                          have similar no_data pixels as reference rasters.              \n",
    "    \n",
    "    returns: The output raster filepath.\n",
    "    \"\"\"\n",
    "    if 'parquet' in input_file: # if parquet file used as input_file\n",
    "        gdf = read_parquet_as_geodataframe(parquet_file=input_file, grid_geometry_file=grid_shapefile, save=False)\n",
    "    else: # if geodataframe used as input_file\n",
    "        gdf = input_file\n",
    "        \n",
    "    if date is not None:\n",
    "        gdf = gdf[gdf['date'] == date]\n",
    "\n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    input_shape = ((geom, value) for geom, value in zip(gdf.geometry, gdf[attribute]))\n",
    "\n",
    "    raster_arr = rasterize(shapes=input_shape, out_shape=ref_arr.shape, fill=no_data_value, out=None, \n",
    "                           transform=ref_file.transform, all_touched=True, \n",
    "                           default_value=no_data_value, dtype=dtype, merge_alg=merge_alg)\n",
    "    \n",
    "    if paste_on_ref_raster:\n",
    "        raster_arr[np.isnan(ref_arr)] = no_data_value\n",
    "        \n",
    "    write_array_to_raster(raster_arr, raster_file=ref_file, transform=ref_file.transform, \n",
    "                          output_path=output_raster, ref_file=None, nodata=no_data_value)\n",
    "\n",
    "    return output_raster\n",
    "\n",
    "\n",
    "\n",
    "def resample_raster_based_on_ref_raster(input_raster, ref_raster, output_dir, raster_name, resampling_alg=Resampling.bilinear,\n",
    "                                        paste_value_on_ref_raster=False):\n",
    "    \"\"\"\n",
    "    Resample raster based on a refernce raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Filepath of input raster to resample.\n",
    "    ref_raster : Filepath of input raster to be used in determining resample height/width/affine transformation/crs/dtype/nodata.\n",
    "    output_raster : Filepath of resampled output raster.\n",
    "    resampling_alg : resampling algorithm. Can be Resampling.nearest/ Resampling.bilinear/Resampling.cubic or \n",
    "                     any resampling algorith rasterio supports Default set to Resampling.bilinear.\n",
    "    paste_value_on_ref_raster : Set to True if want to have nodata pixels on the resampled raster similar to reference raster. \n",
    "    \n",
    "    returns: The resampled output raster filepath.\n",
    "    \"\"\"\n",
    "    makedirs([output_dir])\n",
    "    \n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    \n",
    "    # target shape. use a reference raster (created using GIS for a specific region) to decide.\n",
    "    resampled_height, resampled_width = ref_arr.shape\n",
    "\n",
    "    with rio.open(input_raster) as dataset:\n",
    "        # resample data to target shape\n",
    "        resampled_arr = dataset.read(1,\n",
    "                            out_shape=(1,\n",
    "                                       resampled_height,\n",
    "                                       resampled_width),\n",
    "                            resampling=resampling_alg)\n",
    "\n",
    "        resampled_arr = resampled_arr.squeeze() # removing the 1 (for count) from the dimension\n",
    "        \n",
    "        if paste_value_on_ref_raster:\n",
    "            resampled_arr = np.where(np.isnan(ref_arr), -9999, resampled_arr)\n",
    "        \n",
    "        # Saving the resampled data\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "        write_array_to_raster(raster_arr=resampled_arr, raster_file=ref_file, \n",
    "                              transform=ref_file.transform, output_path=output_raster, \n",
    "                              ref_file=None, nodata=-9999)\n",
    "        \n",
    "        return output_raster\n",
    "    \n",
    "\n",
    "def resample_raster_with_height_width(input_raster, height, width, ref_raster, output_dir, raster_name, \n",
    "                                      resampling_alg=Resampling.bilinear):\n",
    "    \"\"\"\n",
    "    Resample raster based on a refernce raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Filepath of input raster to resample.\n",
    "    height, width : integer of resampling height and width.\n",
    "    ref_raster : Filepath of input raster to be used in determining resample affine transformation/crs/dtype/nodata.\n",
    "    output_raster : Filepath of resampled output raster.\n",
    "    resampling_alg : resampling algorithm. Can be Resampling.nearest/ Resampling.bilinear/Resampling.cubic or \n",
    "                     any resampling algorith rasterio supports Default set to Resampling.bilinear.\n",
    "    \n",
    "    returns: The resampled output raster filepath.\n",
    "    \"\"\"\n",
    "    makedirs([output_dir])\n",
    "    \n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "\n",
    "    with rio.open(input_raster) as dataset:\n",
    "        # resample data to target shape\n",
    "        resampled_arr = dataset.read(1,\n",
    "                            out_shape=(1,\n",
    "                                       height, width),\n",
    "                            resampling=resampling_alg)\n",
    "\n",
    "        resampled_arr = resampled_arr.squeeze() # removing the 1 (for count) from the dimension\n",
    "        \n",
    "        # Saving the resampled data\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "        write_array_to_raster(raster_arr=resampled_arr, raster_file=ref_file, \n",
    "                              transform=ref_file.transform, output_path=output_raster, \n",
    "                              ref_file=None, nodata=-9999)\n",
    "        \n",
    "        return output_raster\n",
    "    \n",
    "    \n",
    "def mask_raster_array_by_shapefile(input_raster, mask_shape, output_dir=None, raster_name=None, invert=False,\n",
    "                                   crop=True, save_masked_arr=False):\n",
    "    \"\"\"\n",
    "    Mask a raster using a input shapefile.\n",
    "\n",
    "    Parameters:\n",
    "    input_raster: Input raster filepath.\n",
    "    mask_shape : Reference shape file to crop input_raster.\n",
    "    output_dir : Defaults to None. Set a output raster directory path if save_masked_arr is True.\n",
    "    raster_name : Defaults to None. Set a output raster name if save_masked_arr is True.\n",
    "    invert : If False (default) pixels outside shapes will be masked.\n",
    "             If True, pixels inside shape will be masked.\n",
    "    crop : Whether to crop the raster to the extent of the shapes. Set to False if invert=True is used.\n",
    "    save_masked_arr : Set to true if want to save cropped/masked raster array. If True, must provide output_raster_name and\n",
    "                       output_dir.\n",
    "\n",
    "    returns : Masked raster array and masked raster filepath.\n",
    "    \"\"\"\n",
    "    input_arr, input_file = read_raster_arr_object(input_raster)\n",
    "    \n",
    "    shapefile = gpd.read_file(mask_shape)\n",
    "    geoms = shapefile['geometry'].values  # list of shapely geometries\n",
    "    geoms = [mapping(geoms[0])]\n",
    "    \n",
    "    # masking\n",
    "    masked_arr, masked_transform = mask(dataset=input_file, shapes=geoms, filled=True, crop=crop, invert=invert, \n",
    "                                        all_touched=False)\n",
    "    masked_arr = masked_arr.squeeze()  # Remove axes of length 1 from the array\n",
    "    \n",
    "\n",
    "    if save_masked_arr:\n",
    "        # naming output file\n",
    "        makedirs([output_dir])\n",
    "        output_raster = os.path.join(output_dir, raster_name)\n",
    "\n",
    "        # saving output raster\n",
    "        masked_raster = write_array_to_raster(raster_arr=masked_arr, raster_file=input_file, transform=masked_transform,\n",
    "                              output_path=output_raster)\n",
    "        return masked_arr, masked_raster\n",
    "   \n",
    "    else: # in case raster is not saved return only masked raster array\n",
    "        return masked_arr\n",
    "\n",
    "\n",
    "def convert_point_geom_to_poly_geom_from_centroid(point_shapefile, output_shapefile, crs='EPSG:4326'):\n",
    "    \"\"\"\n",
    "    Convert point shapefile to polygon shapefile. \n",
    "    ** The cell size will be figured out from distance betwwen centroids.\n",
    "    \n",
    "    params:\n",
    "    point_shapefile : Filepath of point shaepfile.\n",
    "    output_shapefile : Filepath of output shapefile.\n",
    "    crs : Default crs set to 'EPSG:4326'. \n",
    "    \n",
    "    return: The output polygon geometry filepath.\n",
    "    \"\"\"\n",
    "    point_gdf = gpd.read_file(point_shapefile)\n",
    "\n",
    "    # We are not directly inputting cell size as an argument because our calculated/presumed cell size\n",
    "    # might not exact match with grid points' center-to-center distance. Instead, we are calculating cell size\n",
    "    # directly from two (02) adjacent points' center-to-center distance\n",
    "    cell_size = None\n",
    "    \n",
    "    for i, point in enumerate(point_gdf['geometry']):\n",
    "        if i==0:  #1st point's coords\n",
    "            x1 = point.x\n",
    "            y1 = point.y\n",
    "            \n",
    "        else: #2nd point's coords\n",
    "            x2 = point.x\n",
    "            y2 = point.y\n",
    "\n",
    "            # adding a condition so that cell size can be calculated after 2nd itration (after 2nd point's coords has been collected)\n",
    "            if (i>0) & ((x2-x1) > 0.04):  ## taking 0.04 as the filter resolution as both TWC/TWC precip/ERA5 resolution is equal or higher than this values\n",
    "                cell_size = abs(abs(x2)- abs(x1))\n",
    "                break\n",
    "\n",
    "    # create the cells/polygons in a loop\n",
    "    grid_cells = []\n",
    "    for point in point_gdf['geometry']:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "\n",
    "        # Calculating new polygon bounds\n",
    "        x0 = x - (cell_size/2)\n",
    "        y0 = y - (cell_size/2)\n",
    "        x1 = x + (cell_size/2)\n",
    "        y1 = y + (cell_size/2)\n",
    "\n",
    "        grid_cells.append(shapely.geometry.box(x0, y0, x1, y1))\n",
    "\n",
    "    poly_gdf = point_gdf.drop(columns=['geometry'])\n",
    "    poly_gdf['geometry'] = grid_cells\n",
    "    poly_gdf = gpd.GeoDataFrame(poly_gdf, geometry='geometry', crs=crs)\n",
    "    poly_gdf.to_file(output_shapefile)\n",
    "\n",
    "    return output_shapefile\n",
    "\n",
    "\n",
    "def mask_datasets_with_shapefile(input_raster_dir, main_output_dir, mask_shape, exclude_datasets=None,\n",
    "                                 resample=False, resample_target_raster=None):\n",
    "    \"\"\"\n",
    "    Mask datasets in a folder. Will first try going into the subdirectories of the input_raster_dir. \n",
    "    If there are no sub-directories, it will collect all the .tif files in the input_raster_dir and mask them.\n",
    "    \n",
    "    input_raster_dir : Main directory filepath that has sub-directories with the .tif files. \n",
    "                      If there are no sub-directories, will collect the .tif files and process them.\n",
    "    main_output_dir : The main output directory filepath where masked rasters will be saved.\n",
    "    mask_shape : Filepath of shapefile that will be used to mask the rasters.\n",
    "    exclude_datasets : List of datasets to exclude from processing. Default set to None. \n",
    "    \n",
    "    resample : Set to True if want to resample data to height and weight of any target reference raster.\n",
    "               Default set to None.\n",
    "    resample_target_raster : Filepath of target raster to use if resampling. Default set to None.\n",
    "    \"\"\"\n",
    "    # if there are sub-directories inside the input_raster_folder\n",
    "    try:\n",
    "        variables = os.listdir(input_raster_dir)\n",
    "        if exclude_datasets is not None:\n",
    "            variables = [i for i in variables if i not in exclude_datasets]\n",
    "\n",
    "        for var in variables:\n",
    "            datasets = glob(os.path.join(input_raster_dir, var, '*.tif'))\n",
    "            print(f'Masking data for {var}...')\n",
    "\n",
    "            for data in datasets:\n",
    "                # creating output folders\n",
    "                if resample:\n",
    "                    output_dir = os.path.join(main_output_dir, var, 'masked')\n",
    "                else:\n",
    "                    output_dir = os.path.join(main_output_dir, var)\n",
    "                \n",
    "                # masking\n",
    "                raster_name = os.path.basename(data).split('.')[0] + '.tif'\n",
    "                arr, masked_raster_fp = mask_raster_array_by_shapefile(input_raster=data, mask_shape=mask_shape, \n",
    "                                                                       output_dir=output_dir, \n",
    "                                                                       raster_name=raster_name, invert=False,\n",
    "                                                                       crop=True, save_masked_arr=True)\n",
    "                if resample:\n",
    "                    # resampling to make sure the processed raster has the same height*width as as the reference raster\n",
    "                    resampled_output_dir = os.path.join(main_output_dir, var)\n",
    "                    resample_raster_based_on_ref_raster(input_raster=masked_raster_fp, ref_raster=resample_target_raster, \n",
    "                                                        output_dir=resampled_output_dir, raster_name=raster_name)\n",
    "    \n",
    "\n",
    "    except:     # If input_raster_dir is the main directory and has all the .tif files\n",
    "        datasets = glob(os.path.join(input_raster_dir, '*.tif'))   \n",
    "        for data in datasets:\n",
    "            if resample:\n",
    "                output_dir = os.path.join(main_output_dir, 'masked')\n",
    "\n",
    "            raster_name = os.path.basename(data).split('.')[0] + '.tif'\n",
    "            arr, masked_raster_fp = mask_raster_array_by_shapefile(input_raster=data, mask_shape=mask_shape, \n",
    "                                                               output_dir=main_output_dir, \n",
    "                                                               raster_name=raster_name, invert=False,\n",
    "                                                               crop=True, save_masked_arr=True)\n",
    "            if resample:\n",
    "                resampled_output_dir = main_output_dir\n",
    "                # resampling to make sure the processed raster has the same height*width as as the reference raster\n",
    "                resample_raster_based_on_ref_raster(input_raster=masked_raster_fp, ref_raster=resample_target_raster, \n",
    "                                                    output_dir=resampled_output_dir, raster_name=raster_name)\n",
    "                \n",
    "                \n",
    "def fill_nodata_by_interpolation(input_raster, reference_raster, output_raster,\n",
    "                                 use_pixel_to_interp=100):\n",
    "    \"\"\"\n",
    "    Interpolate values to fill nodata gaps in input raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Filepath of input raster. \n",
    "    reference_raster : Filepath of reference raster. The reference raster will be used to decide where nodata values \n",
    "                       will be filled/interpolated.\n",
    "    output_raster : Filepath of output raster.\n",
    "    \n",
    "    returns :None.\n",
    "    \"\"\"\n",
    "    data_arr, data_file = read_raster_arr_object(input_raster)\n",
    "    ref_arr, ref_file = read_raster_arr_object(reference_raster)\n",
    "    \n",
    "    # changing no data from -9999 to np.nan. Otherwise, the masking operation and furthur interpolation doesn't work.\n",
    "    data_arr[data_arr == data_file.nodata] = np.nan\n",
    "    ref_arr[ref_arr == ref_file.nodata] = np.nan\n",
    "    \n",
    "    # creating mask data where zero value locations will be interpolated\n",
    "    mask_arr = data_arr.copy()\n",
    "    mask_arr[~np.isnan(data_arr)] = 1\n",
    "    mask_arr[np.isnan(data_arr)] = 0\n",
    "    \n",
    "    \n",
    "    # Interpolation of array to fill nodata locations\n",
    "    interp_arr = fillnodata(data_arr, mask=mask_arr, max_search_distance=use_pixel_to_interp, smoothing_iterations=0)\n",
    "    interp_arr[np.isnan(ref_arr)] = -9999  # interpolation causes some increase in extent. removing that with ref array\n",
    "    \n",
    "    write_array_to_raster(raster_arr=interp_arr, raster_file=data_file, transform=data_file.transform, \n",
    "                          output_path=output_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600e3da",
   "metadata": {},
   "source": [
    "## Database (parquet / geodataframe) operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a5149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_to_h3(row):\n",
    "    \"\"\"\n",
    "    create polygon geometry for h3 index.\n",
    "    \n",
    "    params:\n",
    "    row : dataframe row. \n",
    "    \n",
    "    returns: polygon geometry for h3 index.\n",
    "    \"\"\"\n",
    "    points = h3.h3_to_geo_boundary(row['h3'], True)\n",
    "    \n",
    "    return Polygon(points)\n",
    "\n",
    "\n",
    "def read_h3_parquet_save_as_geodataframe(parquet_file, h3_geometry_file, save=False, output_folder=None, savename=None):\n",
    "    \"\"\"\n",
    "    Read parquet file with h3 information and save it as a geodataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_file : Filepath of parquet file. Must have h3 information.\n",
    "    h3_geometry-file: Filepath of h3 geometry file. Must have matching h3 information with the parquet file and geometry info.\n",
    "    save : Set to true if want to save data as geodataframe/shapefile.\n",
    "    output_folder : str of output folder to save the data. Default set to None.\n",
    "    savename : str of name of the shapefile. Default set to None.\n",
    "    \n",
    "    returns: geopandas dataframe with data information.\n",
    "    \"\"\"\n",
    "    df_parq = pd.read_parquet(parquet_file) # must have h3 information\n",
    "    df_h3 = gpd.read_file(h3_geometry_file) # must have matching h3 information with the parquet file and geometry info\n",
    "    \n",
    "    df_compiled = df_parq.merge(df_h3, on='h3', how='inner')\n",
    "    gdf_compiled = gpd.GeoDataFrame(df_compiled, geometry='geometry')\n",
    "    \n",
    "    if save:\n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        gdf_compiled.to_file(savefile)\n",
    "    \n",
    "    return gdf_compiled\n",
    "\n",
    "def read_parquet_as_geodataframe(parquet_file, grid_geometry_file, save=False, output_folder=None, savename=None):\n",
    "    \"\"\"\n",
    "    Read parquet file with twc/era5 grid information and save it as a geodataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_file : Filepath of parquet file. Must have twc grid information.\n",
    "    grid_geometry-file: Filepath of twc/era5 grid geometry file. Must have matching twc grid information with the \n",
    "                            parquet file and geometry info.\n",
    "    save : Set to true if want to save data as geodataframe/shapefile. Large files will have error (dieing kernel). \n",
    "           Better to not save when facing such issues.\n",
    "    output_folder : str of output folder to save the data. Default set to None.\n",
    "    savename : str of name of the shapefile. Default set to None.\n",
    "    \n",
    "    returns: geopandas dataframe with data information.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_parq = pd.read_parquet(parquet_file) # must have twc/era5 grid information\n",
    "    df_grid = gpd.read_file(grid_geometry_file) # must have matching twc grid information with the parquet file and geometry info\n",
    "    \n",
    "    df_compiled = df_parq.merge(df_grid, on='grid_id', how='inner')\n",
    "    gdf_compiled = gpd.GeoDataFrame(df_compiled, geometry='geometry')\n",
    "    \n",
    "    if save:\n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        gdf_compiled.to_file(savefile)\n",
    "    \n",
    "    return gdf_compiled\n",
    "\n",
    "\n",
    "def clip_grids_by_admin(grids_file, admin_file, output_folder, savename):\n",
    "    \"\"\"\n",
    "    Clip a shapefile/geodataframe with another shapefile//geodataframe.\n",
    "    \n",
    "    params:\n",
    "    grids_file : shapefile path/geodataframe with twc_grid/era5_rid information.\n",
    "    admin_file : shapefile path/geodataframe used to clip the grids_file.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    \n",
    "    returns: geopandas dataframe of cliiped shapefile/geodataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if '.shp' not in grids_file or admin_file:\n",
    "        grids_df = grids_file\n",
    "        admin_df = admin_file\n",
    "    else:\n",
    "        grids_df = gpd.read_file(grids_file)\n",
    "        admin_df = gpd.read_file(admin_file)\n",
    "\n",
    "        \n",
    "    clipped_gdf = gpd.clip(grids_df['geometry'], admin_df['geometry'])\n",
    "    clipped_gdf = gpd.GeoDataFrame(clipped_gdf, geometry='geometry')\n",
    "    clipped_gdf = clipped_gdf.join(grids_df, on=None, how='left', lsuffix='', rsuffix='R')  # merging lost grids_df info to the clipped grids \n",
    "    clipped_gdf = clipped_gdf.drop(columns=['geometry', 'geometryR'])\n",
    "    clipped_gdf = gpd.GeoDataFrame(clipped_gdf, geometry=gpd.points_from_xy(clipped_gdf.lon, clipped_gdf.lat), \n",
    "                                   crs=\"EPSG:4326\")\n",
    "    clipped_gdf = clipped_gdf.dropna()\n",
    "    clipped_gdf = clipped_gdf.reset_index()\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    clipped_gdf.to_file(savefile)\n",
    "    \n",
    "    return clipped_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b948d2c",
   "metadata": {},
   "source": [
    "## Weather Data processing codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c10c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lat_lon_array_from_raster(input_raster, nodata=-9999):\n",
    "    \"\"\"\n",
    "    Make lat, lon array for each pixel using the input raster.\n",
    "    \n",
    "    params:\n",
    "    input_raster : Input raster filepath that will be used as reference raster.\n",
    "    nodata : No data value. Default set to -9999.\n",
    "    \n",
    "    returns: Lat, lon array with nan value (-9999) applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    raster_file = rio.open(input_raster)\n",
    "    raster_arr = raster_file.read(1)\n",
    "\n",
    "    # calculating lat, lon of each cells centroid\n",
    "    height, width = raster_arr.shape\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rio.transform.xy(rows=rows, cols=cols, transform=raster_file.transform)\n",
    "    \n",
    "    # flattening and reshaping to the input_raster's array size\n",
    "    xs = np.array(xs).flatten()\n",
    "    ys = np.array(ys).flatten()\n",
    "    \n",
    "    lon_arr = xs.reshape(raster_arr.shape)\n",
    "    lat_arr = ys.reshape(raster_arr.shape)\n",
    "    \n",
    "    # assigning no_data_value\n",
    "    lon_arr[raster_arr==nodata] = nodata\n",
    "    lat_arr[raster_arr==nodata] = nodata\n",
    "    \n",
    "    return lon_arr, lat_arr\n",
    "\n",
    "\n",
    "def process_twc_daily_data(twc_parquet_file, twc_geom_shp, ref_raster,\n",
    "                           remove_cols = ['grid_id', 'date', 'index', 'elevation', 'time_zone', 'geometry', 'lat', 'lon'],\n",
    "                           twc_output_dir='../../datasets/weather_raster_data/twc_data'):\n",
    "    \"\"\"\n",
    "    Process TWC daily data for each column attribute.\n",
    "    \n",
    "    params:\n",
    "    twc_parquet_file : Filepath of parquet file with the attributes. The parquet file should have a \n",
    "                       grid_id column to be matched with the grid_shapefile.\n",
    "    twc_geom_shp : Filepath of grid/geometry shapefile. \n",
    "                   Should have a grid_id column to be matched with the parquet file.\n",
    "    ref_raster : Filepath of reference raster to be used in shapefile to raster conversion. \n",
    "                 For example, california_4km_refraster as TWC's original grid size is 4km.\n",
    "    remove_cols : Columns in the TWC file that will not be rasterized.\n",
    "    twc_output_dir : Filepath of folder where rasterized data will be saved.\n",
    "\n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    twc_gdf = read_parquet_as_geodataframe(parquet_file=twc_parquet_file, grid_geometry_file=twc_geom_shp, \n",
    "                                           save=False, output_folder=None, savename=None)\n",
    "    # creating output folder\n",
    "    makedirs([twc_output_dir])\n",
    "    \n",
    "    # making list of columns to be rasterized\n",
    "    twc_attr = list(twc_gdf.columns)\n",
    "    keep_attr = [i for i in twc_attr if i not in remove_cols]\n",
    "    \n",
    "    # making list of unique dates in twc data\n",
    "    dates = twc_gdf['date'].unique()\n",
    "    \n",
    "    # Looping through each attribute and each date to rasterize the data\n",
    "    for attr_col in keep_attr:\n",
    "        print(f'Processing TWC {attr_col} dataset...')\n",
    "        \n",
    "        # making new output directory for specific attribute\n",
    "        process_to_dir = os.path.join(twc_output_dir, attr_col)  \n",
    "        makedirs([process_to_dir])    \n",
    "        \n",
    "        for date in dates:\n",
    "            # making raster name\n",
    "            date_str = ''.join(date.split('-'))\n",
    "            output_raster_fp = os.path.join(process_to_dir, f'{attr_col}_{date_str}.tif')\n",
    "\n",
    "            # rasterization\n",
    "            rasterize_shapefile(input_file=twc_gdf, grid_shapefile=None, attribute=attr_col, \n",
    "                                date=date, ref_raster=ref_raster, output_raster=output_raster_fp, \n",
    "                                merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999)\n",
    "    \n",
    "    # making latitude and longitude rasters\n",
    "    print(f'Processing lat, lon dataset...')\n",
    "    ref_arr, ref_file = read_raster_arr_object(ref_raster)\n",
    "    lon_arr, lat_arr = make_lat_lon_array_from_raster(ref_raster)\n",
    "    \n",
    "    lon_dir = os.path.join(twc_output_dir, 'lon')\n",
    "    lat_dir = os.path.join(twc_output_dir, 'lat')\n",
    "    makedirs([lon_dir, lat_dir])\n",
    "\n",
    "    write_array_to_raster(raster_arr=lon_arr, raster_file=ref_file, transform=ref_file.transform, \n",
    "                          output_path=os.path.join(lon_dir, 'lon.tif'))\n",
    "    write_array_to_raster(raster_arr=lat_arr, raster_file=ref_file, transform=ref_file.transform, \n",
    "                          output_path=os.path.join(lat_dir, 'lat.tif'))\n",
    "\n",
    "\n",
    "def compile_twc_daily_data_to_dataframe(savename, twc_data_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Compile twc daily data in a dataframe. All datasets have to be of same shape.\n",
    "    \n",
    "    params:\n",
    "    savename : (str) Name of the output parquet file.\n",
    "    twc_data_folder : TWC daily data main folder. The code will automatically get data in the sub-directories.\n",
    "    output_folder : Main output folder. The code will automatically save data in the individual sub-directories.\n",
    "    \n",
    "    returns: compiled TWC dataframe.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    # making list of variables in the twc raster data folder\n",
    "    variable_names = os.listdir(twc_data_folder)\n",
    "    variable_paths = [os.path.join(twc_data_folder, folder) for folder in variable_names]\n",
    "    \n",
    "    # will be used to multiply lat/lon data \n",
    "    # the condition is added because we have to process different TWC data and not all has max_temp or total_precip\n",
    "    num_days = len(glob(os.path.join(twc_data_folder, 'max_temp', '*.tif')))\n",
    "    if num_days > 1:\n",
    "        pass\n",
    "    else:\n",
    "        num_days = len(glob(os.path.join(twc_data_folder, 'total_precip', '*.tif')))\n",
    "    \n",
    "    \n",
    "    variable_dict = {}  # a dictionary where daily dataset values will be stored under variable_name \n",
    "    \n",
    "    for path in variable_paths:\n",
    "        all_data = glob(os.path.join(path, '*.tif')) # making list of all dataset in a particular folder\n",
    "        all_data = sorted(all_data)  # to sort data by date so that all variables are compiled in same serial\n",
    "        \n",
    "        variable_name = os.path.basename(path).split('.')[0]  # extracted variable name \n",
    "        if variable_name not in ['lat', 'lon']:\n",
    "            print(f'compiling data for {variable_name}...')\n",
    "\n",
    "            # loop for reading datasets and storing pixel info in a dictionary\n",
    "            for count, data in enumerate(all_data):\n",
    "                # retrieving and storing data\n",
    "                data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "\n",
    "                # extarcting, formatting, and storing date info\n",
    "                date = os.path.basename(data).split('.')[0].split('_')[-1]\n",
    "                year, month, day = date[:4], date[4:6], date[6:]\n",
    "\n",
    "                len_data = len(data_arr)  # number of pixels in each daily dataset (array)\n",
    "                year_list = [int(year)] * len_data\n",
    "                month_list = [int(month)] * len_data\n",
    "                day_list = [int(day)] * len_data\n",
    "                date_list = [int(date)] * len_data\n",
    "\n",
    "                # Assigning all values to the variable_dict\n",
    "                if count == 0:\n",
    "                    variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "                    variable_dict['date'] = date_list\n",
    "                    variable_dict['year'] = year_list\n",
    "                    variable_dict['month'] = month_list\n",
    "                    variable_dict['day'] = day_list\n",
    "\n",
    "                else:\n",
    "                    variable_dict[variable_name].extend(list(data_arr))  # storing flattened data in a dictionary under the variable name)\n",
    "                    variable_dict['date'].extend(date_list)\n",
    "                    variable_dict['year'].extend(year_list)\n",
    "                    variable_dict['month'].extend(month_list)\n",
    "                    variable_dict['day'].extend(day_list)\n",
    "        else: # for lat/lon\n",
    "            data = glob(os.path.join(path, '*.tif'))[0]##############\n",
    "            data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "            data_duplicated_for_days = list(data_arr) * num_days\n",
    "            variable_dict[variable_name] = data_duplicated_for_days\n",
    "\n",
    "    twc_variable_df = pd.DataFrame(variable_dict)\n",
    "    twc_variable_ddf = ddf.from_pandas(twc_variable_df, npartitions=20)\n",
    "    twc_variable_ddf = twc_variable_ddf.dropna()\n",
    "    twc_variable_ddf = twc_variable_ddf.reset_index()\n",
    "    \n",
    "    if '.parquet' in savename:\n",
    "        output_parquet_file = os.path.join(output_folder, savename)\n",
    "    else:\n",
    "        output_parquet_file = os.path.join(output_folder, savename+'.parquet')\n",
    "    \n",
    "    twc_variable_ddf.to_parquet(output_parquet_file)\n",
    "    \n",
    "    end_time = time()\n",
    "    print('time taken', round((end_time-start_time)/60, 3), 'mins')\n",
    "\n",
    "    return twc_variable_ddf\n",
    "      \n",
    "    \n",
    "def process_era5_daily_data(era5_parquet_file, era5_geom_shp, ref_raster_rasterize,\n",
    "                            ref_raster_resample, resampling_alg=Resampling.bilinear, \n",
    "                            remove_cols = ['grid_id', 'date', 'index', 'lat', 'lon', 'time_zone', 'geometry'],\n",
    "                            era5_output_dir='../../datasets/raster_data/era5_data',\n",
    "                            resampled_output_dir= '../../datasets/raster_data/era5_data/resampled_4km_rasters_nearest'):\n",
    "    \"\"\"\n",
    "    Process ERA5 daily data for each column attribute.\n",
    "    \n",
    "    params:\n",
    "    era5_parquet_file : Filepath of parquet file with the attributes. The parquet file should have a \n",
    "                        grid_id column to be matched with the grid_shapefile.\n",
    "    era5_geom_shp : Filepath of grid/geometry shapefile. \n",
    "                    Should have a grid_id column to be matched with the parquet file.\n",
    "    ref_raster_rasterize : Filepath of reference raster (a 28km/0.25 deg raster) to be used in shapefile to raster conversion.\n",
    "    ref_raster_resample : Filepath of reference raster to be used in resampling.\n",
    "    resampling_alg : resampling algorithm. Can be Resampling.nearest/ Resampling.bilinear/Resampling.cubic or \n",
    "                     any resampling algorith rasterio supports Default set to Resampling.bilinear.\n",
    "              \n",
    "    remove_cols : Columns in the era5 file that will not be rasterized.\n",
    "    era5_output_dir : Filepath of folder where rasterized data will be saved.\n",
    "    resampled_output_dir : Filepath of folder where resampled data will be saved. Made this a variable\n",
    "                           because I am resampling data in both nearest_neighbour and bilinear approach.\n",
    "                           Default set to '../../datasets/raster_data/era5_data/resampled_4km_rasters_nearest'.\n",
    "    \n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    era5_gdf = read_parquet_as_geodataframe(parquet_file=era5_parquet_file, grid_geometry_file=era5_geom_shp, \n",
    "                                            save=False, output_folder=None, savename=None)\n",
    "    era5_gdf = era5_gdf.dropna()\n",
    "    \n",
    "    # creating output folders for original rasters, resampled raster\n",
    "    rasterization_dir = os.path.join(era5_output_dir, 'original_28km_rasters')\n",
    "    resampled_raster_dir = os.path.join(resampled_output_dir)\n",
    "    makedirs([era5_output_dir, rasterization_dir, resampled_raster_dir])\n",
    "    \n",
    "    # making list of columns to be rasterized\n",
    "    era5_attr = list(era5_gdf.columns)\n",
    "    keep_attr = [i for i in era5_attr if i not in remove_cols]\n",
    "    \n",
    "    # making list of uniqye dates in era5 data\n",
    "    dates = era5_gdf['date'].unique()\n",
    "\n",
    "    # Looping through each attribute and each date to rasterize the data\n",
    "    for attr_col in keep_attr:\n",
    "        print(f'Processing ERA5 {attr_col} dataset...')\n",
    "        \n",
    "        # making new output directory for specific attribute\n",
    "        rasterize_to_dir = os.path.join(rasterization_dir, attr_col)  # original 28km rasters will be saved  \n",
    "        resample_to_dir = os.path.join(resampled_raster_dir, attr_col)  # resampled 4km rasters will be saved\n",
    "        makedirs([rasterize_to_dir, resample_to_dir])    \n",
    "        \n",
    "        for date in dates:\n",
    "            # making raster name\n",
    "            date_str = ''.join(date.split('-'))\n",
    "\n",
    "            # rasterization\n",
    "            output_raster_fp = os.path.join(rasterize_to_dir, f'{attr_col}_{date_str}.tif')\n",
    "            era5_raster = rasterize_shapefile(input_file=era5_gdf, grid_shapefile=None, attribute=attr_col, \n",
    "                                              date=date, ref_raster=ref_raster_rasterize, \n",
    "                                              output_raster=output_raster_fp, \n",
    "                                              merge_alg = MergeAlg.replace, \n",
    "                                              dtype='float32', no_data_value=-9999)\n",
    "            \n",
    "            # resampling raster\n",
    "            output_raster_name = f'{attr_col}_{date_str}.tif'\n",
    "            resample_raster_based_on_ref_raster(input_raster=era5_raster, resampling_alg=resampling_alg,\n",
    "                                                ref_raster=ref_raster_resample, \n",
    "                                                 output_dir=resample_to_dir, raster_name=output_raster_name)\n",
    "            \n",
    "            \n",
    "def compile_era5_daily_data_to_multiple_dataframe(dataset_in_each_chunk, save_keyword, era5_data_folder, output_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compile ERA5 daily data in multiple dataframes. \n",
    "    *** All input daily datasets have to be of same shape.\n",
    "    *** If datasets are processed in chunks, multiple dataframes will be created and user have to read them separately for \n",
    "    further processing (use compile_era5_multiDF_to_singleDF() to compile into a single dataframe)\n",
    "    \n",
    "    params:\n",
    "    dataset_in_each_chunk: (int). Number of datasets to process in each chunk. If dataset is small, give total number of \n",
    "                           variables to process everything in a signle chunk.\n",
    "    save_keyword : A keyword (str) to distinguish between 4km/8km parquet files. Can set to '4km'/'8km'.\n",
    "    era5_data_folder : ERA5 daily data main folder. The code will automatically get data in the sub-directories.\n",
    "    output_folder : Main output folder. The code will automatically save data in the individual sub-directories.\n",
    "    \n",
    "    returns: None. \n",
    "    \"\"\"\n",
    "    makedirs([output_folder])\n",
    "    \n",
    "    # making list of variables in the era5 raster data folder\n",
    "    variable_names = os.listdir(era5_data_folder)\n",
    "    variable_chunks = [variable_names[x:x+dataset_in_each_chunk] for x in range(0, len(variable_names), dataset_in_each_chunk)]\n",
    "\n",
    "    # will be used to multiply lat/lon data\n",
    "    num_days = len(glob(os.path.join(era5_data_folder, 'total_precip', '*.tif')))\n",
    "    \n",
    "    # first loop for each set/chuck of datasets\n",
    "    for num, chunk in enumerate(variable_chunks):\n",
    "        start_time = time()\n",
    "        \n",
    "        # removing lat/lon if selected in chunk. The, adding lat+lon data in each chunk for merging purpose with TWC. \n",
    "        # If all data are processed in one chunk this isn't required, but homogenizing for all datasets.\n",
    "        if 'lat' in chunk:\n",
    "            chunk.remove('lat')\n",
    "        if 'lon' in chunk:\n",
    "            chunk.remove('lon')\n",
    "        chunk.extend(['lat', 'lon'])\n",
    "        \n",
    "        print(f'processing for {chunk}..')\n",
    "        \n",
    "        variable_paths = [os.path.join(era5_data_folder, folder) for folder in chunk]  \n",
    "    \n",
    "        variable_dict = {}  # a dictionary where daily dataset values will be stored under variable_name \n",
    "    \n",
    "        for path in variable_paths:\n",
    "            all_data = glob(os.path.join(path, '*.tif')) # making list of all dataset in a particular folder\n",
    "            all_data = sorted(all_data)  # to sort data by date so that all variables are compiled in same serial\n",
    "            \n",
    "            # extracting variable name\n",
    "            variable_name = os.path.basename(path).split('.')[0]   \n",
    "            print(f'compiling data for {variable_name}...')\n",
    "\n",
    "            if variable_name not in ['lat', 'lon', 'elevation', 'slope', 'aspect']:\n",
    "                # loop for reading datasets and storing pixel info in a dictionary\n",
    "                for count, data in enumerate(all_data):\n",
    "                    # retrieving and storing data\n",
    "                    data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "            \n",
    "                    # extarcting and storing date info\n",
    "                    date = os.path.basename(data).split('.')[0].split('_')[-1]\n",
    "\n",
    "                    len_data = len(data_arr)  # number of pixels in each daily dataset (array)\n",
    "                    date_list = [int(date)] * len_data\n",
    "\n",
    "                    # Assigning all values to the variable_dict\n",
    "                    if count == 0:\n",
    "                        variable_dict[variable_name] = list(data_arr)  # storing flattened data in a dictionary under the variable name\n",
    "                        variable_dict['date'] = date_list\n",
    "\n",
    "                    else:\n",
    "                        variable_dict[variable_name].extend(list(data_arr))  # storing flattened data in a dictionary under the variable name)\n",
    "                        variable_dict['date'].extend(date_list)\n",
    "\n",
    "            else: # for lat/lon/elevation/slope/aspect data\n",
    "                data = glob(os.path.join(path, '*.tif'))[0]\n",
    "                data_arr = read_raster_arr_object(data, get_file=False).flatten()  # read data as array and flattened it\n",
    "                data_duplicated_for_days = list(data_arr) * num_days\n",
    "                variable_dict[variable_name] = data_duplicated_for_days\n",
    "\n",
    "        era5_variable_df = pd.DataFrame(variable_dict)\n",
    "        era5_variable_ddf = ddf.from_pandas(era5_variable_df, npartitions=20)\n",
    "        era5_variable_ddf = era5_variable_ddf.dropna()\n",
    "        era5_variable_ddf = era5_variable_ddf.reset_index()\n",
    "\n",
    "        output_parquet_file = os.path.join(output_folder, f'{save_keyword}_era5_daily_data_{num}.parquet')\n",
    "        era5_variable_ddf.to_parquet(output_parquet_file)\n",
    "\n",
    "        end_time = time()\n",
    "        print('time taken', round((end_time-start_time)/60, 3), 'mins')\n",
    "\n",
    "        \n",
    "def compile_era5_multiDF_to_singleDF(parquet_folder, output_folder, save_keyword):\n",
    "    \"\"\"\n",
    "    Compile multiple dataframe of era5 data (generated by compile_era5_daily_data_to_multiple_dataframe()) \n",
    "    into a single dataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_folder : Filepath of folder where multiple parquet files (dataframes) are saved.\n",
    "    output_folder : Filepath of output folder where single parquet file (dataframe) with all era5 variales will be saved.\n",
    "    save_keyword : A keyword (str) to distinguish between 4km/8km parquet files. Can set to '4km'/'8km'.\n",
    "    \n",
    "    returns: Compiled single dataframe.\n",
    "    \"\"\"\n",
    "    parquet_files = glob(os.path.join(parquet_folder, '*.parquet'))\n",
    "    for parq in parquet_files:\n",
    "        df = pd.read_parquet(parq)\n",
    "        df = df.drop(columns=['index'])\n",
    "        if parq == parquet_files[0]:\n",
    "            compiled_df = df\n",
    "        else:\n",
    "            compiled_df = compiled_df.merge(df, on=['date', 'lat', 'lon'])\n",
    "    \n",
    "    output_parquet = os.path.join(output_folder, f'{save_keyword}_era5_daily_data.parquet')\n",
    "    compiled_df.to_parquet(output_parquet)\n",
    "    \n",
    "    return compiled_df\n",
    "        \n",
    "    \n",
    "def combine_twc_era5_datasets(twc_dataset, era5_dataset, output_file, merge_on=['date', 'lat', 'lon'], how='inner'):\n",
    "    \"\"\"\n",
    "    Combine twc era5 datasets.\n",
    "    \n",
    "    params:\n",
    "    twc_dataset : TWC dataframe filepath or dataframe.\n",
    "    era5_datset : ERA5 dataframe filepath or dataframe.\n",
    "    output_file : Combined TWC and ERA5 dataframe output filepath.\n",
    "    merge_on : List of columns to use in dataframe merging. Default set to ['date', 'lat', 'lon'].\n",
    "    how : Type of merging. Default set to 'inner'.\n",
    "    \n",
    "    returns: Combined TWC and ERA5 dataframe.\n",
    "    \"\"\"\n",
    "    if isinstance(twc_dataset, pd.DataFrame):\n",
    "        twc_df = twc_dataset\n",
    "        era5_df = era5_dataset\n",
    "    else:\n",
    "        twc_df = pd.read_parquet(twc_dataset)\n",
    "        era5_df = pd.read_parquet(era5_dataset)\n",
    "    \n",
    "    twc_era5_combined = twc_df.merge(era5_df, on=merge_on, suffixes=('_twc', '_era5'), how=how)\n",
    "    twc_era5_combined.to_parquet(output_file)\n",
    "    \n",
    "    return twc_era5_combined\n",
    "            \n",
    "            \n",
    "def resample_weather_datasets_to_100m(variables_to_resample, input_data_main_dir, resampled_output_main_dir, \n",
    "                                     target_raster):\n",
    "    \"\"\"\n",
    "    Resample weather datasets (TWC/ERA5) to 100m resolution using a reference rastr of 100m resolution.\n",
    "    \n",
    "    params:\n",
    "    variables_to_resample : List of weather variables to resample. \n",
    "    input_data_main_dir : Data main directory path. Subdirectories for each variable will be selected by the code.\n",
    "    resampled_output_main_dir : Resampled output data main directory. Subdirectories for each variable will be selected by the code.\n",
    "    target_raster : 100m ref raster. Can set to cities_california_100m_refraster. \n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    # resampling data only the required variables\n",
    "    for var in variables_to_resample:\n",
    "        print(f'Resampling data for {var}...')\n",
    "        variable_dir = os.path.join(input_data_main_dir, var)\n",
    "        all_rasters = glob(os.path.join(variable_dir, '*.tif'))\n",
    "\n",
    "        resampled_output_folder =  os.path.join(resampled_output_main_dir, var)\n",
    "        makedirs([resampled_output_main_dir])\n",
    "        \n",
    "        for raster in all_rasters:\n",
    "            raster_name = os.path.basename(raster).split('.')[0] + '.tif'\n",
    "            resample_raster_based_on_ref_raster(input_raster=raster, ref_raster=target_raster, \n",
    "                                                output_dir=resampled_output_folder, raster_name=raster_name,\n",
    "                                                resampling_alg=Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2366d",
   "metadata": {},
   "source": [
    "# Soil Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f2764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_from_h3(row):\n",
    "    \"\"\"\n",
    "    create point coords for h3 index. This can be further processed to polygon.\n",
    "    \n",
    "    params:\n",
    "    row : dataframe row. \n",
    "    \n",
    "    returns: Tuple of point coords of h3 index.\n",
    "    \"\"\"\n",
    "    points = h3.h3_to_geo_boundary(row['h3'], True)\n",
    "\n",
    "    return points\n",
    "\n",
    "def compile_soil250_data_in_single_dataframe(input_data_dir, output_parquet, search_by):\n",
    "    \"\"\"\n",
    "    Compiles multiple soil250 parquet files for individual aoi/regions to a signle dataframe.\n",
    "    \n",
    "    params:\n",
    "    input_data_dir : Input directory filepath where the parquet files are located.\n",
    "    output_parquet : Filepath of output parquet file.\n",
    "    search_by : For example use \"*V1*.parquet\" for soil250 V1 parquet files. Use \"*V2*.tif\" for soil250 V2 parquet files.\n",
    "    \n",
    "    returns: Compiled single dataframe which is saved as a parquet file.\n",
    "    \"\"\"\n",
    "    soil250_datasets = glob(os.path.join(input_data_dir, search_by))\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for each in soil250_datasets:\n",
    "        df = pd.read_parquet(each)\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # adding polygon geometry based on h3\n",
    "    final_df['points'] = final_df.apply(create_point_from_h3, axis=1)\n",
    "    final_df['geometry'] = final_df['points'].apply(Polygon)\n",
    "    final_df = final_df.drop(columns=['points'])\n",
    "    final_gdf=gpd.GeoDataFrame(final_df, geometry='geometry')\n",
    "    final_gdf = final_gdf.set_crs('EPSG:4326')\n",
    "    \n",
    "    final_gdf.to_parquet(output_parquet)\n",
    "    return final_gdf\n",
    "\n",
    "\n",
    "def process_soil250_data(soil250_file, ref_raster, output_dir,\n",
    "                         remove_cols = ['h3', 'hid', 'hids', 'geometry'],\n",
    "                         interpolation_distance=30):   # while processing also remove the attributes at individual depths if not needed. Average values will be calculated and rasterized\n",
    "    \"\"\"\n",
    "    Process TWC daily data for each column attribute.\n",
    "    \n",
    "    params:\n",
    "    soil250_file : Filepath of parquet file with the attributes. The parquet file should have a \n",
    "                   \"h3\" column to be matched with the grid_shapefile.\n",
    "    ref_raster : Filepath of reference raster to be used in shapefile to raster conversion. \n",
    "                 For example, yolo_county_100m_refraster as we want to resample soil250 data to 100m.\n",
    "    output_dir : Filepath of folder where rasterized data will be saved.\n",
    "    remove_cols : Columns in the TWC file that will not be rasterized.\n",
    "    interpolation_distance : Integer value of how many sorrouding pixels to use in interpolation.  \n",
    "\n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    soil250_gdf = gpd.read_parquet(soil250_file)\n",
    "    \n",
    "    # creating output folder\n",
    "    makedirs([output_dir])\n",
    "    \n",
    "    # Calculating average values of attributes\n",
    "    soil250_attr = list(soil250_gdf.columns)\n",
    "    \n",
    "    if 'awct_0cm' in list(soil250_attr):\n",
    "        soil250_gdf['average_awct'] = soil250_gdf[['awct_0cm', 'awct_5cm', 'awct_15cm', 'awct_30cm']].mean(axis=1)\n",
    "    if 'wwp_0cm' in list(soil250_attr):\n",
    "        soil250_gdf['average_wwp'] = soil250_gdf[['wwp_0cm', 'wwp_5cm', 'wwp_15cm', 'wwp_30cm']].mean(axis=1)\n",
    "    if 'nit0_5' in list(soil250_attr):\n",
    "        soil250_gdf['average_nitrogen'] = soil250_gdf[['nit0_5', 'nit5_15', 'nit15_30']].mean(axis=1)\n",
    "    if 'soc0_5' in list(soil250_attr):\n",
    "        soil250_gdf['average_soc'] = soil250_gdf[['soc0_5', 'soc5_15', 'soc15_30']].mean(axis=1)\n",
    "    \n",
    "    # making list of columns to be rasterized\n",
    "    soil250_attr = list(soil250_gdf.columns)\n",
    "    keep_attr = [i for i in soil250_attr if i not in remove_cols]\n",
    "    \n",
    "    # Looping through each attribute and each date to rasterize the data\n",
    "    for attr_col in keep_attr:\n",
    "        print(f'Processing Soil250 {attr_col} dataset...')\n",
    "        \n",
    "        # making new output directory for specific attribute\n",
    "        process_to_dir = os.path.join(output_dir, attr_col, 'rasterized' )  \n",
    "        makedirs([process_to_dir])    \n",
    "        \n",
    "        output_raster_fp = os.path.join(process_to_dir, f'{attr_col}.tif')\n",
    "\n",
    "        # rasterization\n",
    "        rasterzied_data = rasterize_shapefile(input_file=soil250_gdf, grid_shapefile=None, attribute=attr_col, \n",
    "                                              date=None, ref_raster=ref_raster, output_raster=output_raster_fp, \n",
    "                                              merge_alg = MergeAlg.replace, dtype='float32', no_data_value=-9999)\n",
    "        \n",
    "        # interpolate nodata pixels/gaps\n",
    "        output_gapfilled_raster = os.path.join(output_dir, attr_col, f'{attr_col}.tif')\n",
    "        fill_nodata_by_interpolation(input_raster=rasterzied_data, reference_raster=ref_raster, \n",
    "                                     output_raster=output_gapfilled_raster,\n",
    "                                     use_pixel_to_interp=interpolation_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fb051",
   "metadata": {},
   "source": [
    "## Elevation Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_elevation_data_in_single_dataframe(input_data_dir, output_parquet, search_by='*.parquet'):\n",
    "    \"\"\"\n",
    "    Compiles multiple elevation parquet files for individual aoi/regions to a signle dataframe.\n",
    "    \n",
    "    params:\n",
    "    input_data_dir : Input directory filepath where the parquet files are located.\n",
    "    output_parquet : Filepath of output parquet file.\n",
    "    search_by : Default set to '*.parquet'.\n",
    "    \n",
    "    returns: Compiled single dataframe which is saved as a parquet file.\n",
    "    \"\"\"\n",
    "    elevation_datasets = glob(os.path.join(input_data_dir, search_by))\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    for each in elevation_datasets:\n",
    "        df = pd.read_parquet(each)\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # adding polygon geometry based on h3\n",
    "    final_df['points'] = final_df.apply(create_point_from_h3, axis=1)\n",
    "    final_df['geometry'] = final_df['points'].apply(Polygon)\n",
    "    final_df = final_df.drop(columns=['points'])\n",
    "    final_gdf=gpd.GeoDataFrame(final_df, geometry='geometry')\n",
    "    final_gdf = final_gdf.set_crs('EPSG:4326')\n",
    "    \n",
    "    final_gdf.to_parquet(output_parquet)\n",
    "    return final_gdf\n",
    "\n",
    "\n",
    "def calculate_slope_aspect(dem_filepath, slope_raster_fp, aspect_raster_fp):\n",
    "    \"\"\"\n",
    "    Calculates slope (degress) and aspect (degrees) raster using dem data.\n",
    "    \n",
    "    params:\n",
    "    dem_filepath : DEM raster filepath. \n",
    "    slope_raster_fp : created slope raster filepath.\n",
    "    aspect_raster_fp : created aspect raster filepath.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    dem_arr, dem_file = read_raster_arr_object(dem_filepath)\n",
    "    \n",
    "    # calculating slope and aspect\n",
    "    slope_arr = gg.raster.calculate_slope(dem_file)\n",
    "    aspect_arr = gg.raster.calculate_aspect(dem_file)\n",
    "    \n",
    "    # saving slope and aspect data\n",
    "    slope_outdir = os.path.dirname(slope_raster_fp)\n",
    "    makedirs([slope_outdir])\n",
    "    write_array_to_raster(raster_arr=slope_arr, raster_file=dem_file, transform=dem_file.transform, \n",
    "                          output_path=slope_raster_fp)\n",
    "    \n",
    "    aspect_outdir = os.path.dirname(aspect_raster_fp)\n",
    "    makedirs([aspect_outdir])\n",
    "    write_array_to_raster(raster_arr=aspect_arr, raster_file=dem_file, transform=dem_file.transform, \n",
    "                          output_path=aspect_raster_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036a44a",
   "metadata": {},
   "source": [
    "## Satellite Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567bde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raster_as_single_band(input_raster, output_dir,  \n",
    "                               scale=False, scale_factor=0.001, \n",
    "                               add_value=False, subtract_value=False, value=None, \n",
    "                               change_dtype=False,  reformat_date=True,\n",
    "                               nodata=no_data_value):\n",
    "    \"\"\"\n",
    "    Read a raster with multiple band and save only band 1. Can perform datatype change and scaling if enabled. \n",
    "    \n",
    "    params:\n",
    "    input_raster : Input raster filepath.\n",
    "    output_dir : Output directory filepath where processed rasters will be saved.\n",
    "    scaling : Set to true if data need to be scaled. Default set to False.\n",
    "    scale_factor : Scaling factor to multiply with if scaling=True.\n",
    "    add_value : Set to True if want add a value. Default set to False.\n",
    "    subtract_value : Set to True if want subtract a value. Default set to False.\n",
    "    value : Value to add or subtract. Always put positive value. based on whether value to add or subtract chose add_value or subtract_value.\n",
    "            Default set to None.\n",
    "    change_dtype: Set to True if want to change raster data type to float. Default set to False.\n",
    "    reformat_date: Set to to format date. Default set to True.\n",
    "    no_data_value : No data value assigned to raster. Default set to -9999.\n",
    "    \n",
    "    returns: The output raster.\n",
    "    \"\"\"\n",
    "    makedirs([output_dir])\n",
    "    raster_arr, raster_file = read_raster_arr_object(input_raster)\n",
    "    \n",
    "    if change_dtype:  # changing dtype to float32\n",
    "        raster_arr = raster_arr.astype(np.float32)\n",
    "    \n",
    "    if scale: #scaling data\n",
    "        raster_arr[~np.isnan(raster_arr)] *= scale_factor\n",
    "    \n",
    "    if add_value: # adding a value\n",
    "        raster_arr[~np.isnan(raster_arr)] += value\n",
    "    \n",
    "    if subtract_value: # subtracing a value \n",
    "        raster_arr[~np.isnan(raster_arr)] -= value\n",
    "    \n",
    "    \n",
    "    raster_name = os.path.basename(input_raster).split('.')[0] + '.tif'\n",
    "    \n",
    "    # Formatting final raster name (including similarizing date pattern)\n",
    "    # this is set considering data format of SMAP L-band soil moisture and temperature file format\n",
    "    if reformat_date:\n",
    "        main_raster_name = raster_name[:raster_name.rfind('_')]\n",
    "        date = ''.join(raster_name.split('_')[-1].split('.')[0].split('-'))\n",
    "        if main_raster_name[-1] == '_':\n",
    "            raster_name = main_raster_name + date + '.tif'\n",
    "        else:\n",
    "            raster_name = main_raster_name + '_' + date + '.tif'\n",
    "    \n",
    "    \n",
    "    output_path = os.path.join(output_dir, raster_name)\n",
    "    \n",
    "    with rio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=raster_arr.shape[0],\n",
    "            width=raster_arr.shape[1],\n",
    "            dtype=raster_arr.dtype,\n",
    "            count=1,\n",
    "            crs=raster_file.crs,\n",
    "            transform=raster_file.transform,\n",
    "            nodata=nodata\n",
    "    ) as dst:\n",
    "        dst.write(raster_arr, 1)\n",
    "        \n",
    "    return output_path\n",
    "\n",
    "def harmonize_resample_satellite_data(input_data_dir, search_by, main_output_dir, height, width, ref_raster, \n",
    "                                      scale_factor, scale=True, add_value=False, subtract_value=False, value=None,\n",
    "                                      change_dtype=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract single band info and harmonize (same height and width) satellite data.\n",
    "    \n",
    "    ***Look into the raw data and determine a data as reference raster. All data will be harmonized based on that.\n",
    "    \n",
    "    params: \n",
    "    input_data_dir : Filepath of input data directory.\n",
    "    search_by : search keywork for selecting datasets. For example: '*.tif'.\n",
    "    main_output_dir : Filepath of output data main directory. A sub-directory with intermediate single band data will be created by code.\n",
    "    height, width :Height and width integer.\n",
    "    ***ref_raster*** : Filepath of input raster to be used in determining resample affine transformation/crs/dtype/nodata.\n",
    "    scaling : Set to true if data need to be scaled. Default set to True.\n",
    "    scale_factor : Scaling factor to multiply with if scaling=True.\n",
    "    add_value : Set to True if want add a value. Default set to False.\n",
    "    subtract_value : Set to True if want subtract a value. Default set to False.\n",
    "    value : Value to add or subtract. Always put positive value. based on whether value to add or subtract chose add_value or subtract_value.\n",
    "            Default set to None.\n",
    "    change_dtype: Set to True if want to change raster data type to float. Default set to True.\n",
    "        \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_rasters = glob(os.path.join(input_data_dir, search_by))\n",
    "    \n",
    "    single_band_output_dir = os.path.join(main_output_dir, 'single_band')\n",
    "    makedirs([single_band_output_dir])\n",
    "    \n",
    "    for raw_data in raw_rasters: \n",
    "        single_band_raster = save_raster_as_single_band(input_raster=raw_data, output_dir=single_band_output_dir, \n",
    "                                                        change_dtype=change_dtype, scale=scale, scale_factor=scale_factor,\n",
    "                                                        add_value=add_value, subtract_value=subtract_value, value=value, \n",
    "                                                        nodata=-9999)\n",
    "        \n",
    "        raster_name = os.path.basename(raw_data).split('.')[0] + '.tif'\n",
    "               \n",
    "        resample_raster_with_height_width(input_raster=single_band_raster, height=height, width=width, \n",
    "                                          ref_raster=ref_raster, output_dir=main_output_dir, \n",
    "                                          raster_name=raster_name, resampling_alg=Resampling.bilinear)\n",
    "        \n",
    "        \n",
    "def mask_resample_weather_datasets_to_bbox(variables_to_resample, input_data_main_dir, output_main_dir, bounding_box,\n",
    "                                           target_raster):\n",
    "    \"\"\"\n",
    "    Mask and resample weather rasters (resampled 100m resolution) with a bounding box and target satellite data (100m).\n",
    "    \n",
    "    ##########################\n",
    "    Masking/Resampling Weather data for woodland site. Saving maked data to an interim directory and resampling it again.\n",
    "    Ideally, this should work with only masking (without resamplig). But rasterio.mask is causing this issue\n",
    "    gdal.warp could have done it correctly (tested by QGIS) but I couldn't figure out gdal in AWS\n",
    "    So, I am trying a different approach by first masking the era5 data to using bounding box and then resampling it \n",
    "    using satellite data.\n",
    "    ##########################\n",
    "    \n",
    "    params:\n",
    "    variables_to_resample : List of weather variables to resample. \n",
    "    input_data_main_dir : Data main directory path. Subdirectories for each variable will be selected by the code.\n",
    "    output_main_dir : Output data main directory where masked and resampled data will be saved. \n",
    "                      Subdirectories for each variable will be selected by the code.\n",
    "    target_raster : 100m ref raster. Should be a satellite raster of 100m resolution that is inside the bounding box. \n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "\n",
    "    # resampling data only the required variables\n",
    "    for var in variables_to_resample:\n",
    "        print(f'Masking and resampling data for {var}...')\n",
    "        variable_dir = os.path.join(input_data_main_dir, var)\n",
    "        all_rasters = glob(os.path.join(variable_dir, '*.tif'))\n",
    "        \n",
    "        masked_output_dir =  os.path.join(output_main_dir, var, 'interim') #saving initial masked rasters in an 'interim folder' inside each variable's folder\n",
    "        resampled_output_dir =  os.path.join(output_main_dir, var)\n",
    "        makedirs([masked_output_dir])\n",
    "        \n",
    "        for raster in all_rasters:\n",
    "            raster_name = os.path.basename(raster).split('.')[0] + '.tif'\n",
    "            masked_raster_fp = os.path.join(masked_output_dir, raster_name)\n",
    "            # masking to bounding box\n",
    "            mask_raster_array_by_shapefile(input_raster=raster, mask_shape=bounding_box, \n",
    "                                             output_dir=masked_output_dir, raster_name=raster_name, \n",
    "                                             invert=False, crop=True, save_masked_arr=True)\n",
    "            # resampling to make sure the processed raster is inside the bounding box\n",
    "            resample_raster_based_on_ref_raster(input_raster=masked_raster_fp, ref_raster=target_raster, \n",
    "                                                output_dir=resampled_output_dir, raster_name=raster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55860391",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2e7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(input_dir_file, copy_dir, search_by='*.tif', rename=None):\n",
    "    \"\"\"\n",
    "    Copy a file to the specified directory.\n",
    "\n",
    "    :param input_dir_file: File path of input directory/ Path of the file to copy.\n",
    "    :param copy_dir: File path of copy directory.\n",
    "    :param search_by: Default set to '*.tif'.\n",
    "    :param rename: New name of file if required. Default set to None. DOesn't work if a directory is being copied.\n",
    "\n",
    "    :returns: File path of copied file.\n",
    "    \"\"\"\n",
    "    makedirs([copy_dir])\n",
    "    if '.tif' not in input_dir_file:\n",
    "        input_files = glob(os.path.join(input_dir_file, search_by))\n",
    "\n",
    "        for each in input_files:\n",
    "            file_name = os.path.basename(each)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "            shutil.copyfile(each, copy_file)\n",
    "\n",
    "    else:\n",
    "        if rename is not None:\n",
    "            copy_file = os.path.join(copy_dir, f'{rename}.tif')\n",
    "        else:\n",
    "            file_name = os.path.basename(input_dir_file)\n",
    "            copy_file = os.path.join(copy_dir, file_name)\n",
    "\n",
    "        shutil.copyfile(input_dir_file, copy_file)\n",
    "\n",
    "    return copy_file\n",
    "\n",
    "\n",
    "def makedirs(directory_list):\n",
    "    \"\"\"\n",
    "    Make directory (if not exists) from a list of directory.\n",
    "\n",
    "    :param directory_list: A list of directories to create.\n",
    "\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    for directory in directory_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "def make_folder_in_s3_bucket(new_folder_path, bucket_name='data-pipeline-env-model'):\n",
    "    \"\"\"\n",
    "    Make directory/folder in AWS S3 Bucket.\n",
    "    \n",
    "    params:\n",
    "    new_folder_path : Folder path to create in the S3 bucket. Have to be like this \"Main_folder/subfolder\"\n",
    "    bucket_name : S3 bucket name. Default set to 'data-pipeline-env-model'\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket_name, Key=(new_folder_path+'/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17744b08",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f19215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dated_images_rasters(list_of_images, num_cols, figsize=(8, 6), shapefile=None,\n",
    "                              title=None):\n",
    "    \"\"\"\n",
    "    Plot images with date information. Date should be in the end like this '20211231'\n",
    "    \n",
    "    params:\n",
    "    list_of_images : List of images filepath to plot.\n",
    "    num_cols : number of columns in the plot.\n",
    "    figsize : Figsise. Default set to (8, 6).\n",
    "    shapefile : Shapefile filepath if want to show in plot. Default set to None.\n",
    "    title : Title string. Default set to None.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    # setting number of row\n",
    "    if len(list_of_images)%num_cols == 0:\n",
    "        num_rows = len(list_of_images)//num_cols\n",
    "    else:\n",
    "        num_rows = len(list_of_images)//num_cols + 1\n",
    "    \n",
    "    if shapefile is not None:\n",
    "        gdf = gpd.read_file(shapefile)\n",
    "    \n",
    "    # define the figures and axes\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)\n",
    "    \n",
    "    for image in list_of_images:\n",
    "        # extarcting date\n",
    "        date = os.path.basename(image).split('.')[0].split('_')[-1]\n",
    "        year, month, date = date[:4], date[4:6], date[6:]\n",
    "        date = f'{year}_{month}_{date}'\n",
    "        \n",
    "        img = rio.open(image)\n",
    "        \n",
    "        row = list_of_images.index(image)//num_cols\n",
    "        col = list_of_images.index(image)%num_cols\n",
    "        \n",
    "        if shapefile is not None:\n",
    "            gdf.plot(facecolor='none', edgecolor='black', ax=ax[row, col])\n",
    "        \n",
    "        show(img, ax=ax[row, col], cmap='Spectral_r')\n",
    "        ax[row, col].set_title(f'Date {date}', fontsize=9)\n",
    "        \n",
    "    if title is not None:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "        \n",
    "def plot_dated_images_scatter(images_list_data1, images_list_data2, num_cols, figsize=(8, 6), \n",
    "                                      title=None, xlabel=None, ylabel=None):\n",
    "    \"\"\"\n",
    "    Plot scatter plot of two list of images.Each image should have date info the end like this '20211231'\n",
    "    \n",
    "    params:\n",
    "    images_list_data1 : 1st List of images filepath to plot.\n",
    "    images_list_data2 : 2nd List of images filepath to plot.\n",
    "    num_cols : number of columns in the plot.\n",
    "    figsize : Figsise. Default set to (8, 6).\n",
    "    title, xlabel, ylabel : Title, xlabel, ylabel string. Default set to None.\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # setting number of row\n",
    "    if len(images_list_data1)%num_cols == 0:\n",
    "        num_rows = len(images_list_data1)//num_cols\n",
    "    else:\n",
    "        num_rows = len(images_list_data1)//num_cols + 1\n",
    "    \n",
    "    # define the figures and axes\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)\n",
    "    \n",
    "    for image1 in images_list_data1:\n",
    "        # extarcting date\n",
    "        date = os.path.basename(image1).split('.')[0].split('_')[-1]\n",
    "        \n",
    "        # loop to find data of matching dates in 2nd image list \n",
    "        for image2 in images_list_data2:    \n",
    "            if date in image2:\n",
    "                # formatting date\n",
    "                year, month, dateee = date[:4], date[4:6], date[6:]\n",
    "                date_fmt = f'{year}_{month}_{dateee}'\n",
    "                \n",
    "                # opening matching images\n",
    "                img1 = rio.open(image1).read(1).flatten()\n",
    "                img2 = rio.open(image2).read(1).flatten()\n",
    "                \n",
    "                # filtering out nan or -9999 values\n",
    "                img_df = pd.DataFrame({'img1': img1, 'img2': img2})\n",
    "                img_df = img_df.dropna()\n",
    "                img_df = img_df[(img_df['img1'] != -9999) & (img_df['img2'] != -9999)]\n",
    "                \n",
    "                # calculating r2 score between the variables\n",
    "                r2_val = r2_score(img_df.img1, img_df.img2)  \n",
    "                \n",
    "                # deciding [row, col] index of the plot\n",
    "                row = images_list_data1.index(image1)//num_cols\n",
    "                col = images_list_data1.index(image1)%num_cols\n",
    "        \n",
    "                # scatter plot\n",
    "                ax[row, col].plot(img_df.img1, img_df.img2, 'b.', alpha=0.05)\n",
    "                \n",
    "                # setting xlabel, ylabel min max\n",
    "                maxx = max(img_df.img1.max(), img_df.img2.max())\n",
    "                maxy = max(img_df.img2.max(), img_df.img2.max())\n",
    "                minx = min(img_df.img1.min(), img_df.img2.min())\n",
    "                miny = min(img_df.img1.min(), img_df.img2.min())\n",
    "                \n",
    "                ax[row, col].set_xlim(minx, maxx)\n",
    "                ax[row, col].set_ylim(miny, maxx)\n",
    "                \n",
    "                # including R2 value\n",
    "                if maxx>5:\n",
    "                    min_pos = minx+0.1\n",
    "                    max_pos = maxx-1\n",
    "                else:\n",
    "                    min_pos = 0.01\n",
    "                    max_pos = maxx-0.3\n",
    "                ax[row, col].annotate(text='R2 = {:.3f}'.format(r2_val), xy=(min_pos, max_pos), fontsize=8)\n",
    "                \n",
    "                # setting 1:1 line\n",
    "                pt = (0, 0)\n",
    "                ax[row, col].axline(pt, slope=1, color='red', linewidth=0.3)\n",
    "                \n",
    "                # setting individual plot title\n",
    "                ax[row, col].set_title(f'Date {date_fmt}', fontsize=9)\n",
    "                \n",
    "                # setting xlabel and ylabel\n",
    "                if (xlabel is not None) & (ylabel is not None):\n",
    "                    ax[row, col].set_xlabel(xlabel, fontsize=9)\n",
    "                    ax[row, col].set_ylabel(ylabel, fontsize=9)\n",
    "                \n",
    "        # setting overall plot title \n",
    "        if title is not None:\n",
    "            plt.suptitle(title)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69795d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### shapefile to raster using geocube, can rasterize and resample (cubic/neareast/linear) at the same time\n",
    "\n",
    "# from geocube.api.core import make_geocube\n",
    "# from functools import partial\n",
    "# from geocube.rasterize import rasterize_points_griddata\n",
    "\n",
    "# twc_weather_shape = '../datasets/weather_data_shapefiles/twc_weatherdata_California.parquet'\n",
    "# twc_grid = '../datasets/weather_data_shapefiles/twcGrid_California.shp'\n",
    "\n",
    "# data_gdf = read_twc_parquet_save_as_geodataframe(parquet_file=twc_weather_shape, twc_grid_geometry_file=twc_grid,\n",
    "#                                                 save=False)\n",
    "\n",
    "# data_gdf = data_gdf[data_gdf['date']=='2021-01-01']\n",
    "\n",
    "# converted_raster = make_geocube(\n",
    "#         vector_data=data_gdf,\n",
    "#         measurements=[\"total_precip\"],\n",
    "#         resolution=(-0.036, 0.036),\n",
    "#         fill = no_data_value,\n",
    "#     rasterize_function=partial(rasterize_points_griddata, method=\"nearest\"))\n",
    "    \n",
    "# # Save raster census raster\n",
    "# converted_raster.rio.to_raster('../datasets/reference_rasters/twc_precip2.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a583ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
