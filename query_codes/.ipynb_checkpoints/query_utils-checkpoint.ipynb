{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a38688e",
   "metadata": {},
   "source": [
    "### Author: Md Fahim Hasan\n",
    "### Work Email: mdfahim.hasan@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf12836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "import os\n",
    "import re\n",
    "import s3fs\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import pandas_gbq as gbq\n",
    "from shapely.geometry import Polygon\n",
    "from google.cloud import bigquery\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769342dd",
   "metadata": {},
   "source": [
    "# Read me\n",
    "\n",
    "__This scripts consist of all functions required for querying weather, soil, and elevation data from CSW. It also has the assiting functions required for the querying process.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00821b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.oauth2.service_account.Credentials at 0x7fc04bf99750>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating credentials using service account information\n",
    "from cswCredentials import svc_secret\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "service_account_creds = json.loads(base64.b64decode(svc_secret))\n",
    "bq_project = 'location360-datasets'\n",
    "credentials = service_account.Credentials.from_service_account_info(service_account_creds)\n",
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05789b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l0 : country name\n",
    "# l1 : state/provinve/region name\n",
    "# l2: county/district/division name\n",
    "\n",
    "def extract_geom_info_by_country_state(countries_code, states, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Extract geometry information (dataframe) with country_code and state name.\n",
    "    \n",
    "    params:\n",
    "    country_code : list of str of country, e.g., ['MX'].\n",
    "    states : list of str (multiple states) with city names.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried result.\n",
    "    \"\"\" \n",
    "   \n",
    "    geom_query_df = pd.DataFrame()\n",
    "    for country_code, state in zip(countries_code, states):\n",
    "\n",
    "        query = f\"\"\"\n",
    "                SELECT l0_name as country, l0_iso_code as country_code, l1_name as state_province, l2_name as county_district, l3_name as subdistrict, geog as geometry\n",
    "                FROM location360-datasets.geopolitical.global_l3 as l3\n",
    "                WHERE l3.l0_iso_code = '{country_code}'\n",
    "                AND l3.l1_name IN ('{state}')\n",
    "                \"\"\"\n",
    "        temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "        geom_query_df = pd.concat([geom_query_df, temp_query])\n",
    "        \n",
    "    return geom_query_df\n",
    "\n",
    "\n",
    "def extract_L3_geom_by_country_state(output_shapefile, country_code, state_name,\n",
    "                                     bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Extract L3 geometry information with country code, state name and save as shapefile.\n",
    "    \n",
    "    params:\n",
    "    output_shapefile : Filepath of output shapefile.\n",
    "    country code : Country code for example 'US'.\n",
    "    state_name : State/district name for example 'California'.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried result. ALso, saves the geodataframe as shapefile.\n",
    "    \"\"\"   \n",
    "    query = f\"\"\"\n",
    "            SELECT l0_name as country, l0_iso_code as country_code, l1_name as state_province, l2_name as county_district, l3_name as city, geog as geometry\n",
    "            FROM location360-datasets.geopolitical.global_l3 as l3\n",
    "            WHERE l3.l0_iso_code = '{country_code}'\n",
    "            AND l3.l1_name IN ('{state_name}')\n",
    "            \"\"\"\n",
    "    geom_query_df = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    geom_query_df['geometry'] =  geom_query_df['geometry'].apply(wkt.loads) # converting to wkt is required to create a GeoDataframe\n",
    "    geom_query_gdf = gpd.GeoDataFrame(geom_query_df, geometry='geometry')\n",
    "    \n",
    "    # saving \n",
    "    geom_query_gdf.to_file(output_shapefile)\n",
    "        \n",
    "    return geom_query_gdf\n",
    "\n",
    "\n",
    "def extract_geom_info_by_country_state_city(info_df, county_ID_col='country_id', l1_name_col='state', lat_col='latitude', \n",
    "                                            lon_col='longitude', bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Extract geometry information (dataframe) with country_code, state name, and city's lat lon information.\n",
    "    \n",
    "    params:\n",
    "    info_df : pandas dataframe/csv filepath with information like country_code, state_name, and lat-lon.\n",
    "    county_ID_col : column name with country_code in the given dataframe.\n",
    "    l1_name_col : column name with level 1 admin name (state/province) in the given dataframe.\n",
    "    lat_col : column name with latitude in the given dataframe.\n",
    "    lon_col : column name with longitude in the given dataframe.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried result.\n",
    "    \"\"\" \n",
    "    if isinstance(info_df, str) and '.csv' in info_df:\n",
    "        info_df = pd.read_csv(info_df)\n",
    "    country_code = info_df[county_ID_col]\n",
    "    l1_name = info_df[l1_name_col]\n",
    "    lat = info_df[lat_col]\n",
    "    lon = info_df[lon_col]\n",
    "        \n",
    "    geom_query_df = pd.DataFrame()\n",
    "    for country_code, state, x, y in zip(country_code, l1_name, lon, lat):\n",
    "        query = f\"\"\"\n",
    "                SELECT l0_name as country, l0_iso_code as country_code, l1_name as state_province, l2_name as county_district, geom as geometry\n",
    "                FROM location360-datasets.geopolitical.global_l2 as l2\n",
    "                WHERE l2.l0_iso_code = '{country_code}'\n",
    "                AND l2.l1_name IN ('{state}')\n",
    "                AND ST_Intersects(geom, ST_GEOGPOINT({x},{y}))\n",
    "                \"\"\"\n",
    "        temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "        geom_query_df = pd.concat([geom_query_df, temp_query])\n",
    "        \n",
    "    return geom_query_df\n",
    "\n",
    "def get_min_max_bound_of_state(country_code, state_name, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Extract minx, miny, maxx, maxy bounds with country_code and state_name.\n",
    "    \n",
    "    params:\n",
    "    country_code : str of country code, e.g., ['MX'].\n",
    "    state_name : str of state name.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: minx, miny, maxx, maxy bounds as a tuple.\n",
    "    \"\"\" \n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT l0_name as country, l0_iso_code as country_code, l1_name as state_province, geom as geometry\n",
    "        FROM location360-datasets.geopolitical.global_l1 as l1\n",
    "        WHERE l1.l0_iso_code = '{country_code}'\n",
    "        AND l1.l1_name IN ('{state_name}')\n",
    "        \"\"\"\n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads) # converting to wkt is required to create a GeoDataframe\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "    bounds_df = query_gdf['geometry'].bounds\n",
    "    \n",
    "    minx, miny, maxx, maxy = bounds_df['minx'].values[0], bounds_df['miny'].values[0], bounds_df['maxx'].values[0], \\\n",
    "    bounds_df['maxy'].values[0]\n",
    "    \n",
    "    return minx, miny, maxx, maxy\n",
    "\n",
    "\n",
    "def get_min_max_bound_of_county(country_code, county_name, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Extract minx, miny, maxx, maxy bounds with country_code and county_name.\n",
    "    \n",
    "    params:\n",
    "    country_code : str of country code, e.g., ['MX'].\n",
    "    county_name : str of county_name.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: minx, miny, maxx, maxy bounds as a tuple.\n",
    "    \"\"\" \n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT l0_name as country, l0_iso_code as country_code, l1_name as state, l2_name as county, geom as geometry\n",
    "            FROM location360-datasets.geopolitical.global_l2 as l2\n",
    "            WHERE l2.l0_iso_code = '{country_code}'\n",
    "            AND l2.l2_name IN ('{county_name}')\n",
    "            \"\"\"\n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads) # converting to wkt is required to create a GeoDataframe\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "    bounds_df = query_gdf['geometry'].bounds\n",
    "    \n",
    "    minx, miny, maxx, maxy = bounds_df['minx'].values[0], bounds_df['miny'].values[0], bounds_df['maxx'].values[0], \\\n",
    "    bounds_df['maxy'].values[0]\n",
    "    \n",
    "    return minx, miny, maxx, maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53fc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shapefile_for_state(country_code, state_name, output_folder, savename, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Create shapefile with country_code and state_name.\n",
    "    \n",
    "    params:\n",
    "    country_code : str of country code, e.g., ['MX'].\n",
    "    state_name : str of state_name.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried shapefile.\n",
    "    \"\"\" \n",
    "        \n",
    "    query = f\"\"\"\n",
    "        SELECT l0_name as country, l0_iso_code as cntry_code, l1_name as state, geom as geometry\n",
    "        FROM location360-datasets.geopolitical.global_l1 as l1\n",
    "        WHERE l1.l0_iso_code = '{country_code}'\n",
    "        AND l1.l1_name IN ('{state_name}')\n",
    "        \"\"\"\n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads) # converting to wkt is required to create a GeoDataframe\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    query_gdf.to_file(savefile)\n",
    "    \n",
    "    return query_gdf\n",
    "\n",
    "def create_shapefile_for_county(country_code, county_name, output_folder, savename, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Create shapefile with country_code and county_name.\n",
    "    \n",
    "    params:\n",
    "    country_code : str of country code, e.g., ['MX'].\n",
    "    county_name : str of county_name.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried shapefile.\n",
    "    \"\"\" \n",
    "    \n",
    "    query = f\"\"\"\n",
    "            SELECT l0_name as country, l0_iso_code as cntry_code, l1_name as state, l2_name as county, geom as geometry\n",
    "            FROM location360-datasets.geopolitical.global_l2 as l2\n",
    "            WHERE l2.l0_iso_code = '{country_code}'\n",
    "            AND l2.l2_name IN ('{county_name}')\n",
    "            \"\"\"\n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads)  # converting to wkt is required to create a GeoDataframe\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "\n",
    "    makedirs([output_folder])\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    query_gdf.to_file(savefile)\n",
    "\n",
    "    return query_gdf\n",
    "\n",
    "def clip_grids_by_admin(grids_file, admin_file, output_folder, savename):\n",
    "    \"\"\"\n",
    "    Clip a shapefile/geodataframe with another shapefile//geodataframe.\n",
    "    \n",
    "    \n",
    "    *****the grids_file and admin_file both has to be same type. Either shapefile path/geodataframe*****\n",
    "    \n",
    "    params:\n",
    "    grids_file : shapefile path/geodataframe with twc_grid/era5_rid information.\n",
    "    admin_file : shapefile path/geodataframe used to clip the grids_file.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    \n",
    "    returns: geopandas dataframe of cliiped shapefile/geodataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if '.shp' not in grids_file:  # the grids_file and admin_file both has to be same type. Either shapefile path/geodataframe\n",
    "        grids_df = grids_file\n",
    "        admin_df = admin_file\n",
    "    else:\n",
    "        grids_df = gpd.read_file(grids_file)\n",
    "        admin_df = gpd.read_file(admin_file)\n",
    "\n",
    "        \n",
    "    clipped_gdf = gpd.clip(grids_df['geometry'], admin_df['geometry'])\n",
    "    clipped_gdf = gpd.GeoDataFrame(clipped_gdf, geometry='geometry')\n",
    "    clipped_gdf = clipped_gdf.join(grids_df, on=None, how='left', lsuffix='', rsuffix='R')  # merging lost grids_df info to the clipped grids \n",
    "    clipped_gdf = clipped_gdf.drop(columns=['geometry', 'geometryR'])\n",
    "    clipped_gdf = gpd.GeoDataFrame(clipped_gdf, geometry=gpd.points_from_xy(clipped_gdf.lon, clipped_gdf.lat), \n",
    "                                   crs=\"EPSG:4326\")\n",
    "    clipped_gdf = clipped_gdf.dropna()\n",
    "    clipped_gdf = clipped_gdf.reset_index()\n",
    "    \n",
    "    makedirs([output_folder])\n",
    " \n",
    "    if '.shp' not in savename:\n",
    "        savename = savename + '.shp'\n",
    "        \n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    clipped_gdf.to_file(savefile)\n",
    "    \n",
    "    return clipped_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdd3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_to_h3(row):\n",
    "    \"\"\"\n",
    "    create polygon geometry for h3 index.\n",
    "    \n",
    "    params:\n",
    "    row : dataframe row. \n",
    "    \n",
    "    returns: polygon geometry for h3 index.\n",
    "    \"\"\"\n",
    "    points = h3.h3_to_geo_boundary(row['h3'], True)\n",
    "    \n",
    "    return Polygon(points)\n",
    "\n",
    "def make_h3_hid_using_geodataframe(geom_df, output_folder, savename, h3_level=10, polygon_col='geometry'):\n",
    "    \"\"\"\n",
    "    Create h3 index and hid using geometry info from a geodataframe.\n",
    "    \n",
    "    params:\n",
    "    geom_df : A geodataframe with polygon geometry.\n",
    "    output_folder : Filepath of output folder to save h3 and hid information as a shapefile.\n",
    "    savename : Name of h3_hid shapefile that will be saved.\n",
    "    h3_level : h3 level. Default set to 10. \n",
    "    polygon_col : polygon column in the geodataframe. Default set to 'geometry'.\n",
    "    \n",
    "    returns: A geodataframe with h3 and hid information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(geom_df, gpd.geodataframe.GeoDataFrame):\n",
    "        geom_df['geometry'] = geom_df[polygon_col].apply(wkt.loads) \n",
    "        geom_df_gpd = gpd.GeoDataFrame(geom_df, geometry='geometry')\n",
    "\n",
    "    else:\n",
    "        geom_df_gpd = geom_df\n",
    "        \n",
    "    h3_hid_df = pd.DataFrame()\n",
    "    for poly in geom_df_gpd['geometry']:\n",
    "        query = f\"\"\"\n",
    "                SELECT environmental_data_cube.h3_2_hid(h3, {h3_level}) as hids, h3 as h3\n",
    "                FROM UNNEST(environmental_data_cube.generate_h3_by_wkt_polygon('{poly}', {h3_level})) as h3;\n",
    "                \"\"\"\n",
    "        result_query = gbq.read_gbq(query, project_id='location360-datasets', credentials=credentials)\n",
    "        h3_hid_df = pd.concat([h3_hid_df, result_query])\n",
    "\n",
    "    h3_hid_df['geometry'] = h3_hid_df.apply(add_geometry_to_h3, axis=1)\n",
    "    h3_hid_gdf = gpd.GeoDataFrame(h3_hid_df, geometry='geometry')\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    h3_hid_gdf.to_file(savefile)\n",
    "    \n",
    "    return h3_hid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531e4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h3_parquet_as_geodataframe(parquet_file, h3_geometry_file, save=False, output_folder=None, savename=None):\n",
    "    \"\"\"\n",
    "    Read parquet file with h3 information and save it as a geodataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_file : Filepath of parquet file. Must have h3 information.\n",
    "    h3_geometry-file: Filepath of h3 geometry file. Must have matching h3 information with the parquet file and geometry info.\n",
    "    save : Set to true if want to save data as geodataframe/shapefile.\n",
    "    output_folder : str of output folder to save the data. Default set to None.\n",
    "    savename : str of name of the shapefile. Default set to None.\n",
    "    \n",
    "    returns: geopandas dataframe with data information.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_parq = pd.read_parquet(parquet_file) # must have h3 information\n",
    "    df_h3 = gpd.read_file(h3_geometry_file) # must have matching h3 information with the parquet file and geometry info\n",
    "    \n",
    "    df_compiled = df_parq.merge(df_h3, on='h3', how='inner')\n",
    "    gdf_compiled = gpd.GeoDataFrame(df_compiled, geometry='geometry')\n",
    "    \n",
    "    if save:\n",
    "        makedirs([output_folder])\n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        gdf_compiled.to_file(savefile)\n",
    "    \n",
    "    return gdf_compiled\n",
    "\n",
    "def read_parquet_as_geodataframe(parquet_file, grid_geometry_file, save=False, output_folder=None, savename=None):\n",
    "    \"\"\"\n",
    "    Read parquet file with twc/era5 grid information and save it as a geodataframe.\n",
    "    \n",
    "    params:\n",
    "    parquet_file : Filepath of parquet file. Must have twc grid information.\n",
    "    grid_geometry-file: Filepath of twc/era5 grid geometry file. Must have matching twc grid information with the \n",
    "                            parquet file and geometry info.\n",
    "    save : Set to true if want to save data as geodataframe/shapefile. Large files will have error (dieing kernel). \n",
    "           Better to not save when facing such issues.\n",
    "    output_folder : str of output folder to save the data. Default set to None.\n",
    "    savename : str of name of the shapefile. Default set to None.\n",
    "    \n",
    "    returns: geopandas dataframe with data information.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_parq = pd.read_parquet(parquet_file) # must have twc/era5 grid information\n",
    "    df_grid = gpd.read_file(grid_geometry_file) # must have matching twc grid information with the parquet file and geometry info\n",
    "    \n",
    "    df_compiled = df_parq.merge(df_grid, on='grid_id', how='inner')\n",
    "    gdf_compiled = gpd.GeoDataFrame(df_compiled, geometry='geometry')\n",
    "    \n",
    "    if save:\n",
    "        makedirs([output_folder])\n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        gdf_compiled.to_file(savefile)\n",
    "    \n",
    "    return gdf_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff4ca3",
   "metadata": {},
   "source": [
    "## Weather Data Query Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55c91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_twc_grids_for_specific_bounds(minx, miny, maxx, maxy, dataset_id, output_folder, savename, \n",
    "                                        bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Query and extract TWC grid ids as a geodataframe using minx, miny, maxx, maxy bounds.\n",
    "    \n",
    "    params:\n",
    "    minx, miny, maxx, maxy: minx, miny, maxx, maxy bounds of a region.\n",
    "    dataset_id : str of ID of dataset in Google Cloud. \n",
    "                 'location360-datasets.historical_weather.twc_cod_grids' for all dataset except precipitation.\n",
    "                 'location360-datasets.historical_weather.twc_hires_precip_grids' for precipitation data.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried results with twc_grids and their geometry information.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox_wkt = f'POLYGON(({minx} {miny}, {minx} {maxy}, {maxx} { maxy}, {maxx} {miny}, {minx} {miny}))' # assigning min-max bound information to wkt format \n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT grid_id, lat, lon, geom as geometry, elevation, time_zone\n",
    "    FROM {dataset_id} as twc_grids\n",
    "    WHERE ST_INTERSECTS(geom, ST_GEOGFROMTEXT('{bbox_wkt}'))\n",
    "    \"\"\"\n",
    "    \n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads) # converting to wkt is required to create a GeoDataframe\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "    \n",
    "    makedirs([output_folder])\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    query_gdf.to_file(savefile)\n",
    "    \n",
    "    return query_gdf   \n",
    "\n",
    "\n",
    "def query_twc_weather_by_grid(grid_shp, date, dataset_id, output_folder, savename,\n",
    "                              date_range=None, query_breaks=25,\n",
    "                              var_columns = ['max_temperature', 'min_temperature', 'avg_wind_speed', \n",
    "                                             'avg_relative_humidity', 'avg_atmospheric_pressure'],\n",
    "                              save_as_gpd=False, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Query and extract TWC weather data for a specific date or date_range.\n",
    "    \n",
    "    params:\n",
    "    grid_shp : shapefile path of twc grids.\n",
    "    date : str of date for which to download data. If date is assinged set date_range to None. \n",
    "    dataset_id : str of ID of dataset in Google Cloud. \n",
    "                 'location360-datasets.historical_weather.twc_historical_metric_daily' for all dataset except precipitation.\n",
    "                 'location360-datasets.historical_weather.twc_high_resolution_precipitation_metric_daily' for precipitation data.\n",
    "    date_range : list of str of dates for which to download data. If date_range is assinged set date to None.\n",
    "    query_breaks : Number of breaks to make in twc_grids for faster querying.\n",
    "    var_columns : list of variables to download from twc dataset.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    save_as_gpd : Set to True if want to save queried data as a geodataframe. Default set to False.\n",
    "    \n",
    "    returns: pandas/geopandas dataframe of queried results.\n",
    "    \"\"\"\n",
    "    \n",
    "    var_rename_dict = {'total_precipitation': 'total_precip', 'max_temperature': 'max_temp', 'min_temperature': 'min_temp', \n",
    "                       'avg_wind_speed': 'avg_wind_speed', 'avg_relative_humidity': 'avg_Rhumid', \n",
    "                       'avg_atmospheric_pressure': 'avg_atmPress', 'avg_total_cloud_cover': 'avg_Cloud',\n",
    "                       'total_downward_solar_radiation': 'TotDown_SR', 'max_downward_solar_radiation': 'MaxDown_SR'}\n",
    "    \n",
    "    twc_df = gpd.read_file(grid_shp)\n",
    "    \n",
    "    # rearranging df for quicker query. Each query will ask data for multiple grids.\n",
    "    length = len(twc_df)\n",
    "    query_breaks = np.linspace(start=0, stop=length, num=query_breaks, dtype='int')\n",
    "\n",
    "    df_grids = list(twc_df['grid_id'])\n",
    "    grids_stacked = []\n",
    "\n",
    "    for i in range(len(query_breaks)-1):\n",
    "        grids_new = df_grids[query_breaks[i] : query_breaks[i+1]]\n",
    "        grids_stacked.append(grids_new)\n",
    "\n",
    "    dict_grids = {'grid_id': grids_stacked}\n",
    "    grid_df = pd.DataFrame(dict_grids)\n",
    "\n",
    "    # Querying Data\n",
    "    var_columns = ','.join(var_columns)\n",
    "\n",
    "    twc_weather_query = pd.DataFrame()\n",
    "    if date_range is None:\n",
    "        for idx, row in grid_df.iterrows():\n",
    "            grid_tup = tuple(row['grid_id'])\n",
    "            query = f\"\"\"\n",
    "                    SELECT grid_id, date, {var_columns}\n",
    "                    FROM {dataset_id} as twc_historic_daily\n",
    "                    WHERE grid_id IN {grid_tup} \n",
    "                    AND Date(date) = '{date}'\n",
    "                    \"\"\"\n",
    "            temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "            twc_weather_query = pd.concat([twc_weather_query, temp_query])\n",
    "            \n",
    "    else: # set date to None\n",
    "        date1, date2 = date_range[0], date_range[1] \n",
    "        for idx, row in grid_df.iterrows():\n",
    "            grid_tup = tuple(row['grid_id'])\n",
    "            query = f\"\"\"\n",
    "                    SELECT grid_id, date, {var_columns}\n",
    "                    FROM {dataset_id} as twc_historic_daily\n",
    "                    WHERE grid_id IN {grid_tup} \n",
    "                    AND Date(date) BETWEEN '{date1}' AND '{date2}'  \n",
    "                    \"\"\"\n",
    "            temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "            twc_weather_query = pd.concat([twc_weather_query, temp_query])\n",
    "            \n",
    "    makedirs([output_folder])\n",
    "    if save_as_gpd:\n",
    "        # Converting query to Geopandas\n",
    "        twc_weather_query = twc_weather_query.merge(twc_df, on='grid_id', how='left')\n",
    "        twc_weather_query = twc_weather_query.drop(columns=['index'])\n",
    "        twc_weather_df = gpd.GeoDataFrame(twc_weather_query, geometry='geometry')\n",
    "        twc_weather_df = twc_weather_df.rename(columns=var_rename_dict)\n",
    "        twc_weather_df['date'] = twc_weather_df['date'].astype('str')  # google cloud date data type can't be read into dateformat in pandas. Converting to str.\n",
    "        \n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        twc_weather_df.to_file(savefile)\n",
    "        \n",
    "    else:\n",
    "        # Formatting dataframe\n",
    "        twc_weather_df = twc_weather_query.rename(columns=var_rename_dict)\n",
    "        twc_weather_df['date'] = twc_weather_df['date'].astype('str')  # google cloud date data type can't be read into dateformat in pandas. Converting to str.\n",
    "        \n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        twc_weather_df.to_parquet(savefile)\n",
    "\n",
    "\n",
    "    return twc_weather_df\n",
    "\n",
    "def query_era5_grids_for_specific_bounds(minx, miny, maxx, maxy, output_folder, savename, bq_project='location360-datasets'):\n",
    "    \"\"\"\n",
    "    Query and extract ERA5 grid ids as a geodataframe using minx, miny, maxx, maxy bounds.\n",
    "    \n",
    "    params:\n",
    "    minx, miny, maxx, maxy: minx, miny, maxx, maxy bounds of a region.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried results with twc_grids and their geometry information.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox_wkt = f'POLYGON(({minx} {miny}, {minx} {maxy}, {maxx} { maxy}, {maxx} {miny}, {minx} {miny}))'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT grid_id, lat, lon, time_zone, geom as geometry\n",
    "    FROM location360-datasets.historical_weather.era5_grids as era5_grids\n",
    "    WHERE ST_INTERSECTS(geom, ST_GEOGFROMTEXT('{bbox_wkt}'))\n",
    "    \"\"\"\n",
    "    \n",
    "    query_result = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "    query_result['geometry'] =  query_result['geometry'].apply(wkt.loads)\n",
    "    query_gdf = gpd.GeoDataFrame(query_result, geometry='geometry')\n",
    "    \n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    query_gdf.to_file(savefile)\n",
    "    \n",
    "    return query_gdf \n",
    "\n",
    "def query_era5_weather_by_grid(grid_shp, date, output_folder, savename,\n",
    "                               date_range=None, query_breaks=25,\n",
    "                               var_columns = ['total_precipitation', 'max_temperature', 'min_temperature', \n",
    "                                              'max_dew_point_temperature', 'min_dew_point_temperature', 'avg_dew_point_temperature',\n",
    "                                              'max_wind_speed', 'min_wind_speed', 'avg_wind_speed', 'avg_wind_direction', \n",
    "                                              'max_relative_humidity', 'min_relative_humidity', 'avg_relative_humidity', \n",
    "                                              'max_atmospheric_pressure', 'min_atmospheric_pressure', 'avg_atmospheric_pressure', \n",
    "                                              'avg_snow_depth','avg_total_cloud_cover', 'total_downward_solar_radiation', \n",
    "                                              'max_downward_solar_radiation', 'total_net_solar_radiation', 'max_net_solar_radiation',\n",
    "                                              'avg_soil_temperature_level_1', 'avg_soil_moisture_level_1',                                               'eto', 'etr'],\n",
    "                              save_as_gpd=False, bq_project='location360-datasets', ):\n",
    "    \"\"\"\n",
    "    Query and extract ERA5 weather data for a specific date or date_range.\n",
    "    \n",
    "    params:\n",
    "    grid_shp : shapefile path of era5 grids.\n",
    "    date : str of date for which to download data. If date is assinged set date_range to None. \n",
    "    date_range : list of str of dates for which to download data. If date_range is assinged set date to None.\n",
    "    query_breaks : Number of breaks to make in era5_grids for faster querying.\n",
    "    var_columns : list of variables to download from era5 dataset.\n",
    "    output_folder : str of output folder to save the data. \n",
    "    savename : str of name of the shapefile.\n",
    "    save_as_gpd : Set to True if want to save queried data as a geodataframe. Default set to False.\n",
    "    bq_project : Bigquery project name. Default set to 'location360-datasets'.\n",
    "    \n",
    "    returns: geopandas dataframe of queried results.\n",
    "    \"\"\"\n",
    "    \n",
    "    var_rename_dict = {'total_precipitation': 'total_precip', 'max_temperature': 'max_temp', 'min_temperature': 'min_temp', \n",
    "                       'max_dew_point_temperature': 'max_dew_temp', 'min_dew_point_temperature': 'min_dew_temp', \n",
    "                       'avg_dew_point_temperature': 'avg_dew_temp', 'avg_wind_direction': 'avg_wind_dir',\n",
    "                       'max_relative_humidity': 'max_Rhumid', 'min_relative_humidity': 'min_Rhumid', \n",
    "                       'avg_relative_humidity': 'avg_Rhumid', 'max_atmospheric_pressure': 'max_atmPress', \n",
    "                       'min_atmospheric_pressure': 'min_atmPress',\n",
    "                       'avg_atmospheric_pressure': 'avg_atmPress', 'avg_total_cloud_cover': 'avg_Cloud',\n",
    "                       'total_downward_solar_radiation': 'TotDown_SR', 'max_downward_solar_radiation': 'MaxDown_SR', \n",
    "                       'total_net_solar_radiation': 'TotNet_SR', 'max_net_solar_radiation': 'MaxNet_SR',\n",
    "                       'avg_soil_temperature_level_1': 'avg_ST_L1', 'avg_soil_temperature_level_2': 'avg_ST_L2',\n",
    "                       'avg_soil_temperature_level_3': 'avg_ST_L3', 'avg_soil_temperature_level_4': 'avg_ST_L4',\n",
    "                       'avg_soil_moisture_level_1': 'avg_SM_L1', 'avg_soil_moisture_level_2':'avg_SM_L2',\n",
    "                       'avg_soil_moisture_level_3': 'avg_SM_L3', 'avg_soil_moisture_level_4': 'avg_SM_L4'}\n",
    "    \n",
    "    era5_df = gpd.read_file(grid_shp)\n",
    "    \n",
    "    # rearranging df for quicker query. Each query will ask data for multiple grids.\n",
    "    length = len(era5_df)\n",
    "    query_breaks = np.linspace(start=0, stop=length, num=query_breaks, dtype='int')\n",
    "\n",
    "    df_grids = list(era5_df['grid_id'])\n",
    "    grids_stacked = []\n",
    "\n",
    "    for i in range(len(query_breaks)-1):\n",
    "        grids_new = df_grids[query_breaks[i] : query_breaks[i+1]]\n",
    "        grids_stacked.append(grids_new)\n",
    "\n",
    "    dict_grids = {'grid_id': grids_stacked}\n",
    "    grid_df = pd.DataFrame(dict_grids)\n",
    "\n",
    "    # Querying Data\n",
    "    var_columns = ','.join(var_columns)\n",
    "\n",
    "    era5_weather_query = pd.DataFrame()\n",
    "    if date_range is None:\n",
    "        for idx, row in grid_df.iterrows():\n",
    "            grid_tup = tuple(row['grid_id'])\n",
    "            query = f\"\"\"\n",
    "                    SELECT grid_id, date, {var_columns}\n",
    "                    FROM `location360-datasets.historical_weather.era5_metric_daily` as era5_metric_daily\n",
    "                    WHERE grid_id IN {grid_tup} \n",
    "                    AND Date(date) = '{date}'\n",
    "                    \"\"\"\n",
    "            temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "            era5_weather_query = pd.concat([era5_weather_query, temp_query])\n",
    "            \n",
    "    else: # set date to None\n",
    "        date1, date2 = date_range[0], date_range[1] \n",
    "        for idx, row in grid_df.iterrows():\n",
    "            grid_tup = tuple(row['grid_id'])\n",
    "            query = f\"\"\"\n",
    "                    SELECT grid_id, date, {var_columns}\n",
    "                    FROM `location360-datasets.historical_weather.era5_metric_daily` as era5_metric_daily\n",
    "                    WHERE grid_id IN {grid_tup} \n",
    "                    AND Date(date) BETWEEN '{date1}' AND '{date2}'  \n",
    "                    \"\"\"\n",
    "            temp_query = gbq.read_gbq(query, project_id=bq_project, credentials=credentials)\n",
    "            era5_weather_query = pd.concat([era5_weather_query, temp_query])\n",
    "            \n",
    "    if save_as_gpd:\n",
    "        # Converting query to Geopandas\n",
    "        era5_weather_query = era5_weather_query.merge(era5_df, on='grid_id', how='left')\n",
    "        era5_weather_query = era5_weather_query.drop(columns=['index'])\n",
    "        era5_weather_df = gpd.GeoDataFrame(era5_weather_query, geometry='geometry')\n",
    "        era5_weather_df = era5_weather_df.rename(columns=var_rename_dict)\n",
    "        era5_weather_df['date'] = era5_weather_df['date'].astype('str')\n",
    "    \n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        era5_weather_df.to_file(savefile)\n",
    "        \n",
    "    else:\n",
    "        # Formatting dataframe\n",
    "        era5_weather_df = era5_weather_query.rename(columns=var_rename_dict)\n",
    "        era5_weather_df['date'] = era5_weather_df['date'].astype('str')  # google cloud date data type can't be read into dateformat in pandas. Converting to str.\n",
    "\n",
    "        savefile = os.path.join(output_folder, savename)\n",
    "        era5_weather_df.to_parquet(savefile)\n",
    "    \n",
    "    return era5_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0f805",
   "metadata": {},
   "source": [
    "## Soil Data Query Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8feacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_soil_data_on_hid_h3(hid_h3_df, output_folder, savename, query_breaks=500, hid_col='hids', h3_col='h3',\n",
    "            soil250_cols = ['awcts_depth_0cm', 'awcts_depth_5cm', 'awcts_depth_15cm', 'awcts_depth_30cm', \n",
    "                            'awcts_depth_60cm', 'awcts_depth_100cm','wwp_depth_0cm', 'wwp_depth_5cm', 'wwp_depth_15cm', \n",
    "                            'wwp_depth_30cm', 'wwp_depth_60cm', 'wwp_depth_100cm'],\n",
    "            soil250_V2_cols = ['nitrogen_0_5cm_mean', 'nitrogen_5_15cm_mean', 'nitrogen_15_30cm_mean', 'nitrogen_30_60cm_mean', \n",
    "                               'nitrogen_60_100cm_mean', 'nitrogen_100_200cm_mean', 'soc_0_5cm_mean', 'soc_5_15cm_mean', \n",
    "                               'soc_15_30cm_mean', 'soc_30_60cm_mean', 'soc_60_100cm_mean', 'soc_100_200cm_mean', \n",
    "                               'bulk_density', 'om', 'pH', 'sand', 'silt', 'clay', 'cec', 'soilTexture'],\n",
    "            download_from = 'soil250'):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Queries soil250 V1 and V2 data by hids and h3.\n",
    "    \n",
    "    :param hid_h3_df : A geodataframe with h3 and hid info in columns.\n",
    "    :param output_folder :  Filepath of output folder to save the queried dataframe and parquet.\n",
    "    :param savename : Name of the parquet file. Should include '.parquet'.\n",
    "    :param query_breaks : Number of breaks to make in twc_grids for faster querying.\n",
    "    :param hid_col : Name of column in the hid_h3_df with 'hid' info. Default set to 'hids'.\n",
    "    :param h3_col : Name of column in the hid_h3_df with 'h3' info. Default set to 'h3'.\n",
    "    :param soil250_cols : List of column (data) names to download from soil250 V1 dataset. \n",
    "                          For downloading data from soil250 V1 dataset, set download_from = 'soil250'.\n",
    "    :param soil250_V2_cols : List of column (data) names to download from soil250 V2 dataset. \n",
    "                             For downloading data from soil250 V1 dataset, set download_from = 'soil250_V2'.\n",
    "    :param download_from : Set to 'soil250' to download data from soil250 V1 dataset.\n",
    "                           Set to 'soil250_V2' to download data from soil250 V2 dataset.\n",
    "    \n",
    "    returns : geopandas dataframe of queried result.\n",
    "    \"\"\"\n",
    "    \n",
    "    rename_dict = {'awcts_depth_0cm': 'awct_0cm', 'awcts_depth_5cm': 'awct_5cm', 'awcts_depth_15cm': 'awct_15cm', \n",
    "                   'awcts_depth_30cm': 'awct_30cm', 'awcts_depth_60cm': 'awct_60cm', 'awcts_depth_100cm': 'awct_100cm',\n",
    "                   'wwp_depth_0cm': 'wwp_0cm', 'wwp_depth_5cm': 'wwp_5cm', 'wwp_depth_15cm': 'wwp_15cm',\n",
    "                   'wwp_depth_30cm': 'wwp_30cm', 'wwp_depth_60cm': 'wwp_60cm', 'wwp_depth_100cm': 'wwp_100cm',\n",
    "                   'nitrogen_0_5cm_mean': 'nit0_5', 'nitrogen_5_15cm_mean': 'nit5_15', 'nitrogen_15_30cm_mean': 'nit15_30', \n",
    "                   'nitrogen_30_60cm_mean': 'nit30_60', 'nitrogen_60_100cm_mean': 'nit60_100', \n",
    "                   'nitrogen_100_200cm_mean': 'nit100_200', \n",
    "                   'soc_0_5cm_mean': 'soc0_5', 'soc_5_15cm_mean': 'soc5_15', 'soc_15_30cm_mean': 'soc15_30', \n",
    "                   'soc_30_60cm_mean': 'soc30_60', 'soc_60_100cm_mean': 'soc60_100', \n",
    "                   'soc_100_200cm_mean':'soc100_200', 'bulk_density': 'BulkDense', 'soilTexture': 'soiltext'}\n",
    "    \n",
    "    # Deciding the indices for query chunks. Each chunk will be queried at a time\n",
    "    length = len(hid_h3_df['hids'])\n",
    "    query_breaks = np.linspace(start=0, stop=length, num=query_breaks, dtype='int')\n",
    "\n",
    "    # Making a list of hids/h3 from the input hid_h3_df\n",
    "    hids = list(hid_h3_df['hids'])\n",
    "    h3 = list(hid_h3_df['h3'])\n",
    "\n",
    "    # splitting hids and h3 to multiple chunks. Each chunk of hid/h3 will be queried at a time\n",
    "    hids_stacked = []\n",
    "    h3_stacked = []\n",
    "    for i in range(len(query_breaks)-1):\n",
    "        hids_new = hids[query_breaks[i] : query_breaks[i+1]]\n",
    "        h3_new = h3[query_breaks[i] : query_breaks[i+1]]\n",
    "        hids_stacked.append(hids_new)\n",
    "        h3_stacked.append(h3_new)\n",
    "    \n",
    "    # making a dataframe where each row consists of number of h3/hids to be queried together\n",
    "    dict_hid_h3 = {'hids': hids_stacked, 'h3': h3_stacked}\n",
    "    df = pd.DataFrame(dict_hid_h3)\n",
    "    \n",
    "    # setting up query column and location based on soil250 original and V2 sources\n",
    "    if download_from == 'soil250':\n",
    "        soil_cols = ','.join(soil250_cols)\n",
    "        location = 'location360-datasets.environmental_data_cube.isric_global_soil_250'\n",
    "    \n",
    "    elif download_from == 'soil250_V2':\n",
    "        soil_cols = ','.join(soil250_V2_cols)\n",
    "        location = 'location360-datasets.environmental_data_cube.isric_global_soil_250_v2'\n",
    "    \n",
    "    # Looping through the stacked df and querying by chunks\n",
    "    final_soil_query_df = pd.DataFrame()\n",
    "    for idx, row in df.iterrows():\n",
    "        hid_tup = tuple(row['hids'])\n",
    "        h3_tup = tuple(row['h3'])\n",
    "        \n",
    "        # query from soil250\n",
    "        query = f\"\"\"\n",
    "                SELECT h3_index_10 as h3, hid, {soil_cols}\n",
    "                FROM {location}\n",
    "                WHERE hid in {hid_tup} and h3_index_10 in {h3_tup}\n",
    "                \"\"\"\n",
    "        soil250_query = gbq.read_gbq(query, project_id='location360-datasets', credentials=credentials)\n",
    "        final_soil_query_df = pd.concat([final_soil_query_df, soil250_query])\n",
    "    \n",
    "    final_soil_query_df = final_soil_query_df.rename(columns=rename_dict)\n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    final_soil_query_df.to_parquet(savefile)\n",
    "\n",
    "    return final_soil_query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b6dde",
   "metadata": {},
   "source": [
    "# Elevation Data Query Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_srtm_elevation_data_on_hid_h3(hid_h3_df, output_folder, savename, query_breaks=500, hid_col='hids', h3_col='h3'):\n",
    "    \"\"\"\n",
    "    Queries SRTM DEM data by hids and h3.\n",
    "    \n",
    "    :param hid_h3_df : A geodataframe with h3 and hid info in columns.\n",
    "    :param output_folder :  Filepath of output folder to save the queried dataframe and parquet.\n",
    "    :param savename : Name of the parquet file. Should include '.parquet'.\n",
    "    :param query_breaks : Number of breaks to make in twc_grids for faster querying.\n",
    "    :param hid_col : Name of column in the hid_h3_df with 'hid' info. Default set to 'hids'.\n",
    "    :param h3_col : Name of column in the hid_h3_df with 'h3' info. Default set to 'h3'.\n",
    "    \n",
    "    returns : geopandas dataframe of queried result.\n",
    "    \"\"\"\n",
    "    # Deciding the indices for query chunks. Each chunk will be queried at a time\n",
    "    length = len(hid_h3_df['hids'])\n",
    "    query_breaks = np.linspace(start=0, stop=length, num=query_breaks, dtype='int')\n",
    "\n",
    "    # Making a list of hids/h3 from the input hid_h3_df\n",
    "    hids = list(hid_h3_df['hids'])\n",
    "    h3 = list(hid_h3_df['h3'])\n",
    "\n",
    "    # splitting hids and h3 to multiple chunks. Each chunk of hid/h3 will be queried at a time\n",
    "    hids_stacked = []\n",
    "    h3_stacked = []\n",
    "    for i in range(len(query_breaks)-1):\n",
    "        hids_new = hids[query_breaks[i] : query_breaks[i+1]]\n",
    "        h3_new = h3[query_breaks[i] : query_breaks[i+1]]\n",
    "        hids_stacked.append(hids_new)\n",
    "        h3_stacked.append(h3_new)\n",
    "\n",
    "    # making a dataframe where each row consists of number of h3/hids to be queried together\n",
    "    dict_hid_h3 = {'hids': hids_stacked, 'h3': h3_stacked}\n",
    "    df = pd.DataFrame(dict_hid_h3)\n",
    "    \n",
    "    # Looping through the stacked df and querying by chunks\n",
    "    final_elev_query_df = pd.DataFrame()\n",
    "    for idx, row in df.iterrows():\n",
    "        hid_tup = tuple(row['hids'])\n",
    "        h3_tup = tuple(row['h3'])\n",
    "        \n",
    "        # query from soil250\n",
    "        query = f\"\"\"\n",
    "                SELECT h3, hids, elevation\n",
    "                FROM `location360-datasets.environmental_data_cube.global_srtm_30m_topography`\n",
    "                WHERE hids in {hid_tup} and h3 in {h3_tup}\n",
    "                \"\"\"\n",
    "        temp_elev_query = gbq.read_gbq(query, project_id='location360-datasets', credentials=credentials)\n",
    "        final_elev_query_df = pd.concat([final_elev_query_df, temp_elev_query])\n",
    "    \n",
    "    savefile = os.path.join(output_folder, savename)\n",
    "    final_elev_query_df.to_parquet(savefile)\n",
    "\n",
    "    return final_elev_query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df455b",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18512511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(directory_list):\n",
    "    \"\"\"\n",
    "    Make directory (if not exists) from a list of directory.\n",
    "\n",
    "    :param directory_list: A list of directories to create.\n",
    "\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    for directory in directory_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "def make_folder_in_s3_bucket(new_folder_path, bucket_name='data-pipeline-env-model'):\n",
    "    \"\"\"\n",
    "    Make directory/folder in AWS S3 Bucket.\n",
    "    \n",
    "    params:\n",
    "    new_folder_path : Folder path to create in the S3 bucket. Have to be like this \"Main_folder/subfolder\"\n",
    "    bucket_name : S3 bucket name. Default set to 'data-pipeline-env-model'\n",
    "    \n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket_name, Key=(new_folder_path+'/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328a512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
